<!-- prettier-ignore -->

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>ray.util.spark.cluster_init &#8212; Ray 3.0.0.dev0</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../../../../_static/styles/bootstrap.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />

  
  <link href="../../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=ac02cc09edc035673794" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/termynal.css?v=2fc3cb5e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/csat.css?v=c8f39c76" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/assistant.css?v=d345c55e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/custom.css?v=633d7681" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/docsearch.css?v=45fb5152" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/pydata-docsearch-custom.css?v=7fb92720" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794" />
<link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794" />
  <script src="../../../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=ac02cc09edc035673794"></script>

    <script src="../../../../_static/documentation_options.js?v=d1493c90"></script>
    <script src="../../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../../_static/copybutton.js?v=9a29e97e"></script>
    <script defer="defer" src="../../../../_static/js/termynal.js?v=67cfcf08"></script>
    <script defer="defer" src="../../../../_static/js/custom.js?v=9e3b357f"></script>
    <script src="../../../../_static/js/csat.js?v=8e649b1c"></script>
    <script defer="defer" src="../../../../_static/js/assistant.js?v=73fdc522"></script>
    <script defer="defer" src="../../../../_static/docsearch.js?v=77274085"></script>
    <script defer="defer" src="../../../../_static/docsearch_config.js?v=d25523ed"></script>
    <script src="../../../../_static/design-tabs.js?v=36754332"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/ray/util/spark/cluster_init';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.14.1';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://antgroup.github.io/ant-ray/versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'dev-debugpy';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = false;
        </script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/_modules/ray/util/spark/cluster_init.html" />
    <link rel="icon" href="../../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" /><!-- Extra header to include at the top of each template.
Kept separately so that it can easily be included in any templates
that need to be overridden for individual pages; e.g. included
both in the usual template (layout.html) as well as (index.html). -->

<link rel="preconnect" href="https://fonts.googleapis.com" />
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
<link
  href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;900&family=Roboto:wght@400;500;700&display=swap"
  rel="stylesheet"
/>
<link
  rel="stylesheet"
  title="light"
  href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css"
  disabled="disabled"
/>
<link
  rel="stylesheet"
  title="dark"
  href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css"
  disabled="disabled"
/>
<link
  href="https://cdn.jsdelivr.net/npm/remixicon@4.1.0/fonts/remixicon.css"
  rel="stylesheet"
/>

<!-- Used for text embedded in html on the index page  -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- Parser used to call hljs on responses from Ray Assistant -->
<script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

<!-- Sanitizer for Ray Assistant AI -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/dompurify/2.3.3/purify.min.js"></script>

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<!-- Google Tag Manager -->
<script>
  (function (w, d, s, l, i) {
    w[l] = w[l] || [];
    w[l].push({
      'gtm.start': new Date().getTime(),
      event: 'gtm.js',
    });
    var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s),
      dl = l != 'dataLayer' ? '&l=' + l : '';
    j.async = true;
    j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
    f.parentNode.insertBefore(j, f);
  })(window, document, 'script', 'dataLayer', 'GTM-P8H6KQG');
</script>
<!-- End Google Tag Manager -->
<!-- Data to be shared with JS on every page -->
<script>
  window.data = {
      copyIconSrc: "../../../../_static/copy-button.svg"
  };
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  

<a class="navbar-brand logo" href="../../../../index.html">
  <svg width="400" height="201" viewBox="0 0 400 201" xmlns="http://www.w3.org/2000/svg">
<path id="ray-text" d="M325.949 134.356V109.785L302.442 66.6406H314.244L330.495 97.3062H331.946L348.198 66.6406H360L336.493 109.785V134.356H325.949ZM253.043 134.364L272.391 66.648H290.771L310.021 134.364H299.283L294.834 118.402H268.328L263.878 134.364H253.043ZM270.94 108.728H292.222L282.354 73.1294H280.807L270.94 108.728ZM198.887 134.364V66.648H227.327C231.519 66.648 235.195 67.3896 238.355 68.8729C241.58 70.2918 244.063 72.3555 245.804 75.0641C247.61 77.7727 248.513 80.9973 248.513 84.7378V85.8019C248.513 90.0583 247.481 93.4763 245.417 96.0559C243.418 98.5711 240.967 100.345 238.065 101.376V102.924C240.516 103.053 242.483 103.892 243.966 105.439C245.449 106.923 246.191 109.083 246.191 111.921V134.364H235.647V113.372C235.647 111.631 235.195 110.244 234.292 109.212C233.39 108.18 231.938 107.664 229.939 107.664H209.334V134.364H198.887ZM209.334 98.1842H226.166C229.907 98.1842 232.809 97.249 234.873 95.3788C236.937 93.4441 237.968 90.8322 237.968 87.5431V86.7692C237.968 83.4802 236.937 80.9005 234.873 79.0303C232.874 77.0956 229.971 76.1282 226.166 76.1282H209.334V98.1842Z" fill="black"/>
<path fill-rule="evenodd" clip-rule="evenodd" d="M143.63 101.311L98.3087 146.632L94.9903 143.313L140.311 97.9925L143.63 101.311ZM141.953 102.334L51.4454 102.334V97.6409L141.953 97.6409V102.334ZM94.992 55.9863L140.313 101.307L143.631 97.9886L98.3105 52.6679L94.992 55.9863Z" fill="#02A0CF"/>
<path d="M40 88.3163H62.6604V110.977H40V88.3163ZM85.3207 88.3163H107.981V110.977H85.3207V88.3163ZM85.3207 43H107.981V65.6604H85.3207V43ZM85.3207 133.645H107.981V156.306H85.3207V133.645ZM130.641 88.3163H153.301V110.977H130.641V88.3163Z" fill="#02A0CF"/>
</svg>

</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <div class="navbar-content docutils container">
<ul class="navbar-toplevel">
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/getting-started.html" title="Get Started"><span class="navbar-link-title">Get Started</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/use-cases.html" title="Use Cases"><span class="navbar-link-title">Use Cases</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/examples.html" title="Example Gallery"><span class="navbar-link-title">Example Gallery</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/installation.html" title="Library"><span class="navbar-link-title">Library</span></a></p>
<i class="fa-solid fa-chevron-down"></i></div>
<div class="navbar-dropdown docutils container">
<ul class="navbar-sublevel">
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-core/walkthrough.html" title="Ray Core"><span class="navbar-link-title">Ray Core</span>Scale general Python applications</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../data/data.html" title="Ray Data"><span class="navbar-link-title">Ray Data</span>Scale data ingest and preprocessing</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../train/train.html" title="Ray Train"><span class="navbar-link-title">Ray Train</span>Scale machine learning training</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../tune/index.html" title="Ray Tune"><span class="navbar-link-title">Ray Tune</span>Scale hyperparameter tuning</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../serve/index.html" title="Ray Serve"><span class="navbar-link-title">Ray Serve</span>Scale model serving</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../rllib/index.html" title="Ray RLlib"><span class="navbar-link-title">Ray RLlib</span>Scale reinforcement learning</a></p>
</div>
</li>
</ul>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../index.html" title="Docs"><span class="navbar-link-title">Docs</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://discuss.ray.io" title="Resources"><span class="navbar-link-title">Resources</span></a></p>
<i class="fa-solid fa-chevron-down"></i></div>
<div class="navbar-dropdown docutils container">
<ul class="navbar-sublevel">
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://discuss.ray.io" title="Discussion Forum"><span class="navbar-link-title">Discussion Forum</span>Get your Ray questions answered</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://github.com/ray-project/ray-educational-materials" title="Training"><span class="navbar-link-title">Training</span>Hands-on learning</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/blog" title="Blog"><span class="navbar-link-title">Blog</span>Updates, best practices, user-stories</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/events" title="Events"><span class="navbar-link-title">Events</span>Webinars, meetups, office hours</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/blog/how-ray-and-anyscale-make-it-easy-to-do-massive-scale-machine-learning-on" title="Success Stories"><span class="navbar-link-title">Success Stories</span>Real-world workload examples</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/ray-libraries.html" title="Ecosystem"><span class="navbar-link-title">Ecosystem</span>Libraries integrated with Ray</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.ray.io/community" title="Community"><span class="navbar-link-title">Community</span>Connect with us</a></p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>

</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          <div id="docsearch"></div>
        </div>
      
      
        <div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
      
        <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="versionswitcherbutton" type="button" role="button" class="version-switcher__button btn btn-sm navbar-btn dropdown-toggle" data-bs-toggle="dropdown" aria-haspopup="listbox" aria-controls="versionswitcherlist" aria-label="Version switcher list">
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="versionswitcherlist" class="version-switcher__menu dropdown-menu list-group-flush py-0" role="listbox" aria-labelledby="versionswitcherbutton">
    <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
      
        <div class="navbar-item"><a
  id="try-anyscale-href"
  href="https://www.anyscale.com/ray-on-anyscale/?utm_source=ray_docs&utm_medium=docs&utm_campaign=navbar"
>
  <div id="try-anyscale-text">
    <span>Try Ray on Anyscale</span>
  </div>
</a></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile"><div id="docsearch"></div>
    </div>
  

  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <div class="navbar-content docutils container">
<ul class="navbar-toplevel">
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/getting-started.html" title="Get Started"><span class="navbar-link-title">Get Started</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/use-cases.html" title="Use Cases"><span class="navbar-link-title">Use Cases</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/examples.html" title="Example Gallery"><span class="navbar-link-title">Example Gallery</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/installation.html" title="Library"><span class="navbar-link-title">Library</span></a></p>
<i class="fa-solid fa-chevron-down"></i></div>
<div class="navbar-dropdown docutils container">
<ul class="navbar-sublevel">
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-core/walkthrough.html" title="Ray Core"><span class="navbar-link-title">Ray Core</span>Scale general Python applications</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../data/data.html" title="Ray Data"><span class="navbar-link-title">Ray Data</span>Scale data ingest and preprocessing</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../train/train.html" title="Ray Train"><span class="navbar-link-title">Ray Train</span>Scale machine learning training</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../tune/index.html" title="Ray Tune"><span class="navbar-link-title">Ray Tune</span>Scale hyperparameter tuning</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../serve/index.html" title="Ray Serve"><span class="navbar-link-title">Ray Serve</span>Scale model serving</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../rllib/index.html" title="Ray RLlib"><span class="navbar-link-title">Ray RLlib</span>Scale reinforcement learning</a></p>
</div>
</li>
</ul>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../index.html" title="Docs"><span class="navbar-link-title">Docs</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://discuss.ray.io" title="Resources"><span class="navbar-link-title">Resources</span></a></p>
<i class="fa-solid fa-chevron-down"></i></div>
<div class="navbar-dropdown docutils container">
<ul class="navbar-sublevel">
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://discuss.ray.io" title="Discussion Forum"><span class="navbar-link-title">Discussion Forum</span>Get your Ray questions answered</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://github.com/ray-project/ray-educational-materials" title="Training"><span class="navbar-link-title">Training</span>Hands-on learning</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/blog" title="Blog"><span class="navbar-link-title">Blog</span>Updates, best practices, user-stories</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/events" title="Events"><span class="navbar-link-title">Events</span>Webinars, meetups, office hours</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/blog/how-ray-and-anyscale-make-it-easy-to-do-massive-scale-machine-learning-on" title="Success Stories"><span class="navbar-link-title">Success Stories</span>Real-world workload examples</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/ray-libraries.html" title="Ecosystem"><span class="navbar-link-title">Ecosystem</span>Libraries integrated with Ray</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.ray.io/community" title="Community"><span class="navbar-link-title">Community</span>Connect with us</a></p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>

</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
        
          <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="versionswitcherbutton" type="button" role="button" class="version-switcher__button btn btn-sm navbar-btn dropdown-toggle" data-bs-toggle="dropdown" aria-haspopup="listbox" aria-controls="versionswitcherlist" aria-label="Version switcher list">
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="versionswitcherlist" class="version-switcher__menu dropdown-menu list-group-flush py-0" role="listbox" aria-labelledby="versionswitcherbutton">
    <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
        
          <div class="navbar-item"><a
  id="try-anyscale-href"
  href="https://www.anyscale.com/ray-on-anyscale/?utm_source=ray_docs&utm_medium=docs&utm_campaign=navbar"
>
  <div id="try-anyscale-text">
    <span>Try Ray on Anyscale</span>
  </div>
</a></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item"><nav id="main-sidebar" class="bd-docs-nav bd-links" aria-label="Section Navigation">
  <div class="bd-toc-item navbar-nav"><ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../ray-overview/index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ray-overview/getting-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ray-overview/installation.html">Installation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../ray-overview/use-cases.html">Use Cases</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-air/getting-started.html">Ray for ML Infrastructure</a></li>

</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ray-overview/examples.html">Example Gallery</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ray-overview/ray-libraries.html">Ecosystem</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../ray-core/walkthrough.html">Ray Core</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-core/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-core/user-guide.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/tasks.html">Tasks</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/tasks/nested-tasks.html">Nested Remote Functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/tasks/generators.html">Dynamic generators</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/actors.html">Actors</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/named-actors.html">Named Actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/terminating-actors.html">Terminating Actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/async_api.html">AsyncIO / Concurrency for Actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/concurrency_group_api.html">Limiting Concurrency Per-Method with Concurrency Groups</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/actor-utils.html">Utility Classes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/out-of-band-communication.html">Out-of-band Communication</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/task-orders.html">Actor Task Execution Order</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/objects.html">Objects</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/objects/serialization.html">Serialization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/objects/object-spilling.html">Object Spilling</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/handling-dependencies.html">Environment Dependencies</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/scheduling/index.html">Scheduling</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/scheduling/resources.html">Resources</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/scheduling/accelerators.html">Accelerator Support</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/scheduling/placement-group.html">Placement Groups</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/scheduling/memory-management.html">Memory Management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/scheduling/ray-oom-prevention.html">Out-Of-Memory Prevention</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/fault-tolerance.html">Fault Tolerance</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/fault_tolerance/tasks.html">Task Fault Tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/fault_tolerance/actors.html">Actor Fault Tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/fault_tolerance/objects.html">Object Fault Tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/fault_tolerance/nodes.html">Node Fault Tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/fault_tolerance/gcs.html">GCS Fault Tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/fault_tolerance/head-ha.html">Head High-Availability Feature</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/patterns/index.html">Design Patterns &amp; Anti-patterns</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/nested-tasks.html">Pattern: Using nested tasks to achieve nested parallelism</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/generators.html">Pattern: Using generators to reduce heap memory usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/limit-pending-tasks.html">Pattern: Using ray.wait to limit the number of pending tasks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/limit-running-tasks.html">Pattern: Using resources to limit the number of concurrently running tasks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/concurrent-operations-async-actor.html">Pattern: Using asyncio to run actor methods concurrently</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/actor-sync.html">Pattern: Using an actor to synchronize other tasks and actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/tree-of-actors.html">Pattern: Using a supervisor actor to manage a tree of actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/pipelining.html">Pattern: Using pipelining to increase throughput</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/return-ray-put.html">Anti-pattern: Returning ray.put() ObjectRefs from a task harms performance and fault tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/ray-get-loop.html">Anti-pattern: Calling ray.get in a loop harms parallelism</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/unnecessary-ray-get.html">Anti-pattern: Calling ray.get unnecessarily harms performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/ray-get-submission-order.html">Anti-pattern: Processing results in submission order using ray.get increases runtime</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/ray-get-too-many-objects.html">Anti-pattern: Fetching too many objects at once with ray.get causes failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/too-fine-grained-tasks.html">Anti-pattern: Over-parallelizing with too fine-grained tasks harms speedup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/redefine-task-actor-loop.html">Anti-pattern: Redefining the same remote function or class harms performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/pass-large-arg-by-value.html">Anti-pattern: Passing the same large argument by value repeatedly harms performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/closure-capture-large-objects.html">Anti-pattern: Closure capturing large objects harms performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/global-variables.html">Anti-pattern: Using global variables to share state between tasks and actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/out-of-band-object-ref-serialization.html">Anti-pattern: Serialize ray.ObjectRef out of band</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/fork-new-processes.html">Anti-pattern: Forking new processes in application code</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/compiled-graph/ray-compiled-graph.html">Ray Compiled Graph (beta)</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/compiled-graph/quickstart.html">Quickstart</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/compiled-graph/profiling.html">Profiling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/compiled-graph/overlap.html">Experimental: Overlapping communication and computation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/compiled-graph/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/compiled-graph/compiled-graph-api.html">Compiled Graph API</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/advanced-topics.html">Advanced Topics</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/tips-for-first-time.html">Tips for first-time users</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/starting-ray.html">Starting Ray</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/ray-generator.html">Ray Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/namespaces.html">Using Namespaces</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/cross-language.html">Cross-Language Programming</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/using-ray-with-jupyter.html">Working with Jupyter Notebooks &amp; JupyterLab</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/ray-dag.html">Lazy Computation Graphs with the Ray DAG API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/miscellaneous.html">Miscellaneous Topics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/runtime_env_auth.html">Authenticating Remote URIs in runtime_env</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/user-spawn-processes.html">Lifetimes of a User-Spawn Process</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-core/examples/overview.html">Examples</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/automl_for_time_series.html">Simple AutoML for time series with Ray Core</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/batch_prediction.html">Batch Prediction with Ray Core</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/gentle_walkthrough.html">A Gentle Introduction to Ray Core by Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/highly_parallel.html">Using Ray for Highly Parallelizable Tasks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/map_reduce.html">A Simple MapReduce Example with Ray Core</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/monte_carlo_pi.html">Monte Carlo Estimation of Ï€</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/plot_hyperparameter.html">Simple Parallel Model Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/plot_parameter_server.html">Parameter Server</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/plot_pong_example.html">Learning to Play Pong</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/web-crawler.html">Speed up your web crawler by parallelizing it with Ray</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-core/api/index.html">Ray Core API</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/api/core.html">Core API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/api/scheduling.html">Scheduling API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/api/runtime-env.html">Runtime Env API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/api/utility.html">Utility</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/api/exceptions.html">Exceptions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/api/cli.html">Ray Core CLI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/reference/cli.html">State CLI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/reference/api.html">State API</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../data/data.html">Ray Data</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../data/quickstart.html">Ray Data Quickstart</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../data/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../data/user-guide.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/loading-data.html">Loading Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/inspecting-data.html">Inspecting Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/transforming-data.html">Transforming Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/iterating-over-data.html">Iterating over Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/shuffling-data.html">Shuffling Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/saving-data.html">Saving Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/working-with-images.html">Working with Images</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/working-with-text.html">Working with Text</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/working-with-tensors.html">Working with Tensors / NumPy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/working-with-pytorch.html">Working with PyTorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/working-with-llms.html">Working with LLMs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/monitoring-your-workload.html">Monitoring Your Workload</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/execution-configurations.html">Execution Configurations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/batch_inference.html">End-to-end: Offline Batch Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/performance-tips.html">Advanced: Performance Tips and Tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/custom-datasource-example.html">Advanced: Read and Write Custom File Types</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../data/examples.html">Examples</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../data/api/api.html">Ray Data API</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/input_output.html">Input/Output</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/dataset.html">Dataset API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/data_iterator.html">DataIterator API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/execution_options.html">ExecutionOptions API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/aggregate.html">Aggregation API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/grouped_data.html">GroupedData API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/data_context.html">Global configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/preprocessor.html">Preprocessor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/llm.html">Large Language Model (LLM) API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/from_other_data_libs.html">API Guide for Users from Other Data Libraries</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../data/comparisons.html">Comparing Ray Data to other systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../data/data-internals.html">Ray Data Internals</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../train/train.html">Ray Train</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/overview.html">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/getting-started-pytorch.html">PyTorch Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/getting-started-pytorch-lightning.html">PyTorch Lightning Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/getting-started-transformers.html">Hugging Face Transformers Guide</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../train/more-frameworks.html">More Frameworks</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/huggingface-accelerate.html">Hugging Face Accelerate Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/deepspeed.html">DeepSpeed Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/distributed-tensorflow-keras.html">TensorFlow and Keras Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/examples/xgboost/distributed-xgboost-lightgbm.html">XGBoost and LightGBM Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/horovod.html">Horovod Guide</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../train/user-guides.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/data-loading-preprocessing.html">Data Loading and Preprocessing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/using-gpus.html">Configuring Scale and GPUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/persistent-storage.html">Configuring Persistent Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/monitoring-logging.html">Monitoring and Logging Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/checkpoints.html">Saving and Loading Checkpoints</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/experiment-tracking.html">Experiment Tracking</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/results.html">Inspecting Training Results</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/fault-tolerance.html">Handling Failures and Node Preemption</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/reproducibility.html">Reproducibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/hyperparameter-optimization.html">Hyperparameter Optimization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/examples.html">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/benchmarks.html">Benchmarks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/api/api.html">Ray Train API</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../tune/index.html">Ray Tune</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-20"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../tune/getting-started.html">Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../tune/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../tune/tutorials/overview.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-21"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-run.html">Running Basic Experiments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-output.html">Logging and Outputs in Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-resources.html">Setting Trial Resources</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-search-spaces.html">Using Search Spaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-stopping.html">How to Define Stopping Criteria for a Ray Tune Experiment</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-trial-checkpoints.html">How to Save and Load Trial Checkpoints</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-storage.html">How to Configure Persistent Storage in Ray Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-fault-tolerance.html">How to Enable Fault Tolerance in Ray Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-metrics.html">Using Callbacks and Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune_get_data_in_and_out.html">Getting Data in and out of Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/tune_analyze_results.html">Analyzing Tune Experiment Results</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../tune/examples/pbt_guide.html">A Guide to Population Based Training with Tune</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-22"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/pbt_visualization/pbt_visualization.html">Visualizing and Understanding PBT</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-distributed.html">Deploying Tune in the Cloud</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-lifecycle.html">Tune Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-scalability.html">Scalability Benchmarks</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../tune/examples/index.html">Ray Tune Examples</a><input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-23"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../tune/examples/ml-frameworks.html">Examples using Ray Tune with ML Frameworks</a><input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-24"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/tune-pytorch-cifar.html">PyTorch Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/tune-pytorch-lightning.html">PyTorch Lightning Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/tune-xgboost.html">XGBoost Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/lightgbm_example.html">LightGBM Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/pbt_transformers.html">Hugging Face Transformers Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/pbt_ppo_example.html">Ray RLlib Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/tune_mnist_keras.html">Keras Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/horovod_simple.html">Horovod Example</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../tune/examples/experiment-tracking.html">Tune Experiment Tracking Examples</a><input class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-25"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/tune-wandb.html">Weights &amp; Biases Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/tune-mlflow.html">MLflow Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/tune-aim.html">Aim Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/tune-comet.html">Comet Example</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../tune/examples/hpo-frameworks.html">Tune Hyperparameter Optimization Framework Examples</a><input class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-26"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/ax_example.html">Ax Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/hyperopt_example.html">HyperOpt Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/bayesopt_example.html">Bayesopt Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/bohb_example.html">BOHB Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/nevergrad_example.html">Nevergrad Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/optuna_example.html">Optuna Example</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/other-examples.html">Other Examples</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/exercises.html">Exercises</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../tune/faq.html">Ray Tune FAQ</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../tune/api/api.html">Ray Tune API</a><input class="toctree-checkbox" id="toctree-checkbox-27" name="toctree-checkbox-27" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-27"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/execution.html">Tune Execution (tune.Tuner)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/result_grid.html">Tune Experiment Results (tune.ResultGrid)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/trainable.html">Training in Tune (tune.Trainable, tune.report)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/search_space.html">Tune Search Space API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/suggestion.html">Tune Search Algorithms (tune.search)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/schedulers.html">Tune Trial Schedulers (tune.schedulers)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/stoppers.html">Tune Stopping Mechanisms (tune.stopper)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/reporters.html">Tune Console Output (Reporters)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/syncing.html">Syncing in Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/logging.html">Tune Loggers (tune.logger)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/callbacks.html">Tune Callbacks (tune.Callback)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/env.html">Environment variables used by Ray Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/integration.html">External library integrations for Ray Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/internals.html">Tune Internals</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/cli.html">Tune CLI (Experimental)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../serve/index.html">Ray Serve</a><input class="toctree-checkbox" id="toctree-checkbox-28" name="toctree-checkbox-28" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-28"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/getting_started.html">Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/develop-and-deploy.html">Develop and Deploy an ML Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/model_composition.html">Deploy Compositions of Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/multi-app.html">Deploy Multiple Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/model-multiplexing.html">Model Multiplexing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/configure-serve-deployment.html">Configure Ray Serve deployments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/http-guide.html">Set Up FastAPI and HTTP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/llm/serving-llms.html">Serving LLMs</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../serve/production-guide/index.html">Production Guide</a><input class="toctree-checkbox" id="toctree-checkbox-29" name="toctree-checkbox-29" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-29"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/production-guide/config.html">Serve Config Files</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/production-guide/kubernetes.html">Deploy on Kubernetes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/production-guide/docker.html">Custom Docker Images</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/production-guide/fault-tolerance.html">Add End-to-End Fault Tolerance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/production-guide/handling-dependencies.html">Handle Dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/production-guide/best-practices.html">Best practices in production</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/monitoring.html">Monitor Your Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/resource-allocation.html">Resource Allocation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/autoscaling-guide.html">Ray Serve Autoscaling</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../serve/advanced-guides/index.html">Advanced Guides</a><input class="toctree-checkbox" id="toctree-checkbox-30" name="toctree-checkbox-30" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-30"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/app-builder-guide.html">Pass Arguments to Applications</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/advanced-autoscaling.html">Advanced Ray Serve Autoscaling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/performance.html">Performance Tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/dyn-req-batch.html">Dynamic Request Batching</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/inplace-updates.html">Updating Applications In-Place</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/dev-workflow.html">Development Workflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/grpc-guide.html">Set Up a gRPC Service</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/managing-java-deployments.html">Experimental Java API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/deploy-vm.html">Deploy on VM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/multi-app-container.html">Run Multiple Applications in Different Containers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/architecture.html">Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/examples.html">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/api/index.html">Ray Serve API</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../rllib/index.html">Ray RLlib</a><input class="toctree-checkbox" id="toctree-checkbox-31" name="toctree-checkbox-31" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-31"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../rllib/getting-started.html">Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../rllib/key-concepts.html">Key concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../rllib/rllib-env.html">Environments</a><input class="toctree-checkbox" id="toctree-checkbox-32" name="toctree-checkbox-32" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-32"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/multi-agent-envs.html">Multi-Agent Environments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/hierarchical-envs.html">Hierarchical Environments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/external-envs.html">External Environments and Applications</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../rllib/algorithm-config.html">AlgorithmConfig API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../rllib/rllib-algorithms.html">Algorithms</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../rllib/user-guides.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-33" name="toctree-checkbox-33" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-33"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-advanced-api.html">Advanced Python APIs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-callback.html">Callbacks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/checkpoints.html">Checkpointing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/metrics-logger.html">MetricsLogger API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/single-agent-episode.html">Episodes</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-replay-buffers.html">Replay Buffers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-offline.html">Working with offline data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rl-modules.html">RL Modules</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-learner.html">Learner (Alpha)</a></li>



<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-torch2x.html">Using RLlib with torch 2.x compile</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-fault-tolerance.html">Fault Tolerance And Elastic Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-dev.html">Install RLlib for Development</a></li>




<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/scaling-guide.html">RLlib scaling guide</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../rllib/rllib-examples.html">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../rllib/new-api-stack-migration-guide.html">New API stack migration guide</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../rllib/package_ref/index.html">Ray RLlib API</a><input class="toctree-checkbox" id="toctree-checkbox-34" name="toctree-checkbox-34" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-34"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/algorithm-config.html">Algorithm Configuration API</a><input class="toctree-checkbox" id="toctree-checkbox-35" name="toctree-checkbox-35" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-35"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_algo.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_algo</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner_group.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner_group</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.is_multi_agent.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.is_multi_agent</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.is_offline.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.is_offline</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learner_class.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learner_class</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.model_config.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.model_config</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.rl_module_spec.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.rl_module_spec</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.total_train_batch_size.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.total_train_batch_size</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_learner_class.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_learner_class</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_rl_module_spec.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_rl_module_spec</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_evaluation_config_object.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_evaluation_config_object</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_multi_rl_module_spec.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_multi_rl_module_spec</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_multi_agent_setup.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_multi_agent_setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_rollout_fragment_length.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_rollout_fragment_length</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.copy.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.copy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.validate.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.validate</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.freeze.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.freeze</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/algorithm.html">Algorithms</a><input class="toctree-checkbox" id="toctree-checkbox-36" name="toctree-checkbox-36" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-36"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.html">ray.rllib.algorithms.algorithm.Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.setup.html">ray.rllib.algorithms.algorithm.Algorithm.setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_default_config.html">ray.rllib.algorithms.algorithm.Algorithm.get_default_config</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.env_runner.html">ray.rllib.algorithms.algorithm.Algorithm.env_runner</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.eval_env_runner.html">ray.rllib.algorithms.algorithm.Algorithm.eval_env_runner</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.train.html">ray.rllib.algorithms.algorithm.Algorithm.train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.training_step.html">ray.rllib.algorithms.algorithm.Algorithm.training_step</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.save_to_path.html">ray.rllib.algorithms.algorithm.Algorithm.save_to_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.restore_from_path.html">ray.rllib.algorithms.algorithm.Algorithm.restore_from_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.from_checkpoint.html">ray.rllib.algorithms.algorithm.Algorithm.from_checkpoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_state.html">ray.rllib.algorithms.algorithm.Algorithm.get_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.set_state.html">ray.rllib.algorithms.algorithm.Algorithm.set_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.evaluate.html">ray.rllib.algorithms.algorithm.Algorithm.evaluate</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_module.html">ray.rllib.algorithms.algorithm.Algorithm.get_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.add_policy.html">ray.rllib.algorithms.algorithm.Algorithm.add_policy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.remove_policy.html">ray.rllib.algorithms.algorithm.Algorithm.remove_policy</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/callback.html">Callback APIs</a><input class="toctree-checkbox" id="toctree-checkbox-37" name="toctree-checkbox-37" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-37"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.html">ray.rllib.callbacks.callbacks.RLlibCallback</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_algorithm_init.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_algorithm_init</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_sample_end.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_sample_end</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_train_result.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_train_result</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_evaluate_start.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_evaluate_start</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_evaluate_end.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_evaluate_end</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_env_runners_recreated.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_env_runners_recreated</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_checkpoint_loaded.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_checkpoint_loaded</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_environment_created.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_environment_created</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_created.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_created</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_start.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_start</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_step.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_step</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_end.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_end</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/env.html">Environments</a><input class="toctree-checkbox" id="toctree-checkbox-38" name="toctree-checkbox-38" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-38"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/env/env_runner.html">EnvRunner API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/env/single_agent_env_runner.html">SingleAgentEnvRunner API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/env/single_agent_episode.html">SingleAgentEpisode API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/env/multi_agent_env.html">MultiAgentEnv API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/env/multi_agent_env_runner.html">MultiAgentEnvRunner API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/env/multi_agent_episode.html">MultiAgentEpisode API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/env/utils.html">Env Utils</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/rl_modules.html">RLModule APIs</a><input class="toctree-checkbox" id="toctree-checkbox-39" name="toctree-checkbox-39" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-39"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.build.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.build</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.module_class.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.module_class</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.observation_space.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.observation_space</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.action_space.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.action_space</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.inference_only.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.inference_only</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.learner_only.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.learner_only</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.model_config.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.model_config</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec.build.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec.build</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.default_model_config.DefaultModelConfig.html">ray.rllib.core.rl_module.default_model_config.DefaultModelConfig</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.html">ray.rllib.core.rl_module.rl_module.RLModule</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.observation_space.html">ray.rllib.core.rl_module.rl_module.RLModule.observation_space</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.action_space.html">ray.rllib.core.rl_module.rl_module.RLModule.action_space</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.inference_only.html">ray.rllib.core.rl_module.rl_module.RLModule.inference_only</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.model_config.html">ray.rllib.core.rl_module.rl_module.RLModule.model_config</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.setup.html">ray.rllib.core.rl_module.rl_module.RLModule.setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.as_multi_rl_module.html">ray.rllib.core.rl_module.rl_module.RLModule.as_multi_rl_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.forward_exploration.html">ray.rllib.core.rl_module.rl_module.RLModule.forward_exploration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.forward_inference.html">ray.rllib.core.rl_module.rl_module.RLModule.forward_inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.forward_train.html">ray.rllib.core.rl_module.rl_module.RLModule.forward_train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule._forward.html">ray.rllib.core.rl_module.rl_module.RLModule._forward</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule._forward_exploration.html">ray.rllib.core.rl_module.rl_module.RLModule._forward_exploration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule._forward_inference.html">ray.rllib.core.rl_module.rl_module.RLModule._forward_inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule._forward_train.html">ray.rllib.core.rl_module.rl_module.RLModule._forward_train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.save_to_path.html">ray.rllib.core.rl_module.rl_module.RLModule.save_to_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.restore_from_path.html">ray.rllib.core.rl_module.rl_module.RLModule.restore_from_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.from_checkpoint.html">ray.rllib.core.rl_module.rl_module.RLModule.from_checkpoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.get_state.html">ray.rllib.core.rl_module.rl_module.RLModule.get_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.set_state.html">ray.rllib.core.rl_module.rl_module.RLModule.set_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.setup.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.as_multi_rl_module.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.as_multi_rl_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.add_module.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.add_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.remove_module.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.remove_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.save_to_path.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.save_to_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.restore_from_path.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.restore_from_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.from_checkpoint.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.from_checkpoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.get_state.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.get_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.set_state.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.set_state</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/distributions.html">Distribution API</a><input class="toctree-checkbox" id="toctree-checkbox-40" name="toctree-checkbox-40" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-40"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.distributions.Distribution.html">ray.rllib.models.distributions.Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.distributions.Distribution.from_logits.html">ray.rllib.models.distributions.Distribution.from_logits</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.distributions.Distribution.sample.html">ray.rllib.models.distributions.Distribution.sample</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.distributions.Distribution.rsample.html">ray.rllib.models.distributions.Distribution.rsample</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.distributions.Distribution.logp.html">ray.rllib.models.distributions.Distribution.logp</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.distributions.Distribution.kl.html">ray.rllib.models.distributions.Distribution.kl</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/learner.html">LearnerGroup API</a><input class="toctree-checkbox" id="toctree-checkbox-41" name="toctree-checkbox-41" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-41"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learners.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learners</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner_group.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner_group</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner_group.LearnerGroup.html">ray.rllib.core.learner.learner_group.LearnerGroup</a></li>
</ul>
</li>

<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/offline.html">Offline RL API</a><input class="toctree-checkbox" id="toctree-checkbox-42" name="toctree-checkbox-42" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-42"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.offline_data.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.offline_data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learners.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learners</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.env_runners.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.env_runners</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner.html">ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_data.OfflineData.html">ray.rllib.offline.offline_data.OfflineData</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_data.OfflineData.__init__.html">ray.rllib.offline.offline_data.OfflineData.__init__</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_data.OfflineData.sample.html">ray.rllib.offline.offline_data.OfflineData.sample</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_data.OfflineData.default_map_batches_kwargs.html">ray.rllib.offline.offline_data.OfflineData.default_map_batches_kwargs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_data.OfflineData.default_iter_batches_kwargs.html">ray.rllib.offline.offline_data.OfflineData.default_iter_batches_kwargs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.__init__.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner.__init__</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.SCHEMA.html">ray.rllib.offline.offline_prelearner.SCHEMA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.__call__.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner.__call__</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner._map_to_episodes.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner._map_to_episodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner._map_sample_batch_to_episode.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner._map_sample_batch_to_episode</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner._should_module_be_updated.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner._should_module_be_updated</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.default_prelearner_buffer_class.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner.default_prelearner_buffer_class</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.default_prelearner_buffer_kwargs.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner.default_prelearner_buffer_kwargs</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/replay-buffers.html">Replay Buffer API</a><input class="toctree-checkbox" id="toctree-checkbox-43" name="toctree-checkbox-43" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-43"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.StorageUnit.html">ray.rllib.utils.replay_buffers.replay_buffer.StorageUnit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.prioritized_replay_buffer.PrioritizedReplayBuffer.html">ray.rllib.utils.replay_buffers.prioritized_replay_buffer.PrioritizedReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.reservoir_replay_buffer.ReservoirReplayBuffer.html">ray.rllib.utils.replay_buffers.reservoir_replay_buffer.ReservoirReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.sample.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.sample</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.add.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.add</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.get_state.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.get_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.set_state.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.set_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.multi_agent_replay_buffer.MultiAgentReplayBuffer.html">ray.rllib.utils.replay_buffers.multi_agent_replay_buffer.MultiAgentReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.multi_agent_prioritized_replay_buffer.MultiAgentPrioritizedReplayBuffer.html">ray.rllib.utils.replay_buffers.multi_agent_prioritized_replay_buffer.MultiAgentPrioritizedReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.utils.update_priorities_in_replay_buffer.html">ray.rllib.utils.replay_buffers.utils.update_priorities_in_replay_buffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.utils.sample_min_n_steps_from_buffer.html">ray.rllib.utils.replay_buffers.utils.sample_min_n_steps_from_buffer</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/utils.html">RLlib Utilities</a><input class="toctree-checkbox" id="toctree-checkbox-44" name="toctree-checkbox-44" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-44"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.peek.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger.peek</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_value.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_value</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_dict.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_dict</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.merge_and_log_n_dicts.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger.merge_and_log_n_dicts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_time.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_time</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.schedules.scheduler.Scheduler.html">ray.rllib.utils.schedules.scheduler.Scheduler</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.schedules.scheduler.Scheduler.validate.html">ray.rllib.utils.schedules.scheduler.Scheduler.validate</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.schedules.scheduler.Scheduler.get_current_value.html">ray.rllib.utils.schedules.scheduler.Scheduler.get_current_value</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.schedules.scheduler.Scheduler.update.html">ray.rllib.utils.schedules.scheduler.Scheduler.update</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.schedules.scheduler.Scheduler._create_tensor_variable.html">ray.rllib.utils.schedules.scheduler.Scheduler._create_tensor_variable</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.framework.try_import_torch.html">ray.rllib.utils.framework.try_import_torch</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.clip_gradients.html">ray.rllib.utils.torch_utils.clip_gradients</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.compute_global_norm.html">ray.rllib.utils.torch_utils.compute_global_norm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.convert_to_torch_tensor.html">ray.rllib.utils.torch_utils.convert_to_torch_tensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.explained_variance.html">ray.rllib.utils.torch_utils.explained_variance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.flatten_inputs_to_1d_tensor.html">ray.rllib.utils.torch_utils.flatten_inputs_to_1d_tensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.global_norm.html">ray.rllib.utils.torch_utils.global_norm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.one_hot.html">ray.rllib.utils.torch_utils.one_hot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.reduce_mean_ignore_inf.html">ray.rllib.utils.torch_utils.reduce_mean_ignore_inf</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.sequence_mask.html">ray.rllib.utils.torch_utils.sequence_mask</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.set_torch_seed.html">ray.rllib.utils.torch_utils.set_torch_seed</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.softmax_cross_entropy_with_logits.html">ray.rllib.utils.torch_utils.softmax_cross_entropy_with_logits</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.update_target_network.html">ray.rllib.utils.torch_utils.update_target_network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.aligned_array.html">ray.rllib.utils.numpy.aligned_array</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.concat_aligned.html">ray.rllib.utils.numpy.concat_aligned</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.convert_to_numpy.html">ray.rllib.utils.numpy.convert_to_numpy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.fc.html">ray.rllib.utils.numpy.fc</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.flatten_inputs_to_1d_tensor.html">ray.rllib.utils.numpy.flatten_inputs_to_1d_tensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.make_action_immutable.html">ray.rllib.utils.numpy.make_action_immutable</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.huber_loss.html">ray.rllib.utils.numpy.huber_loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.l2_loss.html">ray.rllib.utils.numpy.l2_loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.lstm.html">ray.rllib.utils.numpy.lstm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.one_hot.html">ray.rllib.utils.numpy.one_hot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.relu.html">ray.rllib.utils.numpy.relu</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.sigmoid.html">ray.rllib.utils.numpy.sigmoid</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.softmax.html">ray.rllib.utils.numpy.softmax</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.checkpoints.try_import_msgpack.html">ray.rllib.utils.checkpoints.try_import_msgpack</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.checkpoints.Checkpointable.html">ray.rllib.utils.checkpoints.Checkpointable</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../ray-more-libs/index.html">More Libraries</a><input class="toctree-checkbox" id="toctree-checkbox-45" name="toctree-checkbox-45" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-45"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-more-libs/joblib.html">Distributed Scikit-learn / Joblib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-more-libs/multiprocessing.html">Distributed multiprocessing.Pool</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-more-libs/ray-collective.html">Ray Collective Communication Lib</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-more-libs/dask-on-ray.html">Using Dask on Ray</a><input class="toctree-checkbox" id="toctree-checkbox-46" name="toctree-checkbox-46" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-46"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.RayDaskCallback.html">ray.util.dask.RayDaskCallback</a><input class="toctree-checkbox" id="toctree-checkbox-47" name="toctree-checkbox-47" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-47"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.RayDaskCallback.ray_active.html">ray.util.dask.RayDaskCallback.ray_active</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_presubmit.html">ray.util.dask.callbacks.RayDaskCallback._ray_presubmit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_postsubmit.html">ray.util.dask.callbacks.RayDaskCallback._ray_postsubmit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_pretask.html">ray.util.dask.callbacks.RayDaskCallback._ray_pretask</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_posttask.html">ray.util.dask.callbacks.RayDaskCallback._ray_posttask</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_postsubmit_all.html">ray.util.dask.callbacks.RayDaskCallback._ray_postsubmit_all</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_finish.html">ray.util.dask.callbacks.RayDaskCallback._ray_finish</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-more-libs/raydp.html">Using Spark on Ray (RayDP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-more-libs/mars-on-ray.html">Using Mars on Ray</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-more-libs/modin/index.html">Using Pandas on Ray (Modin)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../workflows/index.html">Ray Workflows (Deprecated)</a><input class="toctree-checkbox" id="toctree-checkbox-48" name="toctree-checkbox-48" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-48"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/basics.html">Getting Started</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/management.html">Workflow Management</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/metadata.html">Workflow Metadata</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/events.html">Events</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/comparison.html">API Comparisons</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/advanced.html">Advanced Topics</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../workflows/api/api.html">Ray Workflows API</a><input class="toctree-checkbox" id="toctree-checkbox-49" name="toctree-checkbox-49" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-49"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../workflows/api/execution.html">Workflow Execution API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../workflows/api/management.html">Workflow Management API</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../cluster/getting-started.html">Ray Clusters</a><input class="toctree-checkbox" id="toctree-checkbox-50" name="toctree-checkbox-50" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-50"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../cluster/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/index.html">Deploying on Kubernetes</a><input class="toctree-checkbox" id="toctree-checkbox-51" name="toctree-checkbox-51" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-51"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/getting-started.html">Getting Started with KubeRay</a><input class="toctree-checkbox" id="toctree-checkbox-52" name="toctree-checkbox-52" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-52"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/getting-started/raycluster-quick-start.html">RayCluster Quickstart</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/getting-started/rayjob-quick-start.html">RayJob Quickstart</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/getting-started/rayservice-quick-start.html">RayService Quickstart</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-53" name="toctree-checkbox-53" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-53"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/rayservice.html">Deploy Ray Serve Apps</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/rayservice-high-availability.html">RayService high availability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/observability.html">KubeRay Observability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/upgrade-guide.html">KubeRay upgrade guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/k8s-cluster-setup.html">Managed Kubernetes services</a></li>



<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/storage.html">Best Practices for Storage and Dependencies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/config.html">RayCluster Configuration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/configuring-autoscaling.html">KubeRay Autoscaling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/kuberay-gcs-ft.html">GCS fault tolerance in KubeRay</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/kuberay-gcs-persistent-ft.html">Tuning Redis for a Persistent Fault Tolerant GCS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/gke-gcs-bucket.html">Configuring KubeRay to use Google Cloud Storage Buckets in GKE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/persist-kuberay-custom-resource-logs.html">Persist KubeRay custom resource logs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/persist-kuberay-operator-logs.html">Persist KubeRay Operator Logs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/gpu.html">Using GPUs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/tpu.html">Use TPUs with KubeRay</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/rayserve-dev-doc.html">Developing Ray Serve Python scripts on a RayCluster</a></li>









<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/pod-command.html">Specify container commands for Ray head/worker Pods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/helm-chart-rbac.html">Helm Chart RBAC</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/tls.html">TLS Authentication</a></li>






<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/k8s-autoscaler.html">(Advanced) Understanding the Ray Autoscaler in the Context of Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.html">(Advanced) Deploying a static Ray cluster without KubeRay</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/kubectl-plugin.html">Use kubectl plugin (beta)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/kuberay-auth.html">Configure Ray clusters with authentication and access control using KubeRay</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/reduce-image-pull-latency.html">Reducing image pull latency on Kubernetes</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/examples.html">Examples</a><input class="toctree-checkbox" id="toctree-checkbox-54" name="toctree-checkbox-54" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-54"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/ml-example.html">Ray Train XGBoostTrainer on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/gpu-training-example.html">Train PyTorch ResNet model with GPUs on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/mnist-training-example.html">Train a PyTorch model on Fashion MNIST with CPUs on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/stable-diffusion-rayservice.html">Serve a StableDiffusion text-to-image model on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/tpu-serve-stable-diffusion.html">Serve a Stable Diffusion model on GKE with TPUs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/mobilenet-rayservice.html">Serve a MobileNet image classifier on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/text-summarizer-rayservice.html">Serve a text summarizer on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/rayjob-batch-inference-example.html">RayJob Batch Inference Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/rayjob-kueue-priority-scheduling.html">Priority Scheduling with RayJob and Kueue</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/rayjob-kueue-gang-scheduling.html">Gang Scheduling with RayJob and Kueue</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/distributed-checkpointing-with-gcsfuse.html">Distributed checkpointing with KubeRay and GCSFuse</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/modin-example.html">Use Modin with Ray on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/vllm-rayservice.html">Serve a Large Language Model with vLLM on Kubernetes</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem.html">KubeRay Ecosystem</a><input class="toctree-checkbox" id="toctree-checkbox-55" name="toctree-checkbox-55" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-55"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/ingress.html">Ingress</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/prometheus-grafana.html">Using Prometheus and Grafana</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/pyspy.html">Profiling with py-spy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/volcano.html">KubeRay integration with Volcano</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/yunikorn.html">KubeRay integration with Apache YuniKorn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/kueue.html">Gang scheduling and priority scheduling for RayJob with Kueue</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/istio.html">mTLS and L7 observability with Istio</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/benchmarks.html">KubeRay Benchmarks</a><input class="toctree-checkbox" id="toctree-checkbox-56" name="toctree-checkbox-56" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-56"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/benchmarks/memory-scalability-benchmark.html">KubeRay memory and scalability benchmark</a></li>

</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/troubleshooting.html">KubeRay Troubleshooting</a><input class="toctree-checkbox" id="toctree-checkbox-57" name="toctree-checkbox-57" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-57"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/troubleshooting/troubleshooting.html">Troubleshooting guide</a></li>

<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/troubleshooting/rayservice-troubleshooting.html">RayService troubleshooting</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/kubernetes/references.html">API Reference</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../cluster/vms/index.html">Deploying on VMs</a><input class="toctree-checkbox" id="toctree-checkbox-58" name="toctree-checkbox-58" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-58"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/vms/getting-started.html">Getting Started</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/vms/user-guides/index.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-59" name="toctree-checkbox-59" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-59"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/user-guides/launching-clusters/index.html">Launching Ray Clusters on AWS, GCP, Azure, vSphere, On-Prem</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/user-guides/large-cluster-best-practices.html">Best practices for deploying large clusters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/user-guides/configuring-autoscaling.html">Configuring Autoscaling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/user-guides/logging.html">Log Persistence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/user-guides/community/index.html">Community Supported Cluster Managers</a></li>

</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/vms/examples/index.html">Examples</a><input class="toctree-checkbox" id="toctree-checkbox-60" name="toctree-checkbox-60" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-60"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/examples/ml-example.html">Ray Train XGBoostTrainer on VMs</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/vms/references/index.html">API References</a><input class="toctree-checkbox" id="toctree-checkbox-61" name="toctree-checkbox-61" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-61"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/references/ray-cluster-cli.html">Cluster Launcher Commands</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/references/ray-cluster-configuration.html">Cluster YAML Configuration Options</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../cluster/metrics.html">Collecting and monitoring metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../cluster/configure-manage-dashboard.html">Configuring and Managing Ray Dashboard</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../cluster/running-applications/index.html">Applications Guide</a><input class="toctree-checkbox" id="toctree-checkbox-62" name="toctree-checkbox-62" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-62"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/index.html">Ray Jobs Overview</a><input class="toctree-checkbox" id="toctree-checkbox-63" name="toctree-checkbox-63" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-63"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/quickstart.html">Quickstart using the Ray Jobs CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/sdk.html">Python SDK Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/jobs-package-ref.html">Python SDK API Reference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/cli.html">Ray Jobs CLI API Reference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/rest.html">Ray Jobs REST API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/ray-client.html">Ray Client</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/running-applications/autoscaling/reference.html">Programmatic Cluster Scaling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../cluster/faq.html">FAQ</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../cluster/package-overview.html">Ray Cluster Management API</a><input class="toctree-checkbox" id="toctree-checkbox-64" name="toctree-checkbox-64" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-64"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/cli.html">Cluster Management CLI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/jobs-package-ref.html">Python SDK API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/cli.html">Ray Jobs CLI API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/running-applications/autoscaling/reference.html">Programmatic Cluster Scaling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../cluster/usage-stats.html">Usage Stats Collection</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../virtual-cluster/getting-started.html">Ray Virtual Clusters</a><input class="toctree-checkbox" id="toctree-checkbox-65" name="toctree-checkbox-65" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-65"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../virtual-cluster/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../virtual-cluster/design-overview.html">Design Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../virtual-cluster/management.html">Virtual Cluster Management API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../virtual-cluster/cli.html">Virtual Cluster CLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../virtual-cluster/examples.html">Examples</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../ray-observability/index.html">Monitoring and Debugging</a><input class="toctree-checkbox" id="toctree-checkbox-66" name="toctree-checkbox-66" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-66"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-observability/getting-started.html">Ray Dashboard</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-observability/ray-distributed-debugger.html">Ray Distributed Debugger</a></li>



<li class="toctree-l2"><a class="reference internal" href="../../../../ray-observability/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-observability/user-guides/index.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-67" name="toctree-checkbox-67" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-67"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/index.html">Debugging Applications</a><input class="toctree-checkbox" id="toctree-checkbox-68" name="toctree-checkbox-68" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-68"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/general-debugging.html">General Debugging</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/debug-memory.html">Debugging Memory Issues</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/debug-hangs.html">Debugging Hangs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/debug-failures.html">Debugging Failures</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/optimize-performance.html">Optimizing Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-observability/ray-distributed-debugger.html">Ray Distributed Debugger</a></li>



<li class="toctree-l4"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/ray-debugging.html">Using the Ray Debugger</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/user-guides/cli-sdk.html">Monitoring with the CLI or SDK</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/user-guides/configure-logging.html">Configuring Logging</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/user-guides/profiling.html">Profiling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/user-guides/add-app-metrics.html">Adding Application-Level Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/user-guides/ray-tracing.html">Tracing</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-observability/reference/index.html">Reference</a><input class="toctree-checkbox" id="toctree-checkbox-69" name="toctree-checkbox-69" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-69"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/reference/api.html">State API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/reference/cli.html">State CLI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/reference/system-metrics.html">System Metrics</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../ray-contribute/index.html">Developer Guides</a><input class="toctree-checkbox" id="toctree-checkbox-70" name="toctree-checkbox-70" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-70"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-contribute/stability.html">API Stability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-contribute/api-policy.html">API Policy</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-contribute/getting-involved.html">Getting Involved / Contributing</a><input class="toctree-checkbox" id="toctree-checkbox-71" name="toctree-checkbox-71" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-71"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/development.html">Building Ray from Source</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/ci.html">CI Testing Workflow on PRs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/docs.html">Contributing to the Ray Documentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/writing-code-snippets.html">How to write code snippets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/fake-autoscaler.html">Testing Autoscaling Locally</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/testing-tips.html">Tips for testing Ray programs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/debugging.html">Debugging for Ray Developers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/profiling.html">Profiling for Ray Developers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-core/configure.html">Configuring Ray</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-contribute/whitepaper.html">Architecture Whitepapers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ray-references/glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ray-security/index.html">Security</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../../index.html" class="nav-link">Module code</a></li>
    
    
    <li class="breadcrumb-item"><a href="../../util.html" class="nav-link">ray.util</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">ray.util.spa...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <h1>Source code for ray.util.spark.cluster_init</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">signal</span>

<span class="kn">import</span> <span class="nn">yaml</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">socket</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">uuid</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">from</span> <span class="nn">packaging.version</span> <span class="kn">import</span> <span class="n">Version</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Type</span>

<span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">import</span> <span class="nn">ray._private.services</span>
<span class="kn">from</span> <span class="nn">ray.autoscaler._private.spark.node_provider</span> <span class="kn">import</span> <span class="n">HEAD_NODE_ID</span>
<span class="kn">from</span> <span class="nn">ray.util.annotations</span> <span class="kn">import</span> <span class="n">DeveloperAPI</span><span class="p">,</span> <span class="n">PublicAPI</span>
<span class="kn">from</span> <span class="nn">ray._private.utils</span> <span class="kn">import</span> <span class="n">load_class</span>

<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">exec_cmd</span><span class="p">,</span>
    <span class="n">is_port_in_use</span><span class="p">,</span>
    <span class="n">get_random_unused_port</span><span class="p">,</span>
    <span class="n">get_spark_session</span><span class="p">,</span>
    <span class="n">get_spark_application_driver_host</span><span class="p">,</span>
    <span class="n">is_in_databricks_runtime</span><span class="p">,</span>
    <span class="n">get_spark_task_assigned_physical_gpus</span><span class="p">,</span>
    <span class="n">get_avail_mem_per_ray_worker_node</span><span class="p">,</span>
    <span class="n">get_max_num_concurrent_tasks</span><span class="p">,</span>
    <span class="n">gen_cmd_exec_failure_msg</span><span class="p">,</span>
    <span class="n">calc_mem_ray_head_node</span><span class="p">,</span>
    <span class="n">_wait_service_up</span><span class="p">,</span>
    <span class="n">_get_local_ray_node_slots</span><span class="p">,</span>
    <span class="n">get_configured_spark_executor_memory_bytes</span><span class="p">,</span>
    <span class="n">_get_cpu_cores</span><span class="p">,</span>
    <span class="n">_get_num_physical_gpus</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">.start_hook_base</span> <span class="kn">import</span> <span class="n">RayOnSparkStartHook</span>
<span class="kn">from</span> <span class="nn">.databricks_hook</span> <span class="kn">import</span> <span class="n">DefaultDatabricksRayOnSparkStartHook</span>
<span class="kn">from</span> <span class="nn">threading</span> <span class="kn">import</span> <span class="n">Event</span>


<span class="n">_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;ray.util.spark&quot;</span><span class="p">)</span>
<span class="n">_logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>


<span class="n">RAY_ON_SPARK_START_HOOK</span> <span class="o">=</span> <span class="s2">&quot;RAY_ON_SPARK_START_HOOK&quot;</span>

<span class="n">MAX_NUM_WORKER_NODES</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

<span class="n">RAY_ON_SPARK_COLLECT_LOG_TO_PATH</span> <span class="o">=</span> <span class="s2">&quot;RAY_ON_SPARK_COLLECT_LOG_TO_PATH&quot;</span>
<span class="n">RAY_ON_SPARK_START_RAY_PARENT_PID</span> <span class="o">=</span> <span class="s2">&quot;RAY_ON_SPARK_START_RAY_PARENT_PID&quot;</span>


<span class="k">def</span> <span class="nf">_check_system_environment</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">name</span> <span class="o">!=</span> <span class="s2">&quot;posix&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Ray on spark only supports running on POSIX system.&quot;</span><span class="p">)</span>

    <span class="n">spark_dependency_error</span> <span class="o">=</span> <span class="s2">&quot;ray.util.spark module requires pyspark &gt;= 3.3&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="kn">import</span> <span class="nn">pyspark</span>

        <span class="k">if</span> <span class="n">Version</span><span class="p">(</span><span class="n">pyspark</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span><span class="o">.</span><span class="n">release</span> <span class="o">&lt;</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">spark_dependency_error</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">spark_dependency_error</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">RayClusterOnSpark</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class is the type of instance returned by the `_setup_ray_cluster` interface.</span>
<span class="sd">    Its main functionality is to:</span>
<span class="sd">    Connect to, disconnect from, and shutdown the Ray cluster running on Apache Spark.</span>
<span class="sd">    Serve as a Python context manager for the `RayClusterOnSpark` instance.</span>

<span class="sd">    Args</span>
<span class="sd">        address: The url for the ray head node (defined as the hostname and unused</span>
<span class="sd">                 port on Spark driver node)</span>
<span class="sd">        head_proc: Ray head process</span>
<span class="sd">        spark_job_group_id: The Spark job id for a submitted ray job</span>
<span class="sd">        num_workers_node: The number of workers in the ray cluster.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">address</span><span class="p">,</span>
        <span class="n">head_proc</span><span class="p">,</span>
        <span class="n">min_worker_nodes</span><span class="p">,</span>
        <span class="n">max_worker_nodes</span><span class="p">,</span>
        <span class="n">temp_dir</span><span class="p">,</span>
        <span class="n">cluster_unique_id</span><span class="p">,</span>
        <span class="n">start_hook</span><span class="p">,</span>
        <span class="n">ray_dashboard_port</span><span class="p">,</span>
        <span class="n">spark_job_server</span><span class="p">,</span>
        <span class="n">global_cluster_lock_fd</span><span class="p">,</span>
        <span class="n">ray_client_server_port</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">address</span> <span class="o">=</span> <span class="n">address</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_proc</span> <span class="o">=</span> <span class="n">head_proc</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_worker_nodes</span> <span class="o">=</span> <span class="n">min_worker_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_worker_nodes</span> <span class="o">=</span> <span class="n">max_worker_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">temp_dir</span> <span class="o">=</span> <span class="n">temp_dir</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cluster_unique_id</span> <span class="o">=</span> <span class="n">cluster_unique_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start_hook</span> <span class="o">=</span> <span class="n">start_hook</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ray_dashboard_port</span> <span class="o">=</span> <span class="n">ray_dashboard_port</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spark_job_server</span> <span class="o">=</span> <span class="n">spark_job_server</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">global_cluster_lock_fd</span> <span class="o">=</span> <span class="n">global_cluster_lock_fd</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ray_client_server_port</span> <span class="o">=</span> <span class="n">ray_client_server_port</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">is_shutdown</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spark_job_is_canceled</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">background_job_exception</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Ray client context returns by `ray.init`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ray_ctx</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">wait_until_ready</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="kn">import</span> <span class="nn">ray</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_shutdown</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;The ray cluster has been shut down or it failed to start.&quot;</span>
            <span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">ray</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">address</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">address</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ray_dashboard_port</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">_wait_service_up</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">address</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ray_dashboard_port</span><span class="p">,</span>
                <span class="n">_RAY_DASHBOARD_STARTUP_TIMEOUT</span><span class="p">,</span>
            <span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">start_hook</span><span class="o">.</span><span class="n">on_ray_dashboard_created</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ray_dashboard_port</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="nb">__import__</span><span class="p">(</span><span class="s2">&quot;ray.dashboard.optional_deps&quot;</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">ModuleNotFoundError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="s2">&quot;Dependencies to launch the optional dashboard API &quot;</span>
                        <span class="s2">&quot;server cannot be found. They can be installed with &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;pip install ray[default], root cause: (</span><span class="si">{</span><span class="nb">repr</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">)&quot;</span>
                    <span class="p">)</span>

            <span class="n">last_alive_worker_count</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">last_progress_move_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
            <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
                <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">_RAY_CLUSTER_STARTUP_PROGRESS_CHECKING_INTERVAL</span><span class="p">)</span>

                <span class="c1"># Inside the waiting ready loop,</span>
                <span class="c1"># checking `self.background_job_exception`, if it is not None,</span>
                <span class="c1"># it means the background spark job has failed,</span>
                <span class="c1"># in this case, raise error directly.</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">background_job_exception</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s2">&quot;Ray workers failed to start.&quot;</span>
                    <span class="p">)</span> <span class="kn">from</span> <span class="nn">self.background_job_exception</span>

                <span class="n">cur_alive_worker_count</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="nb">len</span><span class="p">([</span><span class="n">node</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">ray</span><span class="o">.</span><span class="n">nodes</span><span class="p">()</span> <span class="k">if</span> <span class="n">node</span><span class="p">[</span><span class="s2">&quot;Alive&quot;</span><span class="p">]])</span> <span class="o">-</span> <span class="mi">1</span>
                <span class="p">)</span>  <span class="c1"># Minus 1 means excluding the head node.</span>

                <span class="k">if</span> <span class="n">cur_alive_worker_count</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_worker_nodes</span><span class="p">:</span>
                    <span class="n">_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Started </span><span class="si">{</span><span class="n">cur_alive_worker_count</span><span class="si">}</span><span class="s2"> Ray worker nodes, &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;meet the minimum number of Ray worker nodes required.&quot;</span>
                    <span class="p">)</span>
                    <span class="k">return</span>

                <span class="k">if</span> <span class="n">cur_alive_worker_count</span> <span class="o">&gt;</span> <span class="n">last_alive_worker_count</span><span class="p">:</span>
                    <span class="n">last_alive_worker_count</span> <span class="o">=</span> <span class="n">cur_alive_worker_count</span>
                    <span class="n">last_progress_move_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
                    <span class="n">_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="s2">&quot;Ray worker nodes are starting. Progress: &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="n">cur_alive_worker_count</span><span class="si">}</span><span class="s2"> / </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">max_worker_nodes</span><span class="si">}</span><span class="s2">)&quot;</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="p">(</span>
                        <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">last_progress_move_time</span>
                        <span class="o">&gt;</span> <span class="n">_RAY_CONNECT_CLUSTER_POLL_PROGRESS_TIMEOUT</span>
                    <span class="p">):</span>
                        <span class="k">if</span> <span class="n">cur_alive_worker_count</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                            <span class="p">(</span>
                                <span class="n">job_server_host</span><span class="p">,</span>
                                <span class="n">job_server_port</span><span class="p">,</span>
                            <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">spark_job_server</span><span class="o">.</span><span class="n">server_address</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
                            <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
                                <span class="n">url</span><span class="o">=</span><span class="p">(</span>
                                    <span class="sa">f</span><span class="s2">&quot;http://</span><span class="si">{</span><span class="n">job_server_host</span><span class="si">}</span><span class="s2">:</span><span class="si">{</span><span class="n">job_server_port</span><span class="si">}</span><span class="s2">&quot;</span>
                                    <span class="s2">&quot;/query_last_worker_err&quot;</span>
                                <span class="p">),</span>
                                <span class="n">json</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;spark_job_group_id&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">},</span>
                            <span class="p">)</span>
                            <span class="n">response</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>

                            <span class="n">decoded_resp</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span>
                            <span class="n">json_res</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">decoded_resp</span><span class="p">)</span>
                            <span class="n">last_worker_err</span> <span class="o">=</span> <span class="n">json_res</span><span class="p">[</span><span class="s2">&quot;last_worker_err&quot;</span><span class="p">]</span>

                            <span class="k">if</span> <span class="n">last_worker_err</span><span class="p">:</span>
                                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                                    <span class="s2">&quot;Starting Ray worker node failed, error:</span><span class="se">\n</span><span class="s2">&quot;</span>
                                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">last_worker_err</span><span class="si">}</span><span class="s2">&quot;</span>
                                <span class="p">)</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                                    <span class="s2">&quot;Current spark cluster has no resources to launch &quot;</span>
                                    <span class="s2">&quot;Ray worker nodes.&quot;</span>
                                <span class="p">)</span>
                        <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                            <span class="s2">&quot;Timeout in waiting for minimal ray workers to start. &quot;</span>
                            <span class="s2">&quot;Started / Total requested: &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="n">cur_alive_worker_count</span><span class="si">}</span><span class="s2"> / </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">min_worker_nodes</span><span class="si">}</span><span class="s2">). &quot;</span>
                            <span class="s2">&quot;Current spark cluster does not have sufficient resources &quot;</span>
                            <span class="s2">&quot;to launch requested minimal number of Ray worker nodes.&quot;</span>
                        <span class="p">)</span>
                        <span class="k">return</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="n">ray</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">connect</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">ray</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Already connected to Ray cluster.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ray_ctx</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">address</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">address</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">disconnect</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">ray</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ray_ctx</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">shutdown</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Shutdown the ray cluster created by the `setup_ray_cluster` API.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">import</span> <span class="nn">fcntl</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_shutdown</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">disconnect</span><span class="p">()</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                <span class="k">pass</span>
            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;RAY_ADDRESS&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_cluster_lock_fd</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># release global mode cluster lock.</span>
                <span class="n">fcntl</span><span class="o">.</span><span class="n">flock</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">global_cluster_lock_fd</span><span class="p">,</span> <span class="n">fcntl</span><span class="o">.</span><span class="n">LOCK_UN</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">spark_job_server</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">head_proc</span><span class="o">.</span><span class="n">terminate</span><span class="p">()</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="c1"># swallow exception.</span>
                <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;An Error occurred during shutdown of ray head node: &quot;</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">repr</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_shutdown</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_val</span><span class="p">,</span> <span class="n">exc_tb</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_convert_ray_node_option</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="n">converted_key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;--</span><span class="si">{</span><span class="n">key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;-&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;system_config&quot;</span><span class="p">,</span> <span class="s2">&quot;resources&quot;</span><span class="p">,</span> <span class="s2">&quot;labels&quot;</span><span class="p">]:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">converted_key</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">converted_key</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">converted_key</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>


<span class="k">def</span> <span class="nf">_convert_ray_node_options</span><span class="p">(</span><span class="n">options</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">_convert_ray_node_option</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">options</span><span class="o">.</span><span class="n">items</span><span class="p">()]</span>


<span class="n">_RAY_HEAD_STARTUP_TIMEOUT</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">_RAY_DASHBOARD_STARTUP_TIMEOUT</span> <span class="o">=</span> <span class="mi">60</span>
<span class="n">_BACKGROUND_JOB_STARTUP_WAIT</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;RAY_ON_SPARK_BACKGROUND_JOB_STARTUP_WAIT&quot;</span><span class="p">,</span> <span class="s2">&quot;30&quot;</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">_RAY_CLUSTER_STARTUP_PROGRESS_CHECKING_INTERVAL</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">_RAY_WORKER_NODE_STARTUP_INTERVAL</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;RAY_ON_SPARK_RAY_WORKER_NODE_STARTUP_INTERVAL&quot;</span><span class="p">,</span> <span class="s2">&quot;10&quot;</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">_RAY_CONNECT_CLUSTER_POLL_PROGRESS_TIMEOUT</span> <span class="o">=</span> <span class="mi">120</span>


<span class="k">def</span> <span class="nf">_preallocate_ray_worker_port_range</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    If we start multiple ray workers on a machine concurrently, some ray worker</span>
<span class="sd">    processes might fail due to ray port conflicts, this is because race condition</span>
<span class="sd">    on getting free port and opening the free port.</span>
<span class="sd">    To address the issue, this function use an exclusive file lock to delay the</span>
<span class="sd">    worker processes to ensure that port acquisition does not create a resource</span>
<span class="sd">    contention issue due to a race condition.</span>

<span class="sd">    After acquiring lock, it will allocate port range for worker ports</span>
<span class="sd">    (for ray node config --min-worker-port and --max-worker-port).</span>
<span class="sd">    Because on a spark cluster, multiple ray cluster might be created, so on one spark</span>
<span class="sd">    worker machine, there might be multiple ray worker nodes running, these worker</span>
<span class="sd">    nodes might belong to different ray cluster, and we must ensure these ray nodes on</span>
<span class="sd">    the same machine using non-overlapping worker port range, to achieve this, in this</span>
<span class="sd">    function, it creates a file `/tmp/ray_on_spark_worker_port_allocation.txt` file,</span>
<span class="sd">    the file format is composed of multiple lines, each line contains 2 number: `pid`</span>
<span class="sd">    and `port_range_slot_index`, each port range slot allocates 1000 ports, and</span>
<span class="sd">    corresponding port range is:</span>
<span class="sd">     - range_begin (inclusive): 20000 + port_range_slot_index * 1000</span>
<span class="sd">     - range_end (exclusive): range_begin + 1000</span>
<span class="sd">    In this function, it first scans `/tmp/ray_on_spark_worker_port_allocation.txt`</span>
<span class="sd">    file, removing lines that containing dead process pid, then find the first unused</span>
<span class="sd">    port_range_slot_index, then regenerate this file, and return the allocated port</span>
<span class="sd">    range.</span>

<span class="sd">    Returns: Allocated port range for current worker ports</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">psutil</span>
    <span class="kn">import</span> <span class="nn">fcntl</span>

    <span class="k">def</span> <span class="nf">acquire_lock</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
        <span class="n">mode</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">O_RDWR</span> <span class="o">|</span> <span class="n">os</span><span class="o">.</span><span class="n">O_CREAT</span> <span class="o">|</span> <span class="n">os</span><span class="o">.</span><span class="n">O_TRUNC</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">fd</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="n">mode</span><span class="p">)</span>
            <span class="c1"># The lock file must be readable / writable to all users.</span>
            <span class="n">os</span><span class="o">.</span><span class="n">chmod</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="mo">0o0777</span><span class="p">)</span>
            <span class="c1"># Allow for retrying getting a file lock a maximum number of seconds</span>
            <span class="n">max_lock_iter</span> <span class="o">=</span> <span class="mi">600</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_lock_iter</span><span class="p">):</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">fcntl</span><span class="o">.</span><span class="n">flock</span><span class="p">(</span><span class="n">fd</span><span class="p">,</span> <span class="n">fcntl</span><span class="o">.</span><span class="n">LOCK_EX</span> <span class="o">|</span> <span class="n">fcntl</span><span class="o">.</span><span class="n">LOCK_NB</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">BlockingIOError</span><span class="p">:</span>
                    <span class="c1"># Lock is used by other processes, continue loop to wait for lock</span>
                    <span class="c1"># available</span>
                    <span class="k">pass</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Acquire lock successfully.</span>
                    <span class="k">return</span> <span class="n">fd</span>
                <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
            <span class="k">raise</span> <span class="ne">TimeoutError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Acquiring lock on file </span><span class="si">{</span><span class="n">file_path</span><span class="si">}</span><span class="s2"> timeout.&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="n">fd</span><span class="p">)</span>

    <span class="n">lock_file_path</span> <span class="o">=</span> <span class="s2">&quot;/tmp/ray_on_spark_worker_startup_barrier_lock.lock&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">lock_fd</span> <span class="o">=</span> <span class="n">acquire_lock</span><span class="p">(</span><span class="n">lock_file_path</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">TimeoutError</span><span class="p">:</span>
        <span class="c1"># If timeout happens, the file lock might be hold by another process and that</span>
        <span class="c1"># process does not release the lock in time by some unexpected reason.</span>
        <span class="c1"># In this case, remove the existing lock file and create the file again, and</span>
        <span class="c1"># then acquire file lock on the new file.</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">lock_file_path</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="n">lock_fd</span> <span class="o">=</span> <span class="n">acquire_lock</span><span class="p">(</span><span class="n">lock_file_path</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">release_lock</span><span class="p">():</span>
        <span class="n">fcntl</span><span class="o">.</span><span class="n">flock</span><span class="p">(</span><span class="n">lock_fd</span><span class="p">,</span> <span class="n">fcntl</span><span class="o">.</span><span class="n">LOCK_UN</span><span class="p">)</span>
        <span class="n">os</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="n">lock_fd</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">port_alloc_file</span> <span class="o">=</span> <span class="s2">&quot;/tmp/ray_on_spark_worker_port_allocation.txt&quot;</span>

        <span class="c1"># NB: reading / writing `port_alloc_file` is protected by exclusive lock</span>
        <span class="c1"># on file `lock_file_path`</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">port_alloc_file</span><span class="p">):</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">port_alloc_file</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
                <span class="n">port_alloc_data</span> <span class="o">=</span> <span class="n">fp</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
            <span class="n">port_alloc_table</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">port_alloc_data</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="p">]</span>
            <span class="n">port_alloc_table</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">pid_str</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">slot_index_str</span><span class="p">))</span>
                <span class="k">for</span> <span class="n">pid_str</span><span class="p">,</span> <span class="n">slot_index_str</span> <span class="ow">in</span> <span class="n">port_alloc_table</span>
            <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">port_alloc_table</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">port_alloc_file</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;w&quot;</span><span class="p">):</span>
                <span class="k">pass</span>
            <span class="c1"># The port range allocation file must be readable / writable to all users.</span>
            <span class="n">os</span><span class="o">.</span><span class="n">chmod</span><span class="p">(</span><span class="n">port_alloc_file</span><span class="p">,</span> <span class="mo">0o0777</span><span class="p">)</span>

        <span class="n">port_alloc_map</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">pid</span><span class="p">:</span> <span class="n">slot_index</span>
            <span class="k">for</span> <span class="n">pid</span><span class="p">,</span> <span class="n">slot_index</span> <span class="ow">in</span> <span class="n">port_alloc_table</span>
            <span class="k">if</span> <span class="n">psutil</span><span class="o">.</span><span class="n">pid_exists</span><span class="p">(</span><span class="n">pid</span><span class="p">)</span>  <span class="c1"># remove slot used by dead process</span>
        <span class="p">}</span>

        <span class="n">allocated_slot_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">port_alloc_map</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">allocated_slot_set</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">new_slot_index</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_slot_index</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">allocated_slot_set</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">new_slot_index</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">index</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">allocated_slot_set</span><span class="p">:</span>
                    <span class="n">new_slot_index</span> <span class="o">=</span> <span class="n">index</span>
                    <span class="k">break</span>

        <span class="n">port_alloc_map</span><span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">getpid</span><span class="p">()]</span> <span class="o">=</span> <span class="n">new_slot_index</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">port_alloc_file</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">pid</span><span class="p">,</span> <span class="n">slot_index</span> <span class="ow">in</span> <span class="n">port_alloc_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">fp</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">pid</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">slot_index</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">worker_port_range_begin</span> <span class="o">=</span> <span class="mi">20000</span> <span class="o">+</span> <span class="n">new_slot_index</span> <span class="o">*</span> <span class="mi">1000</span>
        <span class="n">worker_port_range_end</span> <span class="o">=</span> <span class="n">worker_port_range_begin</span> <span class="o">+</span> <span class="mi">1000</span>

        <span class="k">if</span> <span class="n">worker_port_range_end</span> <span class="o">&gt;</span> <span class="mi">65536</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Too many ray worker nodes are running on this machine, cannot &quot;</span>
                <span class="s2">&quot;allocate worker port range for new ray worker node.&quot;</span>
            <span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
        <span class="n">release_lock</span><span class="p">()</span>
        <span class="k">raise</span>

    <span class="k">def</span> <span class="nf">hold_lock</span><span class="p">():</span>
        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">_RAY_WORKER_NODE_STARTUP_INTERVAL</span><span class="p">)</span>
        <span class="n">release_lock</span><span class="p">()</span>

    <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">hold_lock</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">())</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">worker_port_range_begin</span><span class="p">,</span> <span class="n">worker_port_range_end</span>


<span class="k">def</span> <span class="nf">_append_default_spilling_dir_config</span><span class="p">(</span><span class="n">head_node_options</span><span class="p">,</span> <span class="n">object_spilling_dir</span><span class="p">):</span>
    <span class="k">if</span> <span class="s2">&quot;system_config&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">head_node_options</span><span class="p">:</span>
        <span class="n">head_node_options</span><span class="p">[</span><span class="s2">&quot;system_config&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">sys_conf</span> <span class="o">=</span> <span class="n">head_node_options</span><span class="p">[</span><span class="s2">&quot;system_config&quot;</span><span class="p">]</span>
    <span class="k">if</span> <span class="s2">&quot;object_spilling_config&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">sys_conf</span><span class="p">:</span>
        <span class="n">sys_conf</span><span class="p">[</span><span class="s2">&quot;object_spilling_config&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;filesystem&quot;</span><span class="p">,</span>
                <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">{</span>
                    <span class="s2">&quot;directory_path&quot;</span><span class="p">:</span> <span class="n">object_spilling_dir</span><span class="p">,</span>
                <span class="p">},</span>
            <span class="p">}</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">head_node_options</span>


<span class="k">def</span> <span class="nf">_append_resources_config</span><span class="p">(</span><span class="n">node_options</span><span class="p">,</span> <span class="n">resources</span><span class="p">):</span>
    <span class="k">if</span> <span class="s2">&quot;resources&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">node_options</span><span class="p">:</span>
        <span class="n">node_options</span><span class="p">[</span><span class="s2">&quot;resources&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">node_options</span><span class="p">[</span><span class="s2">&quot;resources&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">resources</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">node_options</span>


<span class="k">def</span> <span class="nf">_get_default_ray_tmp_dir</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;RAY_TMPDIR&quot;</span><span class="p">,</span> <span class="s2">&quot;/tmp&quot;</span><span class="p">),</span> <span class="s2">&quot;ray&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_create_hook_entry</span><span class="p">(</span><span class="n">is_global</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">RAY_ON_SPARK_START_HOOK</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">load_class</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">RAY_ON_SPARK_START_HOOK</span><span class="p">])()</span>
    <span class="k">elif</span> <span class="n">is_in_databricks_runtime</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">DefaultDatabricksRayOnSparkStartHook</span><span class="p">(</span><span class="n">is_global</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">RayOnSparkStartHook</span><span class="p">(</span><span class="n">is_global</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_setup_ray_cluster</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_worker_nodes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">min_worker_nodes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_cpus_worker_node</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_cpus_head_node</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_gpus_worker_node</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_gpus_head_node</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">using_stage_scheduling</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">heap_memory_worker_node</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">heap_memory_head_node</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">object_store_memory_worker_node</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">object_store_memory_head_node</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">head_node_options</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
    <span class="n">worker_node_options</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
    <span class="n">ray_temp_root_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">collect_log_to_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">autoscale_upscaling_speed</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">autoscale_idle_timeout_minutes</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">is_global</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Type</span><span class="p">[</span><span class="n">RayClusterOnSpark</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The public API `ray.util.spark.setup_ray_cluster` does some argument</span>
<span class="sd">    validation and then pass validated arguments to this interface.</span>
<span class="sd">    and it returns a `RayClusterOnSpark` instance.</span>

<span class="sd">    The returned instance can be used to connect to, disconnect from and shutdown the</span>
<span class="sd">    ray cluster. This instance can also be used as a context manager (used by</span>
<span class="sd">    encapsulating operations within `with _setup_ray_cluster(...):`). Upon entering the</span>
<span class="sd">    managed scope, the ray cluster is initiated and connected to. When exiting the</span>
<span class="sd">    scope, the ray cluster is disconnected and shut down.</span>

<span class="sd">    Note: This function interface is stable and can be used for</span>
<span class="sd">    instrumentation logging patching.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">fcntl</span>

    <span class="n">start_hook</span> <span class="o">=</span> <span class="n">_create_hook_entry</span><span class="p">(</span><span class="n">is_global</span><span class="p">)</span>
    <span class="n">spark</span> <span class="o">=</span> <span class="n">get_spark_session</span><span class="p">()</span>

    <span class="n">ray_head_ip</span> <span class="o">=</span> <span class="n">socket</span><span class="o">.</span><span class="n">gethostbyname</span><span class="p">(</span><span class="n">get_spark_application_driver_host</span><span class="p">(</span><span class="n">spark</span><span class="p">))</span>
    <span class="n">ray_head_port</span> <span class="o">=</span> <span class="n">get_random_unused_port</span><span class="p">(</span><span class="n">ray_head_ip</span><span class="p">,</span> <span class="n">min_port</span><span class="o">=</span><span class="mi">9000</span><span class="p">,</span> <span class="n">max_port</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
    <span class="n">port_exclude_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">ray_head_port</span><span class="p">]</span>

    <span class="c1"># Make a copy for head_node_options to avoid changing original dict in user code.</span>
    <span class="n">head_node_options</span> <span class="o">=</span> <span class="n">head_node_options</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">include_dashboard</span> <span class="o">=</span> <span class="n">head_node_options</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;include_dashboard&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">ray_dashboard_port</span> <span class="o">=</span> <span class="n">head_node_options</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;dashboard_port&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">is_global</span><span class="p">:</span>
        <span class="n">ray_client_server_port</span> <span class="o">=</span> <span class="mi">10001</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ray_client_server_port</span> <span class="o">=</span> <span class="n">get_random_unused_port</span><span class="p">(</span>
            <span class="n">ray_head_ip</span><span class="p">,</span>
            <span class="n">min_port</span><span class="o">=</span><span class="mi">9000</span><span class="p">,</span>
            <span class="n">max_port</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
            <span class="n">exclude_list</span><span class="o">=</span><span class="n">port_exclude_list</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">port_exclude_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ray_client_server_port</span><span class="p">)</span>

    <span class="n">spark_job_server_port</span> <span class="o">=</span> <span class="n">get_random_unused_port</span><span class="p">(</span>
        <span class="n">ray_head_ip</span><span class="p">,</span>
        <span class="n">min_port</span><span class="o">=</span><span class="mi">9000</span><span class="p">,</span>
        <span class="n">max_port</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
        <span class="n">exclude_list</span><span class="o">=</span><span class="n">port_exclude_list</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">port_exclude_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">spark_job_server_port</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">include_dashboard</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">include_dashboard</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">ray_dashboard_port</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">ray_dashboard_port</span> <span class="o">=</span> <span class="n">get_random_unused_port</span><span class="p">(</span>
                <span class="n">ray_head_ip</span><span class="p">,</span>
                <span class="n">min_port</span><span class="o">=</span><span class="mi">9000</span><span class="p">,</span>
                <span class="n">max_port</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
                <span class="n">exclude_list</span><span class="o">=</span><span class="n">port_exclude_list</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">port_exclude_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ray_dashboard_port</span><span class="p">)</span>
        <span class="n">ray_dashboard_agent_port</span> <span class="o">=</span> <span class="n">get_random_unused_port</span><span class="p">(</span>
            <span class="n">ray_head_ip</span><span class="p">,</span>
            <span class="n">min_port</span><span class="o">=</span><span class="mi">9000</span><span class="p">,</span>
            <span class="n">max_port</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
            <span class="n">exclude_list</span><span class="o">=</span><span class="n">port_exclude_list</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">port_exclude_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ray_dashboard_agent_port</span><span class="p">)</span>

        <span class="n">dashboard_options</span> <span class="o">=</span> <span class="p">[</span>
            <span class="s2">&quot;--dashboard-host=0.0.0.0&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--dashboard-port=</span><span class="si">{</span><span class="n">ray_dashboard_port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--dashboard-agent-listen-port=</span><span class="si">{</span><span class="n">ray_dashboard_agent_port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="p">]</span>
        <span class="c1"># If include_dashboard is None, we don&#39;t set `--include-dashboard` option,</span>
        <span class="c1"># in this case Ray will decide whether dashboard can be started</span>
        <span class="c1"># (e.g. checking any missing dependencies).</span>
        <span class="k">if</span> <span class="n">include_dashboard</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">dashboard_options</span> <span class="o">+=</span> <span class="p">[</span><span class="s2">&quot;--include-dashboard=true&quot;</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dashboard_options</span> <span class="o">=</span> <span class="p">[</span>
            <span class="s2">&quot;--include-dashboard=false&quot;</span><span class="p">,</span>
        <span class="p">]</span>

    <span class="n">_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Ray head hostname: </span><span class="si">{</span><span class="n">ray_head_ip</span><span class="si">}</span><span class="s2">, port: </span><span class="si">{</span><span class="n">ray_head_port</span><span class="si">}</span><span class="s2">, &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;ray client server port: </span><span class="si">{</span><span class="n">ray_client_server_port</span><span class="si">}</span><span class="s2">.&quot;</span>
    <span class="p">)</span>

    <span class="n">cluster_unique_id</span> <span class="o">=</span> <span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">()</span><span class="o">.</span><span class="n">hex</span><span class="p">[:</span><span class="mi">8</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">is_global</span><span class="p">:</span>
        <span class="c1"># global mode enabled</span>
        <span class="c1"># for global mode, Ray always uses default temp dir</span>
        <span class="c1"># so that local Ray client can discover it without specifying</span>
        <span class="c1"># head node address.</span>
        <span class="k">if</span> <span class="n">ray_temp_root_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Ray on spark global mode cluster does not allow you to set &quot;</span>
                <span class="s2">&quot;&#39;ray_temp_root_dir&#39; argument.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># We only allow user to launch one active Ray on spark global cluster</span>
        <span class="c1"># at a time. So acquiring a global file lock before setting up a new</span>
        <span class="c1"># Ray on spark global cluster.</span>
        <span class="n">global_cluster_lock_fd</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">open</span><span class="p">(</span>
            <span class="s2">&quot;/tmp/ray_on_spark_global_cluster.lock&quot;</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">O_RDWR</span> <span class="o">|</span> <span class="n">os</span><span class="o">.</span><span class="n">O_CREAT</span> <span class="o">|</span> <span class="n">os</span><span class="o">.</span><span class="n">O_TRUNC</span>
        <span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># acquiring exclusive lock to ensure copy logs and removing dir safely.</span>
            <span class="n">fcntl</span><span class="o">.</span><span class="n">flock</span><span class="p">(</span><span class="n">global_cluster_lock_fd</span><span class="p">,</span> <span class="n">fcntl</span><span class="o">.</span><span class="n">LOCK_EX</span> <span class="o">|</span> <span class="n">fcntl</span><span class="o">.</span><span class="n">LOCK_NB</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">BlockingIOError</span><span class="p">:</span>
            <span class="c1"># acquiring global lock failed.</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Acquiring global lock failed for setting up new global mode Ray on &quot;</span>
                <span class="s2">&quot;spark cluster. If there is an active global mode Ray on spark &quot;</span>
                <span class="s2">&quot;cluster, please shut down it before you create a new one.&quot;</span>
            <span class="p">)</span>

        <span class="n">ray_temp_dir</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">ray_default_tmp_dir</span> <span class="o">=</span> <span class="n">_get_default_ray_tmp_dir</span><span class="p">()</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">ray_default_tmp_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">object_spilling_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ray_default_tmp_dir</span><span class="p">,</span> <span class="s2">&quot;spill&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">global_cluster_lock_fd</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">ray_temp_root_dir</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">ray_temp_root_dir</span> <span class="o">=</span> <span class="n">start_hook</span><span class="o">.</span><span class="n">get_default_temp_root_dir</span><span class="p">()</span>
        <span class="n">ray_temp_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="n">ray_temp_root_dir</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;ray-</span><span class="si">{</span><span class="n">ray_head_port</span><span class="si">}</span><span class="s2">-</span><span class="si">{</span><span class="n">cluster_unique_id</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">ray_temp_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">object_spilling_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ray_temp_dir</span><span class="p">,</span> <span class="s2">&quot;spill&quot;</span><span class="p">)</span>

    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">object_spilling_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">head_node_options</span> <span class="o">=</span> <span class="n">_append_default_spilling_dir_config</span><span class="p">(</span>
        <span class="n">head_node_options</span><span class="p">,</span> <span class="n">object_spilling_dir</span>
    <span class="p">)</span>

    <span class="kn">from</span> <span class="nn">ray.autoscaler._private.spark.spark_job_server</span> <span class="kn">import</span> <span class="p">(</span>
        <span class="n">_start_spark_job_server</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">ray_node_custom_env</span> <span class="o">=</span> <span class="n">start_hook</span><span class="o">.</span><span class="n">custom_environment_variables</span><span class="p">()</span>
    <span class="n">spark_job_server</span> <span class="o">=</span> <span class="n">_start_spark_job_server</span><span class="p">(</span>
        <span class="n">ray_head_ip</span><span class="p">,</span> <span class="n">spark_job_server_port</span><span class="p">,</span> <span class="n">spark</span><span class="p">,</span> <span class="n">ray_node_custom_env</span>
    <span class="p">)</span>
    <span class="n">autoscaling_cluster</span> <span class="o">=</span> <span class="n">AutoscalingCluster</span><span class="p">(</span>
        <span class="n">head_resources</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="n">num_cpus_head_node</span><span class="p">,</span>
            <span class="s2">&quot;GPU&quot;</span><span class="p">:</span> <span class="n">num_gpus_head_node</span><span class="p">,</span>
            <span class="s2">&quot;memory&quot;</span><span class="p">:</span> <span class="n">heap_memory_head_node</span><span class="p">,</span>
            <span class="s2">&quot;object_store_memory&quot;</span><span class="p">:</span> <span class="n">object_store_memory_head_node</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="n">worker_node_types</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;ray.worker&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;resources&quot;</span><span class="p">:</span> <span class="p">{</span>
                    <span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="n">num_cpus_worker_node</span><span class="p">,</span>
                    <span class="s2">&quot;GPU&quot;</span><span class="p">:</span> <span class="n">num_gpus_worker_node</span><span class="p">,</span>
                    <span class="s2">&quot;memory&quot;</span><span class="p">:</span> <span class="n">heap_memory_worker_node</span><span class="p">,</span>
                    <span class="s2">&quot;object_store_memory&quot;</span><span class="p">:</span> <span class="n">object_store_memory_worker_node</span><span class="p">,</span>
                <span class="p">},</span>
                <span class="s2">&quot;node_config&quot;</span><span class="p">:</span> <span class="p">{},</span>
                <span class="s2">&quot;min_workers&quot;</span><span class="p">:</span> <span class="n">min_worker_nodes</span><span class="p">,</span>
                <span class="s2">&quot;max_workers&quot;</span><span class="p">:</span> <span class="n">max_worker_nodes</span><span class="p">,</span>
            <span class="p">},</span>
        <span class="p">},</span>
        <span class="n">extra_provider_config</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;ray_head_ip&quot;</span><span class="p">:</span> <span class="n">ray_head_ip</span><span class="p">,</span>
            <span class="s2">&quot;ray_head_port&quot;</span><span class="p">:</span> <span class="n">ray_head_port</span><span class="p">,</span>
            <span class="s2">&quot;cluster_unique_id&quot;</span><span class="p">:</span> <span class="n">cluster_unique_id</span><span class="p">,</span>
            <span class="s2">&quot;using_stage_scheduling&quot;</span><span class="p">:</span> <span class="n">using_stage_scheduling</span><span class="p">,</span>
            <span class="s2">&quot;ray_temp_dir&quot;</span><span class="p">:</span> <span class="n">ray_temp_dir</span><span class="p">,</span>
            <span class="s2">&quot;worker_node_options&quot;</span><span class="p">:</span> <span class="n">worker_node_options</span><span class="p">,</span>
            <span class="s2">&quot;collect_log_to_path&quot;</span><span class="p">:</span> <span class="n">collect_log_to_path</span><span class="p">,</span>
            <span class="s2">&quot;spark_job_server_port&quot;</span><span class="p">:</span> <span class="n">spark_job_server_port</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="n">upscaling_speed</span><span class="o">=</span><span class="n">autoscale_upscaling_speed</span><span class="p">,</span>
        <span class="n">idle_timeout_minutes</span><span class="o">=</span><span class="n">autoscale_idle_timeout_minutes</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">ray_head_proc</span><span class="p">,</span> <span class="n">tail_output_deque</span> <span class="o">=</span> <span class="n">autoscaling_cluster</span><span class="o">.</span><span class="n">start</span><span class="p">(</span>
        <span class="n">ray_head_ip</span><span class="p">,</span>
        <span class="n">ray_head_port</span><span class="p">,</span>
        <span class="n">ray_client_server_port</span><span class="p">,</span>
        <span class="n">ray_temp_dir</span><span class="p">,</span>
        <span class="n">dashboard_options</span><span class="p">,</span>
        <span class="n">head_node_options</span><span class="p">,</span>
        <span class="n">collect_log_to_path</span><span class="p">,</span>
        <span class="n">ray_node_custom_env</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">ray_head_node_cmd</span> <span class="o">=</span> <span class="n">autoscaling_cluster</span><span class="o">.</span><span class="n">ray_head_node_cmd</span>

    <span class="c1"># wait ray head node spin up.</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">_RAY_HEAD_STARTUP_TIMEOUT</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_port_in_use</span><span class="p">(</span><span class="n">ray_head_ip</span><span class="p">,</span> <span class="n">ray_head_port</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">ray_head_proc</span><span class="o">.</span><span class="n">poll</span><span class="p">()</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Ray head GCS service is down. Kill ray head node.</span>
            <span class="n">ray_head_proc</span><span class="o">.</span><span class="n">terminate</span><span class="p">()</span>
            <span class="c1"># wait killing complete.</span>
            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

        <span class="n">cmd_exec_failure_msg</span> <span class="o">=</span> <span class="n">gen_cmd_exec_failure_msg</span><span class="p">(</span>
            <span class="n">ray_head_node_cmd</span><span class="p">,</span> <span class="n">ray_head_proc</span><span class="o">.</span><span class="n">returncode</span><span class="p">,</span> <span class="n">tail_output_deque</span>
        <span class="p">)</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Start Ray head node failed!</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">cmd_exec_failure_msg</span><span class="p">)</span>

    <span class="n">_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Ray head node started.&quot;</span><span class="p">)</span>

    <span class="n">cluster_address</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">ray_head_ip</span><span class="si">}</span><span class="s2">:</span><span class="si">{</span><span class="n">ray_head_port</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="c1"># Set RAY_ADDRESS environment variable to the cluster address.</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;RAY_ADDRESS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">cluster_address</span>

    <span class="n">ray_cluster_handler</span> <span class="o">=</span> <span class="n">RayClusterOnSpark</span><span class="p">(</span>
        <span class="n">address</span><span class="o">=</span><span class="n">cluster_address</span><span class="p">,</span>
        <span class="n">head_proc</span><span class="o">=</span><span class="n">ray_head_proc</span><span class="p">,</span>
        <span class="n">min_worker_nodes</span><span class="o">=</span><span class="n">min_worker_nodes</span><span class="p">,</span>
        <span class="n">max_worker_nodes</span><span class="o">=</span><span class="n">max_worker_nodes</span><span class="p">,</span>
        <span class="n">temp_dir</span><span class="o">=</span><span class="n">ray_temp_dir</span><span class="p">,</span>
        <span class="n">cluster_unique_id</span><span class="o">=</span><span class="n">cluster_unique_id</span><span class="p">,</span>
        <span class="n">start_hook</span><span class="o">=</span><span class="n">start_hook</span><span class="p">,</span>
        <span class="n">ray_dashboard_port</span><span class="o">=</span><span class="n">ray_dashboard_port</span><span class="p">,</span>
        <span class="n">spark_job_server</span><span class="o">=</span><span class="n">spark_job_server</span><span class="p">,</span>
        <span class="n">global_cluster_lock_fd</span><span class="o">=</span><span class="n">global_cluster_lock_fd</span><span class="p">,</span>
        <span class="n">ray_client_server_port</span><span class="o">=</span><span class="n">ray_client_server_port</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">start_hook</span><span class="o">.</span><span class="n">on_cluster_created</span><span class="p">(</span><span class="n">ray_cluster_handler</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ray_cluster_handler</span>


<span class="n">_active_ray_cluster</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">_active_ray_cluster_rwlock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">RLock</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_create_resource_profile</span><span class="p">(</span><span class="n">num_cpus_per_node</span><span class="p">,</span> <span class="n">num_gpus_per_node</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">pyspark.resource.profile</span> <span class="kn">import</span> <span class="n">ResourceProfileBuilder</span>
    <span class="kn">from</span> <span class="nn">pyspark.resource.requests</span> <span class="kn">import</span> <span class="n">TaskResourceRequests</span>

    <span class="n">task_res_req</span> <span class="o">=</span> <span class="n">TaskResourceRequests</span><span class="p">()</span><span class="o">.</span><span class="n">cpus</span><span class="p">(</span><span class="n">num_cpus_per_node</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">num_gpus_per_node</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">task_res_req</span> <span class="o">=</span> <span class="n">task_res_req</span><span class="o">.</span><span class="n">resource</span><span class="p">(</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">num_gpus_per_node</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ResourceProfileBuilder</span><span class="p">()</span><span class="o">.</span><span class="n">require</span><span class="p">(</span><span class="n">task_res_req</span><span class="p">)</span><span class="o">.</span><span class="n">build</span>


<span class="c1"># A dict storing blocked key to replacement argument you should use.</span>
<span class="n">_head_node_option_block_keys</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;temp_dir&quot;</span><span class="p">:</span> <span class="s2">&quot;ray_temp_root_dir&quot;</span><span class="p">,</span>
    <span class="s2">&quot;block&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;head&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;node_ip_address&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;port&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;num_cpus&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;num_gpus&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;dashboard_host&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;dashboard_agent_listen_port&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">_worker_node_option_block_keys</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;temp_dir&quot;</span><span class="p">:</span> <span class="s2">&quot;ray_temp_root_dir&quot;</span><span class="p">,</span>
    <span class="s2">&quot;block&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;head&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;address&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;num_cpus&quot;</span><span class="p">:</span> <span class="s2">&quot;num_cpus_worker_node&quot;</span><span class="p">,</span>
    <span class="s2">&quot;num_gpus&quot;</span><span class="p">:</span> <span class="s2">&quot;num_gpus_worker_node&quot;</span><span class="p">,</span>
    <span class="s2">&quot;memory&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;object_store_memory&quot;</span><span class="p">:</span> <span class="s2">&quot;object_store_memory_worker_node&quot;</span><span class="p">,</span>
    <span class="s2">&quot;dashboard_agent_listen_port&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;min_worker_port&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;max_worker_port&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>


<span class="k">def</span> <span class="nf">_verify_node_options</span><span class="p">(</span><span class="n">node_options</span><span class="p">,</span> <span class="n">block_keys</span><span class="p">,</span> <span class="n">node_type</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">node_options</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;--&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="s2">&quot;-&quot;</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;For a ray node option like &#39;--foo-bar&#39;, you should convert it to &quot;</span>
                <span class="s2">&quot;following format &#39;foo_bar&#39; in &#39;head_node_options&#39; / &quot;</span>
                <span class="s2">&quot;&#39;worker_node_options&#39; arguments.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">block_keys</span><span class="p">:</span>
            <span class="n">common_err_msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Setting the option &#39;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&#39; for </span><span class="si">{</span><span class="n">node_type</span><span class="si">}</span><span class="s2"> nodes is not allowed.&quot;</span>
            <span class="p">)</span>
            <span class="n">replacement_arg</span> <span class="o">=</span> <span class="n">block_keys</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">replacement_arg</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">common_err_msg</span><span class="si">}</span><span class="s2"> You should set the &#39;</span><span class="si">{</span><span class="n">replacement_arg</span><span class="si">}</span><span class="s2">&#39; option &quot;</span>
                    <span class="s2">&quot;instead.&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">common_err_msg</span><span class="si">}</span><span class="s2"> This option is controlled by Ray on Spark.&quot;</span>
                <span class="p">)</span>


<span class="k">def</span> <span class="nf">_setup_ray_cluster_internal</span><span class="p">(</span>
    <span class="n">max_worker_nodes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">min_worker_nodes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">num_cpus_worker_node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">num_cpus_head_node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">num_gpus_worker_node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">num_gpus_head_node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">heap_memory_worker_node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">heap_memory_head_node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">object_store_memory_worker_node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">object_store_memory_head_node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">head_node_options</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">],</span>
    <span class="n">worker_node_options</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">],</span>
    <span class="n">ray_temp_root_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">strict_mode</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">collect_log_to_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">autoscale_upscaling_speed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
    <span class="n">autoscale_idle_timeout_minutes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
    <span class="n">is_global</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
    <span class="k">global</span> <span class="n">_active_ray_cluster</span>

    <span class="n">_check_system_environment</span><span class="p">()</span>
    <span class="n">_install_sigterm_signal</span><span class="p">()</span>

    <span class="n">head_node_options</span> <span class="o">=</span> <span class="n">head_node_options</span> <span class="ow">or</span> <span class="p">{}</span>
    <span class="n">worker_node_options</span> <span class="o">=</span> <span class="n">worker_node_options</span> <span class="ow">or</span> <span class="p">{}</span>

    <span class="n">_verify_node_options</span><span class="p">(</span>
        <span class="n">head_node_options</span><span class="p">,</span>
        <span class="n">_head_node_option_block_keys</span><span class="p">,</span>
        <span class="s2">&quot;Ray head node on spark&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">_verify_node_options</span><span class="p">(</span>
        <span class="n">worker_node_options</span><span class="p">,</span>
        <span class="n">_worker_node_option_block_keys</span><span class="p">,</span>
        <span class="s2">&quot;Ray worker node on spark&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">_active_ray_cluster</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Current active ray cluster on spark haven&#39;t shut down. Please call &quot;</span>
            <span class="s2">&quot;`ray.util.spark.shutdown_ray_cluster()` before initiating a new Ray &quot;</span>
            <span class="s2">&quot;cluster on spark.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">ray</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Current python process already initialized Ray, Please shut down it &quot;</span>
            <span class="s2">&quot;by `ray.shutdown()` before initiating a Ray cluster on spark.&quot;</span>
        <span class="p">)</span>

    <span class="n">spark</span> <span class="o">=</span> <span class="n">get_spark_session</span><span class="p">()</span>

    <span class="n">spark_master</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">master</span>

    <span class="n">is_spark_local_mode</span> <span class="o">=</span> <span class="n">spark_master</span> <span class="o">==</span> <span class="s2">&quot;local&quot;</span> <span class="ow">or</span> <span class="n">spark_master</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;local[&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
        <span class="n">spark_master</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;spark://&quot;</span><span class="p">)</span>
        <span class="ow">or</span> <span class="n">spark_master</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;local-cluster[&quot;</span><span class="p">)</span>
        <span class="ow">or</span> <span class="n">is_spark_local_mode</span>
    <span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Ray on Spark only supports spark cluster in standalone mode, &quot;</span>
            <span class="s2">&quot;local-cluster mode or spark local mode.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">is_spark_local_mode</span><span class="p">:</span>
        <span class="n">support_stage_scheduling</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">elif</span> <span class="p">(</span>
        <span class="n">is_in_databricks_runtime</span><span class="p">()</span>
        <span class="ow">and</span> <span class="n">Version</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;DATABRICKS_RUNTIME_VERSION&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">major</span> <span class="o">&gt;=</span> <span class="mi">12</span>
    <span class="p">):</span>
        <span class="n">support_stage_scheduling</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="kn">import</span> <span class="nn">pyspark</span>

        <span class="k">if</span> <span class="n">Version</span><span class="p">(</span><span class="n">pyspark</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span><span class="o">.</span><span class="n">release</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">support_stage_scheduling</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">support_stage_scheduling</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="s2">&quot;num_cpus_per_node&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">num_cpus_worker_node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;&#39;num_cpus_per_node&#39; and &#39;num_cpus_worker_node&#39; arguments are &quot;</span>
                <span class="s2">&quot;equivalent. Only set &#39;num_cpus_worker_node&#39;.&quot;</span>
            <span class="p">)</span>
        <span class="n">num_cpus_worker_node</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;num_cpus_per_node&quot;</span><span class="p">]</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;&#39;num_cpus_per_node&#39; argument is deprecated, please use &quot;</span>
            <span class="s2">&quot;&#39;num_cpus_worker_node&#39; argument instead.&quot;</span><span class="p">,</span>
            <span class="ne">DeprecationWarning</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="s2">&quot;num_gpus_per_node&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">num_gpus_worker_node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;&#39;num_gpus_per_node&#39; and &#39;num_gpus_worker_node&#39; arguments are &quot;</span>
                <span class="s2">&quot;equivalent. Only set &#39;num_gpus_worker_node&#39;.&quot;</span>
            <span class="p">)</span>
        <span class="n">num_gpus_worker_node</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;num_gpus_per_node&quot;</span><span class="p">]</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;&#39;num_gpus_per_node&#39; argument is deprecated, please use &quot;</span>
            <span class="s2">&quot;&#39;num_gpus_worker_node&#39; argument instead.&quot;</span><span class="p">,</span>
            <span class="ne">DeprecationWarning</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="s2">&quot;object_store_memory_per_node&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">object_store_memory_worker_node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;&#39;object_store_memory_per_node&#39; and &#39;object_store_memory_worker_node&#39; &quot;</span>
                <span class="s2">&quot;arguments  are equivalent. Only set &quot;</span>
                <span class="s2">&quot;&#39;object_store_memory_worker_node&#39;.&quot;</span>
            <span class="p">)</span>
        <span class="n">object_store_memory_worker_node</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;object_store_memory_per_node&quot;</span><span class="p">]</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;&#39;object_store_memory_per_node&#39; argument is deprecated, please use &quot;</span>
            <span class="s2">&quot;&#39;object_store_memory_worker_node&#39; argument instead.&quot;</span><span class="p">,</span>
            <span class="ne">DeprecationWarning</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Environment configurations within the Spark Session that dictate how many cpus</span>
    <span class="c1"># and gpus to use for each submitted spark task.</span>
    <span class="n">num_spark_task_cpus</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">getConf</span><span class="p">()</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;spark.task.cpus&quot;</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">num_cpus_worker_node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">num_cpus_worker_node</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Argument `num_cpus_worker_node` value must be &gt; 0.&quot;</span><span class="p">)</span>

    <span class="c1"># note: spark.task.resource.gpu.amount config might be fractional value like 0.5</span>
    <span class="n">default_num_spark_task_gpus</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span>
        <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">getConf</span><span class="p">()</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;spark.task.resource.gpu.amount&quot;</span><span class="p">,</span> <span class="s2">&quot;0&quot;</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">rounded_num_spark_task_gpus</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">default_num_spark_task_gpus</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">default_num_spark_task_gpus</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">warn_msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;You configured &#39;spark.task.resource.gpu.amount&#39; to &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">default_num_spark_task_gpus</span><span class="si">}</span><span class="s2">,&quot;</span>
            <span class="s2">&quot;we recommend setting this value to 0 so that Spark jobs do not &quot;</span>
            <span class="s2">&quot;reserve GPU resources, preventing Ray-on-Spark workloads from having the &quot;</span>
            <span class="s2">&quot;maximum number of GPUs available.&quot;</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">is_in_databricks_runtime</span><span class="p">():</span>
            <span class="kn">from</span> <span class="nn">ray.util.spark.databricks_hook</span> <span class="kn">import</span> <span class="p">(</span>
                <span class="n">get_databricks_display_html_function</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">get_databricks_display_html_function</span><span class="p">()(</span>
                <span class="sa">f</span><span class="s2">&quot;&lt;b style=&#39;color:red;&#39;&gt;</span><span class="si">{</span><span class="n">warn_msg</span><span class="si">}</span><span class="s2">&lt;/b&gt;&quot;</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">num_gpus_worker_node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">num_gpus_worker_node</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Argument `num_gpus_worker_node` value must be &gt;= 0.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_spark_worker_resources</span><span class="p">(</span><span class="n">_</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">ray.util.spark.utils</span> <span class="kn">import</span> <span class="p">(</span>
            <span class="n">_get_cpu_cores</span><span class="p">,</span>
            <span class="n">_get_num_physical_gpus</span><span class="p">,</span>
            <span class="n">_get_spark_worker_total_physical_memory</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">num_cpus_spark_worker</span> <span class="o">=</span> <span class="n">_get_cpu_cores</span><span class="p">()</span>
        <span class="n">num_gpus_spark_worker</span> <span class="o">=</span> <span class="n">_get_num_physical_gpus</span><span class="p">()</span>
        <span class="n">total_mem_bytes</span> <span class="o">=</span> <span class="n">_get_spark_worker_total_physical_memory</span><span class="p">()</span>

        <span class="k">return</span> <span class="p">(</span>
            <span class="n">num_cpus_spark_worker</span><span class="p">,</span>
            <span class="n">num_gpus_spark_worker</span><span class="p">,</span>
            <span class="n">total_mem_bytes</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="p">(</span><span class="n">num_cpus_spark_worker</span><span class="p">,</span> <span class="n">num_gpus_spark_worker</span><span class="p">,</span> <span class="n">spark_worker_mem_bytes</span><span class="p">,)</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">_get_spark_worker_resources</span><span class="p">)</span>
        <span class="o">.</span><span class="n">collect</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">num_cpus_worker_node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">num_gpus_worker_node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">support_stage_scheduling</span><span class="p">:</span>
            <span class="n">using_stage_scheduling</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">res_profile</span> <span class="o">=</span> <span class="n">_create_resource_profile</span><span class="p">(</span>
                <span class="n">num_cpus_worker_node</span><span class="p">,</span> <span class="n">num_gpus_worker_node</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Current spark version does not support stage scheduling, so that &quot;</span>
                <span class="s2">&quot;you cannot set the argument `num_cpus_worker_node` and &quot;</span>
                <span class="s2">&quot;`num_gpus_worker_node` values. Without setting the 2 arguments, &quot;</span>
                <span class="s2">&quot;per-Ray worker node will be assigned with number of &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;&#39;spark.task.cpus&#39; (equals to </span><span class="si">{</span><span class="n">num_spark_task_cpus</span><span class="si">}</span><span class="s2">) cpu cores &quot;</span>
                <span class="s2">&quot;and rounded down number of &#39;spark.task.resource.gpu.amount&#39; &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;(equals to </span><span class="si">{</span><span class="n">rounded_num_spark_task_gpus</span><span class="si">}</span><span class="s2">) GPUs. To enable spark &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;stage scheduling, you need to upgrade spark to 3.4 version or use &quot;</span>
                <span class="s2">&quot;Databricks Runtime 12.x, and you cannot use spark local mode.&quot;</span>
            <span class="p">)</span>
    <span class="k">elif</span> <span class="n">num_cpus_worker_node</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">num_gpus_worker_node</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">support_stage_scheduling</span><span class="p">:</span>
            <span class="c1"># Make one Ray worker node using maximum CPU / GPU resources</span>
            <span class="c1"># of the whole spark worker node, this is the optimal</span>
            <span class="c1"># configuration.</span>
            <span class="n">num_cpus_worker_node</span> <span class="o">=</span> <span class="n">num_cpus_spark_worker</span>
            <span class="n">num_gpus_worker_node</span> <span class="o">=</span> <span class="n">num_gpus_spark_worker</span>
            <span class="n">using_stage_scheduling</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">res_profile</span> <span class="o">=</span> <span class="n">_create_resource_profile</span><span class="p">(</span>
                <span class="n">num_cpus_worker_node</span><span class="p">,</span> <span class="n">num_gpus_worker_node</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">using_stage_scheduling</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">res_profile</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="n">num_cpus_worker_node</span> <span class="o">=</span> <span class="n">num_spark_task_cpus</span>
            <span class="n">num_gpus_worker_node</span> <span class="o">=</span> <span class="n">rounded_num_spark_task_gpus</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;&#39;num_cpus_worker_node&#39; and &#39;num_gpus_worker_node&#39; arguments must be&quot;</span>
            <span class="s2">&quot;set together or unset together.&quot;</span>
        <span class="p">)</span>

    <span class="p">(</span>
        <span class="n">ray_worker_node_heap_mem_bytes</span><span class="p">,</span>
        <span class="n">ray_worker_node_object_store_mem_bytes</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">=</span> <span class="n">get_avail_mem_per_ray_worker_node</span><span class="p">(</span>
        <span class="n">spark</span><span class="p">,</span>
        <span class="n">heap_memory_worker_node</span><span class="p">,</span>
        <span class="n">object_store_memory_worker_node</span><span class="p">,</span>
        <span class="n">num_cpus_worker_node</span><span class="p">,</span>
        <span class="n">num_gpus_worker_node</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">spark_worker_ray_node_slots</span> <span class="o">=</span> <span class="n">_get_local_ray_node_slots</span><span class="p">(</span>
        <span class="n">num_cpus_spark_worker</span><span class="p">,</span>
        <span class="n">num_gpus_spark_worker</span><span class="p">,</span>
        <span class="n">num_cpus_worker_node</span><span class="p">,</span>
        <span class="n">num_gpus_worker_node</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">spark_executor_memory_bytes</span> <span class="o">=</span> <span class="n">get_configured_spark_executor_memory_bytes</span><span class="p">(</span><span class="n">spark</span><span class="p">)</span>
    <span class="n">spark_worker_required_memory_bytes</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">spark_executor_memory_bytes</span>
        <span class="o">+</span> <span class="n">spark_worker_ray_node_slots</span>
        <span class="o">*</span> <span class="p">(</span><span class="n">ray_worker_node_heap_mem_bytes</span> <span class="o">+</span> <span class="n">ray_worker_node_object_store_mem_bytes</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">spark_worker_required_memory_bytes</span> <span class="o">&gt;</span> <span class="mf">0.8</span> <span class="o">*</span> <span class="n">spark_worker_mem_bytes</span><span class="p">:</span>
        <span class="n">warn_msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;In each spark worker node, we recommend making the sum of &quot;</span>
            <span class="s2">&quot;&#39;spark_executor_memory + num_Ray_worker_nodes_per_spark_worker * &quot;</span>
            <span class="s2">&quot;(memory_worker_node + object_store_memory_worker_node)&#39; to be less than &quot;</span>
            <span class="s2">&quot;&#39;spark_worker_physical_memory * 0.8&#39;, otherwise it might lead to &quot;</span>
            <span class="s2">&quot;spark worker physical memory exhaustion and Ray task OOM errors.&quot;</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">is_in_databricks_runtime</span><span class="p">():</span>
            <span class="kn">from</span> <span class="nn">ray.util.spark.databricks_hook</span> <span class="kn">import</span> <span class="p">(</span>
                <span class="n">get_databricks_display_html_function</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">get_databricks_display_html_function</span><span class="p">()(</span>
                <span class="sa">f</span><span class="s2">&quot;&lt;b style=&#39;background-color:Cyan;&#39;&gt;</span><span class="si">{</span><span class="n">warn_msg</span><span class="si">}</span><span class="s2">&lt;br&gt;&lt;/b&gt;&quot;</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>

    <span class="k">if</span> <span class="s2">&quot;num_worker_nodes&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;&#39;num_worker_nodes&#39; argument is removed, please set &quot;</span>
            <span class="s2">&quot;&#39;max_worker_nodes&#39; and &#39;min_worker_nodes&#39; argument instead.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">max_worker_nodes</span> <span class="o">==</span> <span class="n">MAX_NUM_WORKER_NODES</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">min_worker_nodes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;If you set &#39;max_worker_nodes&#39; to &#39;MAX_NUM_WORKER_NODES&#39;, autoscaling &quot;</span>
                <span class="s2">&quot;is not supported, so that you cannot set &#39;min_worker_nodes&#39; argument &quot;</span>
                <span class="s2">&quot;and &#39;min_worker_nodes&#39; is automatically set to be equal to &quot;</span>
                <span class="s2">&quot;&#39;max_worker_nodes&#39;.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># max_worker_nodes=MAX_NUM_WORKER_NODES represents using all available</span>
        <span class="c1"># spark task slots</span>
        <span class="n">max_worker_nodes</span> <span class="o">=</span> <span class="n">get_max_num_concurrent_tasks</span><span class="p">(</span><span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="p">,</span> <span class="n">res_profile</span><span class="p">)</span>
        <span class="n">min_worker_nodes</span> <span class="o">=</span> <span class="n">max_worker_nodes</span>
    <span class="k">elif</span> <span class="n">max_worker_nodes</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;The value of &#39;max_worker_nodes&#39; argument must be either a positive &quot;</span>
            <span class="s2">&quot;integer or &#39;ray.util.spark.MAX_NUM_WORKER_NODES&#39;.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="s2">&quot;autoscale&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;&#39;autoscale&#39; argument is removed. You can set &#39;min_worker_nodes&#39; argument &quot;</span>
            <span class="s2">&quot;to be less than &#39;max_worker_nodes&#39; to make autoscaling enabled.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">min_worker_nodes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">min_worker_nodes</span> <span class="o">=</span> <span class="n">max_worker_nodes</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="p">(</span><span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">min_worker_nodes</span> <span class="o">&lt;=</span> <span class="n">max_worker_nodes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;The value of &#39;max_worker_nodes&#39; argument must be an integer &gt;= 0 &quot;</span>
            <span class="s2">&quot;and &lt;= &#39;max_worker_nodes&#39;&quot;</span>
        <span class="p">)</span>

    <span class="n">insufficient_resources</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">if</span> <span class="n">num_cpus_worker_node</span> <span class="o">&lt;</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">insufficient_resources</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="s2">&quot;The provided CPU resources for each ray worker are inadequate to start &quot;</span>
            <span class="s2">&quot;a ray cluster. Based on the total cpu resources available and the &quot;</span>
            <span class="s2">&quot;configured task sizing, each ray worker node would start with &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">num_cpus_worker_node</span><span class="si">}</span><span class="s2"> CPU cores. This is less than the recommended &quot;</span>
            <span class="s2">&quot;value of `4` CPUs per worker. On spark version &gt;= 3.4 or Databricks &quot;</span>
            <span class="s2">&quot;Runtime 12.x, you can set the argument `num_cpus_worker_node` to &quot;</span>
            <span class="s2">&quot;a value &gt;= 4 to address it, otherwise you need to increase the spark &quot;</span>
            <span class="s2">&quot;application configuration &#39;spark.task.cpus&#39; to a minimum of `4` to &quot;</span>
            <span class="s2">&quot;address it.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">ray_worker_node_heap_mem_bytes</span> <span class="o">&lt;</span> <span class="mi">10</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">:</span>
        <span class="n">insufficient_resources</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="s2">&quot;The provided memory resources for each ray worker node are inadequate. &quot;</span>
            <span class="s2">&quot;Based on the total memory available on the spark cluster and the &quot;</span>
            <span class="s2">&quot;configured task sizing, each ray worker would start with &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">ray_worker_node_heap_mem_bytes</span><span class="si">}</span><span class="s2"> bytes heap memory. This is less than &quot;</span>
            <span class="s2">&quot;the recommended value of 10GB. The ray worker node heap memory size is &quot;</span>
            <span class="s2">&quot;calculated by &quot;</span>
            <span class="s2">&quot;(SPARK_WORKER_PHYSICAL_MEMORY / num_local_spark_task_slots * 0.8) - &quot;</span>
            <span class="s2">&quot;object_store_memory_worker_node. To increase the heap space available, &quot;</span>
            <span class="s2">&quot;increase the memory in the spark cluster by using instance types with &quot;</span>
            <span class="s2">&quot;larger memory, or increase number of CPU/GPU per Ray worker node &quot;</span>
            <span class="s2">&quot;(so it leads to less Ray worker node slots per spark worker node), &quot;</span>
            <span class="s2">&quot;or apply a lower `object_store_memory_worker_node`.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">insufficient_resources</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">strict_mode</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;You are creating ray cluster on spark with strict mode (it can be &quot;</span>
                <span class="s2">&quot;disabled by setting argument &#39;strict_mode=False&#39; when calling API &quot;</span>
                <span class="s2">&quot;&#39;setup_ray_cluster&#39;), strict mode requires the spark cluster config &quot;</span>
                <span class="s2">&quot;satisfying following criterion: &quot;</span>
                <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">insufficient_resources</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">insufficient_resources</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">num_cpus_head_node</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">is_global</span><span class="p">:</span>
            <span class="n">num_cpus_head_node</span> <span class="o">=</span> <span class="n">_get_cpu_cores</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">num_cpus_head_node</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">num_cpus_head_node</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Argument `num_cpus_head_node` value must be &gt;= 0. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Current value is </span><span class="si">{</span><span class="n">num_cpus_head_node</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

    <span class="k">if</span> <span class="n">num_gpus_head_node</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">is_global</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">num_gpus_head_node</span> <span class="o">=</span> <span class="n">_get_num_physical_gpus</span><span class="p">()</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                <span class="n">num_gpus_head_node</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">num_gpus_head_node</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">num_gpus_head_node</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Argument `num_gpus_head_node` value must be &gt;= 0.&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Current value is </span><span class="si">{</span><span class="n">num_gpus_head_node</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="n">num_cpus_head_node</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="ow">and</span> <span class="n">num_gpus_head_node</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="ow">and</span> <span class="n">object_store_memory_head_node</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="p">):</span>
        <span class="c1"># Because tasks that require CPU or GPU resources are not scheduled to Ray</span>
        <span class="c1"># head node, and user does not set `object_store_memory_head_node` explicitly,</span>
        <span class="c1"># limit the heap memory and object store memory allocation to the</span>
        <span class="c1"># head node, in order to save spark driver memory.</span>
        <span class="n">heap_memory_head_node</span> <span class="o">=</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span>
        <span class="n">object_store_memory_head_node</span> <span class="o">=</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">heap_memory_head_node</span><span class="p">,</span> <span class="n">object_store_memory_head_node</span> <span class="o">=</span> <span class="n">calc_mem_ray_head_node</span><span class="p">(</span>
            <span class="n">heap_memory_head_node</span><span class="p">,</span> <span class="n">object_store_memory_head_node</span>
        <span class="p">)</span>

    <span class="k">with</span> <span class="n">_active_ray_cluster_rwlock</span><span class="p">:</span>
        <span class="n">cluster</span> <span class="o">=</span> <span class="n">_setup_ray_cluster</span><span class="p">(</span>
            <span class="n">max_worker_nodes</span><span class="o">=</span><span class="n">max_worker_nodes</span><span class="p">,</span>
            <span class="n">min_worker_nodes</span><span class="o">=</span><span class="n">min_worker_nodes</span><span class="p">,</span>
            <span class="n">num_cpus_worker_node</span><span class="o">=</span><span class="n">num_cpus_worker_node</span><span class="p">,</span>
            <span class="n">num_cpus_head_node</span><span class="o">=</span><span class="n">num_cpus_head_node</span><span class="p">,</span>
            <span class="n">num_gpus_worker_node</span><span class="o">=</span><span class="n">num_gpus_worker_node</span><span class="p">,</span>
            <span class="n">num_gpus_head_node</span><span class="o">=</span><span class="n">num_gpus_head_node</span><span class="p">,</span>
            <span class="n">using_stage_scheduling</span><span class="o">=</span><span class="n">using_stage_scheduling</span><span class="p">,</span>
            <span class="n">heap_memory_worker_node</span><span class="o">=</span><span class="n">ray_worker_node_heap_mem_bytes</span><span class="p">,</span>
            <span class="n">heap_memory_head_node</span><span class="o">=</span><span class="n">heap_memory_head_node</span><span class="p">,</span>
            <span class="n">object_store_memory_worker_node</span><span class="o">=</span><span class="n">ray_worker_node_object_store_mem_bytes</span><span class="p">,</span>
            <span class="n">object_store_memory_head_node</span><span class="o">=</span><span class="n">object_store_memory_head_node</span><span class="p">,</span>
            <span class="n">head_node_options</span><span class="o">=</span><span class="n">head_node_options</span><span class="p">,</span>
            <span class="n">worker_node_options</span><span class="o">=</span><span class="n">worker_node_options</span><span class="p">,</span>
            <span class="n">ray_temp_root_dir</span><span class="o">=</span><span class="n">ray_temp_root_dir</span><span class="p">,</span>
            <span class="n">collect_log_to_path</span><span class="o">=</span><span class="n">collect_log_to_path</span><span class="p">,</span>
            <span class="n">autoscale_upscaling_speed</span><span class="o">=</span><span class="n">autoscale_upscaling_speed</span><span class="p">,</span>
            <span class="n">autoscale_idle_timeout_minutes</span><span class="o">=</span><span class="n">autoscale_idle_timeout_minutes</span><span class="p">,</span>
            <span class="n">is_global</span><span class="o">=</span><span class="n">is_global</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># set global _active_ray_cluster to be the</span>
        <span class="c1"># started cluster.</span>
        <span class="n">_active_ray_cluster</span> <span class="o">=</span> <span class="n">cluster</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">cluster</span><span class="o">.</span><span class="n">wait_until_ready</span><span class="p">()</span>  <span class="c1"># NB: this line might raise error.</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">shutdown_ray_cluster</span><span class="p">()</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                <span class="k">pass</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Launch Ray-on-Saprk cluster failed&quot;</span><span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>

    <span class="n">head_ip</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">address</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">remote_connection_address</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;ray://</span><span class="si">{</span><span class="n">head_ip</span><span class="si">}</span><span class="s2">:</span><span class="si">{</span><span class="n">cluster</span><span class="o">.</span><span class="n">ray_client_server_port</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">return</span> <span class="n">cluster</span><span class="o">.</span><span class="n">address</span><span class="p">,</span> <span class="n">remote_connection_address</span>


<div class="viewcode-block" id="setup_ray_cluster">
<a class="viewcode-back" href="../../../../cluster/vms/user-guides/community/spark.html#ray.util.spark.setup_ray_cluster">[docs]</a>
<span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">setup_ray_cluster</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_worker_nodes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">min_worker_nodes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_cpus_worker_node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_cpus_head_node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_gpus_worker_node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_gpus_head_node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">memory_worker_node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">memory_head_node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">object_store_memory_worker_node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">object_store_memory_head_node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_node_options</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">worker_node_options</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">ray_temp_root_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">strict_mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">collect_log_to_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">autoscale_upscaling_speed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">autoscale_idle_timeout_minutes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set up a ray cluster on the spark cluster by starting a ray head node in the</span>
<span class="sd">    spark application&#39;s driver side node.</span>
<span class="sd">    After creating the head node, a background spark job is created that</span>
<span class="sd">    generates an instance of `RayClusterOnSpark` that contains configuration for the</span>
<span class="sd">    ray cluster that will run on the Spark cluster&#39;s worker nodes.</span>
<span class="sd">    After a ray cluster is set up, &quot;RAY_ADDRESS&quot; environment variable is set to</span>
<span class="sd">    the cluster address, so you can call `ray.init()` without specifying ray cluster</span>
<span class="sd">    address to connect to the cluster. To shut down the cluster you can call</span>
<span class="sd">    `ray.util.spark.shutdown_ray_cluster()`.</span>
<span class="sd">    Note: If the active ray cluster haven&#39;t shut down, you cannot create a new ray</span>
<span class="sd">    cluster.</span>

<span class="sd">    Args:</span>
<span class="sd">        max_worker_nodes: This argument represents maximum ray worker nodes to start</span>
<span class="sd">            for the ray cluster. you can</span>
<span class="sd">            specify the `max_worker_nodes` as `ray.util.spark.MAX_NUM_WORKER_NODES`</span>
<span class="sd">            represents a ray cluster</span>
<span class="sd">            configuration that will use all available resources configured for the</span>
<span class="sd">            spark application.</span>
<span class="sd">            To create a spark application that is intended to exclusively run a</span>
<span class="sd">            shared ray cluster in non-scaling, it is recommended to set this argument</span>
<span class="sd">            to `ray.util.spark.MAX_NUM_WORKER_NODES`.</span>
<span class="sd">        min_worker_nodes: Minimal number of worker nodes (default `None`),</span>
<span class="sd">            if &quot;max_worker_nodes&quot; value is equal to &quot;min_worker_nodes&quot; argument,</span>
<span class="sd">            or &quot;min_worker_nodes&quot; argument value is None, then autoscaling is disabled</span>
<span class="sd">            and Ray cluster is launched with fixed number &quot;max_worker_nodes&quot; of</span>
<span class="sd">            Ray worker nodes, otherwise autoscaling is enabled.</span>
<span class="sd">        num_cpus_worker_node: Number of cpus available to per-ray worker node, if not</span>
<span class="sd">            provided, if spark stage scheduling is supported, &#39;num_cpus_head_node&#39;</span>
<span class="sd">            value equals to number of cpu cores per spark worker node, otherwise</span>
<span class="sd">            it uses spark application configuration &#39;spark.task.cpus&#39; instead.</span>
<span class="sd">            **Limitation** Only spark version &gt;= 3.4 or Databricks Runtime 12.x</span>
<span class="sd">            supports setting this argument.</span>
<span class="sd">        num_cpus_head_node: Number of cpus available to Ray head node, if not provide,</span>
<span class="sd">            if it is global mode Ray cluster, use number of cpu cores in spark driver</span>
<span class="sd">            node, otherwise use 0 instead.</span>
<span class="sd">            use 0 instead. Number 0 means tasks requiring CPU resources are not</span>
<span class="sd">            scheduled to Ray head node.</span>
<span class="sd">        num_gpus_worker_node: Number of gpus available to per-ray worker node, if not</span>
<span class="sd">            provided, if spark stage scheduling is supported, &#39;num_gpus_worker_node&#39;</span>
<span class="sd">            value equals to number of GPUs per spark worker node, otherwise</span>
<span class="sd">            it uses rounded down value of spark application configuration</span>
<span class="sd">            &#39;spark.task.resource.gpu.amount&#39; instead.</span>
<span class="sd">            This argument is only available on spark cluster that is configured with</span>
<span class="sd">            &#39;gpu&#39; resources.</span>
<span class="sd">            **Limitation** Only spark version &gt;= 3.4 or Databricks Runtime 12.x</span>
<span class="sd">            supports setting this argument.</span>
<span class="sd">        num_gpus_head_node: Number of gpus available to Ray head node, if not provide,</span>
<span class="sd">            if it is global mode Ray cluster, use number of GPUs in spark driver node,</span>
<span class="sd">            otherwise use 0 instead.</span>
<span class="sd">            This argument is only available on spark cluster which spark driver node</span>
<span class="sd">            has GPUs.</span>
<span class="sd">        memory_worker_node: Optional[int]:</span>
<span class="sd">            Heap memory configured for Ray worker node. This is basically setting</span>
<span class="sd">            `--memory` option when starting Ray node by `ray start` command.</span>
<span class="sd">        memory_head_node: Optional[int]:</span>
<span class="sd">            Heap memory configured for Ray head node. This is basically setting</span>
<span class="sd">            `--memory` option when starting Ray node by `ray start` command.</span>
<span class="sd">        object_store_memory_worker_node: Object store memory available to per-ray worker</span>
<span class="sd">            node, but it is capped by</span>
<span class="sd">            &quot;dev_shm_available_size * 0.8 / num_tasks_per_spark_worker&quot;.</span>
<span class="sd">            The default value equals to</span>
<span class="sd">            &quot;0.3 * spark_worker_physical_memory * 0.8 / num_tasks_per_spark_worker&quot;.</span>
<span class="sd">        object_store_memory_head_node: Object store memory available to Ray head</span>
<span class="sd">            node, but it is capped by &quot;dev_shm_available_size * 0.8&quot;.</span>
<span class="sd">            The default value equals to</span>
<span class="sd">            &quot;0.3 * spark_driver_physical_memory * 0.8&quot;.</span>
<span class="sd">        head_node_options: A dict representing Ray head node extra options, these</span>
<span class="sd">            options will be passed to `ray start` script. Note you need to convert</span>
<span class="sd">            `ray start` options key from `--foo-bar` format to `foo_bar` format.</span>
<span class="sd">            For flag options (e.g. &#39;--disable-usage-stats&#39;), you should set the value</span>
<span class="sd">            to None in the option dict, like `{&quot;disable_usage_stats&quot;: None}`.</span>
<span class="sd">            Note: Short name options (e.g. &#39;-v&#39;) are not supported.</span>
<span class="sd">        worker_node_options: A dict representing Ray worker node extra options,</span>
<span class="sd">            these options will be passed to `ray start` script. Note you need to</span>
<span class="sd">            convert `ray start` options key from `--foo-bar` format to `foo_bar`</span>
<span class="sd">            format.</span>
<span class="sd">            For flag options (e.g. &#39;--disable-usage-stats&#39;), you should set the value</span>
<span class="sd">            to None in the option dict, like `{&quot;disable_usage_stats&quot;: None}`.</span>
<span class="sd">            Note: Short name options (e.g. &#39;-v&#39;) are not supported.</span>
<span class="sd">        ray_temp_root_dir: A local disk path to store the ray temporary data. The</span>
<span class="sd">            created cluster will create a subdirectory</span>
<span class="sd">            &quot;ray-{head_port}-{random_suffix}&quot; beneath this path.</span>
<span class="sd">        strict_mode: Boolean flag to fast-fail initialization of the ray cluster if</span>
<span class="sd">            the available spark cluster does not have sufficient resources to fulfill</span>
<span class="sd">            the resource allocation for memory, cpu and gpu. When set to true, if the</span>
<span class="sd">            requested resources are not available for recommended minimum recommended</span>
<span class="sd">            functionality, an exception will be raised that details the inadequate</span>
<span class="sd">            spark cluster configuration settings. If overridden as `False`,</span>
<span class="sd">            a warning is raised.</span>
<span class="sd">        collect_log_to_path: If specified, after ray head / worker nodes terminated,</span>
<span class="sd">            collect their logs to the specified path. On Databricks Runtime, we</span>
<span class="sd">            recommend you to specify a local path starts with &#39;/dbfs/&#39;, because the</span>
<span class="sd">            path mounts with a centralized storage device and stored data is persisted</span>
<span class="sd">            after Databricks spark cluster terminated.</span>
<span class="sd">        autoscale_upscaling_speed: If autoscale enabled, it represents the number of</span>
<span class="sd">            nodes allowed to be pending as a multiple of the current number of nodes.</span>
<span class="sd">            The higher the value, the more aggressive upscaling will be. For example,</span>
<span class="sd">            if this is set to 1.0, the cluster can grow in size by at most 100% at any</span>
<span class="sd">            time, so if the cluster currently has 20 nodes, at most 20 pending launches</span>
<span class="sd">            are allowed. The minimum number of pending launches is 5 regardless of</span>
<span class="sd">            this setting.</span>
<span class="sd">            Default value is 1.0, minimum value is 1.0</span>
<span class="sd">        autoscale_idle_timeout_minutes: If autoscale enabled, it represents the number</span>
<span class="sd">            of minutes that need to pass before an idle worker node is removed by the</span>
<span class="sd">            autoscaler. The smaller the value, the more aggressive downscaling will be.</span>
<span class="sd">            Worker nodes are considered idle when they hold no active tasks, actors,</span>
<span class="sd">            or referenced objects (either in-memory or spilled to disk). This parameter</span>
<span class="sd">            does not affect the head node.</span>
<span class="sd">            Default value is 1.0, minimum value is 0</span>
<span class="sd">    Returns:</span>
<span class="sd">        returns a tuple of (address, remote_connection_address)</span>
<span class="sd">        &quot;address&quot; is in format of &quot;&lt;ray_head_node_ip&gt;:&lt;port&gt;&quot;</span>
<span class="sd">        &quot;remote_connection_address&quot; is in format of</span>
<span class="sd">        &quot;ray://&lt;ray_head_node_ip&gt;:&lt;ray-client-server-port&gt;&quot;,</span>
<span class="sd">        if your client runs on a machine that also hosts a Ray cluster node locally,</span>
<span class="sd">        you can connect to the Ray cluster via ``ray.init(address)``,</span>
<span class="sd">        otherwise you can connect to the Ray cluster via</span>
<span class="sd">        ``ray.init(remote_connection_address)``.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">_setup_ray_cluster_internal</span><span class="p">(</span>
        <span class="n">max_worker_nodes</span><span class="o">=</span><span class="n">max_worker_nodes</span><span class="p">,</span>
        <span class="n">min_worker_nodes</span><span class="o">=</span><span class="n">min_worker_nodes</span><span class="p">,</span>
        <span class="n">num_cpus_worker_node</span><span class="o">=</span><span class="n">num_cpus_worker_node</span><span class="p">,</span>
        <span class="n">num_cpus_head_node</span><span class="o">=</span><span class="n">num_cpus_head_node</span><span class="p">,</span>
        <span class="n">num_gpus_worker_node</span><span class="o">=</span><span class="n">num_gpus_worker_node</span><span class="p">,</span>
        <span class="n">num_gpus_head_node</span><span class="o">=</span><span class="n">num_gpus_head_node</span><span class="p">,</span>
        <span class="n">heap_memory_worker_node</span><span class="o">=</span><span class="n">memory_worker_node</span><span class="p">,</span>
        <span class="n">heap_memory_head_node</span><span class="o">=</span><span class="n">memory_head_node</span><span class="p">,</span>
        <span class="n">object_store_memory_worker_node</span><span class="o">=</span><span class="n">object_store_memory_worker_node</span><span class="p">,</span>
        <span class="n">object_store_memory_head_node</span><span class="o">=</span><span class="n">object_store_memory_head_node</span><span class="p">,</span>
        <span class="n">head_node_options</span><span class="o">=</span><span class="n">head_node_options</span><span class="p">,</span>
        <span class="n">worker_node_options</span><span class="o">=</span><span class="n">worker_node_options</span><span class="p">,</span>
        <span class="n">ray_temp_root_dir</span><span class="o">=</span><span class="n">ray_temp_root_dir</span><span class="p">,</span>
        <span class="n">strict_mode</span><span class="o">=</span><span class="n">strict_mode</span><span class="p">,</span>
        <span class="n">collect_log_to_path</span><span class="o">=</span><span class="n">collect_log_to_path</span><span class="p">,</span>
        <span class="n">autoscale_upscaling_speed</span><span class="o">=</span><span class="n">autoscale_upscaling_speed</span><span class="p">,</span>
        <span class="n">autoscale_idle_timeout_minutes</span><span class="o">=</span><span class="n">autoscale_idle_timeout_minutes</span><span class="p">,</span>
        <span class="n">is_global</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span></div>



<div class="viewcode-block" id="setup_global_ray_cluster">
<a class="viewcode-back" href="../../../../cluster/vms/user-guides/community/spark.html#ray.util.spark.setup_global_ray_cluster">[docs]</a>
<span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">setup_global_ray_cluster</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_worker_nodes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">is_blocking</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">min_worker_nodes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_cpus_worker_node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_cpus_head_node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_gpus_worker_node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_gpus_head_node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">memory_worker_node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">memory_head_node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">object_store_memory_worker_node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">object_store_memory_head_node</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_node_options</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">worker_node_options</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">strict_mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">collect_log_to_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">autoscale_upscaling_speed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">autoscale_idle_timeout_minutes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set up a global mode cluster.</span>
<span class="sd">    The global Ray on spark cluster means:</span>
<span class="sd">    - You can only create one active global Ray on spark cluster at a time.</span>
<span class="sd">    On databricks cluster, the global Ray cluster can be used by all users,</span>
<span class="sd">    - as contrast, non-global Ray cluster can only be used by current notebook</span>
<span class="sd">    user.</span>
<span class="sd">    - It is up persistently without automatic shutdown.</span>
<span class="sd">    - On databricks notebook, you can connect to the global cluster by calling</span>
<span class="sd">    ``ray.init()`` without specifying its address, it will discover the</span>
<span class="sd">    global cluster automatically if it is up.</span>

<span class="sd">    For global mode, the ``ray_temp_root_dir`` argument is not supported.</span>
<span class="sd">    Global model Ray cluster always use the default Ray temporary directory</span>
<span class="sd">    path.</span>

<span class="sd">    All arguments are the same with ``setup_ray_cluster`` API except that:</span>
<span class="sd">    - the ``ray_temp_root_dir`` argument is not supported.</span>
<span class="sd">    Global model Ray cluster always use the default Ray temporary directory</span>
<span class="sd">    path.</span>
<span class="sd">    - A new argument &quot;is_blocking&quot; (default ``True``) is added.</span>
<span class="sd">    If &quot;is_blocking&quot; is True,</span>
<span class="sd">    then keep the call blocking until it is interrupted.</span>
<span class="sd">    once the call is interrupted, the global Ray on spark cluster is shut down and</span>
<span class="sd">    `setup_global_ray_cluster` call terminates.</span>
<span class="sd">    If &quot;is_blocking&quot; is False,</span>
<span class="sd">    once Ray cluster setup completes, return immediately.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">cluster_address</span> <span class="o">=</span> <span class="n">_setup_ray_cluster_internal</span><span class="p">(</span>
        <span class="n">max_worker_nodes</span><span class="o">=</span><span class="n">max_worker_nodes</span><span class="p">,</span>
        <span class="n">min_worker_nodes</span><span class="o">=</span><span class="n">min_worker_nodes</span><span class="p">,</span>
        <span class="n">num_cpus_worker_node</span><span class="o">=</span><span class="n">num_cpus_worker_node</span><span class="p">,</span>
        <span class="n">num_cpus_head_node</span><span class="o">=</span><span class="n">num_cpus_head_node</span><span class="p">,</span>
        <span class="n">num_gpus_worker_node</span><span class="o">=</span><span class="n">num_gpus_worker_node</span><span class="p">,</span>
        <span class="n">num_gpus_head_node</span><span class="o">=</span><span class="n">num_gpus_head_node</span><span class="p">,</span>
        <span class="n">heap_memory_worker_node</span><span class="o">=</span><span class="n">memory_worker_node</span><span class="p">,</span>
        <span class="n">heap_memory_head_node</span><span class="o">=</span><span class="n">memory_head_node</span><span class="p">,</span>
        <span class="n">object_store_memory_worker_node</span><span class="o">=</span><span class="n">object_store_memory_worker_node</span><span class="p">,</span>
        <span class="n">object_store_memory_head_node</span><span class="o">=</span><span class="n">object_store_memory_head_node</span><span class="p">,</span>
        <span class="n">head_node_options</span><span class="o">=</span><span class="n">head_node_options</span><span class="p">,</span>
        <span class="n">worker_node_options</span><span class="o">=</span><span class="n">worker_node_options</span><span class="p">,</span>
        <span class="n">ray_temp_root_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">strict_mode</span><span class="o">=</span><span class="n">strict_mode</span><span class="p">,</span>
        <span class="n">collect_log_to_path</span><span class="o">=</span><span class="n">collect_log_to_path</span><span class="p">,</span>
        <span class="n">autoscale_upscaling_speed</span><span class="o">=</span><span class="n">autoscale_upscaling_speed</span><span class="p">,</span>
        <span class="n">autoscale_idle_timeout_minutes</span><span class="o">=</span><span class="n">autoscale_idle_timeout_minutes</span><span class="p">,</span>
        <span class="n">is_global</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_blocking</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">cluster_address</span>

    <span class="k">global</span> <span class="n">_global_ray_cluster_cancel_event</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">_global_ray_cluster_cancel_event</span> <span class="o">=</span> <span class="n">Event</span><span class="p">()</span>
        <span class="c1"># serve forever until user cancel the command.</span>
        <span class="n">_global_ray_cluster_cancel_event</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">_global_ray_cluster_cancel_event</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># once the program is interrupted,</span>
        <span class="c1"># or the corresponding databricks notebook command is interrupted</span>
        <span class="c1"># shut down the Ray cluster.</span>
        <span class="n">shutdown_ray_cluster</span><span class="p">()</span></div>



<span class="k">def</span> <span class="nf">_start_ray_worker_nodes</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">spark_job_server</span><span class="p">,</span>
    <span class="n">spark_job_group_id</span><span class="p">,</span>
    <span class="n">spark_job_group_desc</span><span class="p">,</span>
    <span class="n">num_worker_nodes</span><span class="p">,</span>
    <span class="n">using_stage_scheduling</span><span class="p">,</span>
    <span class="n">ray_head_ip</span><span class="p">,</span>
    <span class="n">ray_head_port</span><span class="p">,</span>
    <span class="n">ray_temp_dir</span><span class="p">,</span>
    <span class="n">num_cpus_per_node</span><span class="p">,</span>
    <span class="n">num_gpus_per_node</span><span class="p">,</span>
    <span class="n">heap_memory_per_node</span><span class="p">,</span>
    <span class="n">object_store_memory_per_node</span><span class="p">,</span>
    <span class="n">worker_node_options</span><span class="p">,</span>
    <span class="n">collect_log_to_path</span><span class="p">,</span>
    <span class="n">node_id</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># NB:</span>
    <span class="c1"># In order to start ray worker nodes on spark cluster worker machines,</span>
    <span class="c1"># We launch a background spark job:</span>
    <span class="c1">#  1. Each spark task launches one ray worker node. This design ensures all ray</span>
    <span class="c1">#     worker nodes have the same shape (same cpus / gpus / memory configuration).</span>
    <span class="c1">#     If ray worker nodes have a non-uniform shape, the Ray cluster setup will</span>
    <span class="c1">#     be non-deterministic and could create issues with node sizing.</span>
    <span class="c1">#  2. A ray worker node is started via the `ray start` CLI. In each spark task,</span>
    <span class="c1">#     a child process is started and will execute a `ray start ...` command in</span>
    <span class="c1">#     blocking mode.</span>
    <span class="c1">#  3. Each task will acquire a file lock for 10s to ensure that the ray worker</span>
    <span class="c1">#     init will acquire a port connection to the ray head node that does not</span>
    <span class="c1">#     contend with other worker processes on the same Spark worker node.</span>
    <span class="c1">#  4. When the ray cluster is shutdown, killing ray worker nodes is implemented by</span>
    <span class="c1">#     `sparkContext.cancelJobGroup` to cancel the background spark job, sending a</span>
    <span class="c1">#     SIGKILL signal to all spark tasks. Once the spark tasks are killed,</span>
    <span class="c1">#     `ray_start_node` process detects parent died event then it kills ray</span>
    <span class="c1">#     worker node.</span>
    <span class="n">spark</span> <span class="o">=</span> <span class="n">spark_job_server</span><span class="o">.</span><span class="n">spark</span>
    <span class="n">spark_job_server_port</span> <span class="o">=</span> <span class="n">spark_job_server</span><span class="o">.</span><span class="n">server_address</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">ray_node_custom_env</span> <span class="o">=</span> <span class="n">spark_job_server</span><span class="o">.</span><span class="n">ray_node_custom_env</span>

    <span class="k">def</span> <span class="nf">ray_cluster_job_mapper</span><span class="p">(</span><span class="n">_</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">pyspark.taskcontext</span> <span class="kn">import</span> <span class="n">TaskContext</span>

        <span class="n">_worker_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;ray.util.spark.worker&quot;</span><span class="p">)</span>

        <span class="n">context</span> <span class="o">=</span> <span class="n">TaskContext</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>

        <span class="p">(</span>
            <span class="n">worker_port_range_begin</span><span class="p">,</span>
            <span class="n">worker_port_range_end</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="n">_preallocate_ray_worker_port_range</span><span class="p">()</span>

        <span class="c1"># 10001 is used as ray client server port of global mode ray cluster.</span>
        <span class="n">ray_worker_node_dashboard_agent_port</span> <span class="o">=</span> <span class="n">get_random_unused_port</span><span class="p">(</span>
            <span class="n">ray_head_ip</span><span class="p">,</span> <span class="n">min_port</span><span class="o">=</span><span class="mi">10002</span><span class="p">,</span> <span class="n">max_port</span><span class="o">=</span><span class="mi">20000</span>
        <span class="p">)</span>
        <span class="n">ray_worker_node_cmd</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">executable</span><span class="p">,</span>
            <span class="s2">&quot;-m&quot;</span><span class="p">,</span>
            <span class="s2">&quot;ray.util.spark.start_ray_node&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--num-cpus=</span><span class="si">{</span><span class="n">num_cpus_per_node</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="s2">&quot;--block&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--address=</span><span class="si">{</span><span class="n">ray_head_ip</span><span class="si">}</span><span class="s2">:</span><span class="si">{</span><span class="n">ray_head_port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--memory=</span><span class="si">{</span><span class="n">heap_memory_per_node</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--object-store-memory=</span><span class="si">{</span><span class="n">object_store_memory_per_node</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--min-worker-port=</span><span class="si">{</span><span class="n">worker_port_range_begin</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--max-worker-port=</span><span class="si">{</span><span class="n">worker_port_range_end</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--dashboard-agent-listen-port=</span><span class="si">{</span><span class="n">ray_worker_node_dashboard_agent_port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="o">*</span><span class="n">_convert_ray_node_options</span><span class="p">(</span><span class="n">worker_node_options</span><span class="p">),</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="n">ray_temp_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">ray_worker_node_cmd</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;--temp-dir=</span><span class="si">{</span><span class="n">ray_temp_dir</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">ray_worker_node_extra_envs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">RAY_ON_SPARK_COLLECT_LOG_TO_PATH</span><span class="p">:</span> <span class="n">collect_log_to_path</span> <span class="ow">or</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
            <span class="n">RAY_ON_SPARK_START_RAY_PARENT_PID</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getpid</span><span class="p">()),</span>
            <span class="s2">&quot;RAY_ENABLE_WINDOWS_OR_OSX_CLUSTER&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">,</span>
            <span class="o">**</span><span class="n">ray_node_custom_env</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="n">num_gpus_per_node</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">task_resources</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">resources</span><span class="p">()</span>

            <span class="k">if</span> <span class="s2">&quot;gpu&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">task_resources</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;Couldn&#39;t get the gpu id, Please check the GPU resource &quot;</span>
                    <span class="s2">&quot;configuration&quot;</span>
                <span class="p">)</span>
            <span class="n">gpu_addr_list</span> <span class="o">=</span> <span class="p">[</span>
                <span class="nb">int</span><span class="p">(</span><span class="n">addr</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span> <span class="k">for</span> <span class="n">addr</span> <span class="ow">in</span> <span class="n">task_resources</span><span class="p">[</span><span class="s2">&quot;gpu&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">addresses</span>
            <span class="p">]</span>

            <span class="n">available_physical_gpus</span> <span class="o">=</span> <span class="n">get_spark_task_assigned_physical_gpus</span><span class="p">(</span>
                <span class="n">gpu_addr_list</span>
            <span class="p">)</span>
            <span class="n">ray_worker_node_cmd</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;--num-gpus=</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">available_physical_gpus</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">ray_worker_node_extra_envs</span><span class="p">[</span><span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">gpu_id</span><span class="p">)</span> <span class="k">for</span> <span class="n">gpu_id</span> <span class="ow">in</span> <span class="n">available_physical_gpus</span><span class="p">]</span>
            <span class="p">)</span>

        <span class="n">_worker_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Start Ray worker, command: </span><span class="si">{</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ray_worker_node_cmd</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">is_task_reschedule_failure</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="c1"># Check node id availability</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
                <span class="n">url</span><span class="o">=</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;http://</span><span class="si">{</span><span class="n">ray_head_ip</span><span class="si">}</span><span class="s2">:</span><span class="si">{</span><span class="n">spark_job_server_port</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="s2">&quot;/check_node_id_availability&quot;</span>
                <span class="p">),</span>
                <span class="n">json</span><span class="o">=</span><span class="p">{</span>
                    <span class="s2">&quot;node_id&quot;</span><span class="p">:</span> <span class="n">node_id</span><span class="p">,</span>
                    <span class="s2">&quot;spark_job_group_id&quot;</span><span class="p">:</span> <span class="n">spark_job_group_id</span><span class="p">,</span>
                <span class="p">},</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()[</span><span class="s2">&quot;available&quot;</span><span class="p">]:</span>
                <span class="c1"># The case happens when a Ray node is down unexpected</span>
                <span class="c1"># caused by spark worker node down and spark tries to</span>
                <span class="c1"># reschedule the spark task, so it triggers node</span>
                <span class="c1"># creation with duplicated node id.</span>
                <span class="c1"># in this case, finish the spark task immediately</span>
                <span class="c1"># so spark won&#39;t try to reschedule this task</span>
                <span class="c1"># and Ray autoscaler will trigger a new node creation</span>
                <span class="c1"># with new node id, and a new spark job will be created</span>
                <span class="c1"># for holding it.</span>
                <span class="n">is_task_reschedule_failure</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;Starting Ray worker node twice with the same node id &quot;</span>
                    <span class="s2">&quot;is not allowed.&quot;</span>
                <span class="p">)</span>

            <span class="c1"># Notify job server the task has been launched.</span>
            <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
                <span class="n">url</span><span class="o">=</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;http://</span><span class="si">{</span><span class="n">ray_head_ip</span><span class="si">}</span><span class="s2">:</span><span class="si">{</span><span class="n">spark_job_server_port</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="s2">&quot;/notify_task_launched&quot;</span>
                <span class="p">),</span>
                <span class="n">json</span><span class="o">=</span><span class="p">{</span>
                    <span class="s2">&quot;spark_job_group_id&quot;</span><span class="p">:</span> <span class="n">spark_job_group_id</span><span class="p">,</span>
                <span class="p">},</span>
            <span class="p">)</span>

            <span class="c1"># Note:</span>
            <span class="c1"># When a pyspark job cancelled, the UDF python worker process are killed by</span>
            <span class="c1"># signal &quot;SIGKILL&quot;, then `start_ray_node` process will detect the parent</span>
            <span class="c1"># died event (see `ray.util.spark.start_ray_node.check_parent_alive`) and</span>
            <span class="c1"># then kill ray worker node process and execute cleanup routine.</span>
            <span class="n">exec_cmd</span><span class="p">(</span>
                <span class="n">ray_worker_node_cmd</span><span class="p">,</span>
                <span class="n">synchronous</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">extra_env</span><span class="o">=</span><span class="n">ray_worker_node_extra_envs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="c1"># In the following 2 cases, exception is raised:</span>
            <span class="c1"># (1)</span>
            <span class="c1"># Starting Ray worker node fails, the `e` will contain detail</span>
            <span class="c1"># subprocess stdout/stderr output.</span>
            <span class="c1"># (2)</span>
            <span class="c1"># In autoscaling mode, when Ray worker node is down, autoscaler will</span>
            <span class="c1"># try to start new Ray worker node if necessary,</span>
            <span class="c1"># and it creates a new spark job to launch Ray worker node process,</span>
            <span class="c1"># note the old spark job will reschedule the failed spark task</span>
            <span class="c1"># and raise error of &quot;Starting Ray worker node twice with the same</span>
            <span class="c1"># node id is not allowed&quot;.</span>
            <span class="c1">#</span>
            <span class="c1"># For either case (1) or case (2),</span>
            <span class="c1"># to avoid Spark triggers more spark task retries, we swallow</span>
            <span class="c1"># exception here to make spark the task exit normally.</span>
            <span class="n">err_msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Ray worker node process exit, reason: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">err_msg</span><span class="p">)</span>

            <span class="k">yield</span> <span class="n">err_msg</span><span class="p">,</span> <span class="n">is_task_reschedule_failure</span>

    <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">setJobGroup</span><span class="p">(</span>
        <span class="n">spark_job_group_id</span><span class="p">,</span>
        <span class="n">spark_job_group_desc</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Starting a normal spark job (not barrier spark job) to run ray worker</span>
    <span class="c1"># nodes, the design purpose is:</span>
    <span class="c1"># 1. Using normal spark job, spark tasks can automatically retry</span>
    <span class="c1"># individually, we don&#39;t need to write additional retry logic, But, in</span>
    <span class="c1"># barrier mode, if one spark task fails, it will cause all other spark</span>
    <span class="c1"># tasks killed.</span>
    <span class="c1"># 2. Using normal spark job, we can support failover when a spark worker</span>
    <span class="c1"># physical machine crashes. (spark will try to re-schedule the spark task</span>
    <span class="c1"># to other spark worker nodes)</span>
    <span class="c1"># 3. Using barrier mode job, if the cluster resources does not satisfy</span>
    <span class="c1"># &quot;idle spark task slots &gt;= argument num_spark_task&quot;, then the barrier</span>
    <span class="c1"># job gets stuck and waits until enough idle task slots available, this</span>
    <span class="c1"># behavior is not user-friendly, on a shared spark cluster, user is hard</span>
    <span class="c1"># to estimate how many idle tasks available at a time, But, if using normal</span>
    <span class="c1"># spark job, it can launch job with less spark tasks (i.e. user will see a</span>
    <span class="c1"># ray cluster setup with less worker number initially), and when more task</span>
    <span class="c1"># slots become available, it continues to launch tasks on new available</span>
    <span class="c1"># slots, and user can see the ray cluster worker number increases when more</span>
    <span class="c1"># slots available.</span>
    <span class="n">job_rdd</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span>
        <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_worker_nodes</span><span class="p">)),</span> <span class="n">num_worker_nodes</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">using_stage_scheduling</span><span class="p">:</span>
        <span class="n">resource_profile</span> <span class="o">=</span> <span class="n">_create_resource_profile</span><span class="p">(</span>
            <span class="n">num_cpus_per_node</span><span class="p">,</span>
            <span class="n">num_gpus_per_node</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">job_rdd</span> <span class="o">=</span> <span class="n">job_rdd</span><span class="o">.</span><span class="n">withResources</span><span class="p">(</span><span class="n">resource_profile</span><span class="p">)</span>

    <span class="n">hook_entry</span> <span class="o">=</span> <span class="n">_create_hook_entry</span><span class="p">(</span><span class="n">is_global</span><span class="o">=</span><span class="p">(</span><span class="n">ray_temp_dir</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">))</span>
    <span class="n">hook_entry</span><span class="o">.</span><span class="n">on_spark_job_created</span><span class="p">(</span><span class="n">spark_job_group_id</span><span class="p">)</span>

    <span class="n">err_msg</span><span class="p">,</span> <span class="n">is_task_reschedule_failure</span> <span class="o">=</span> <span class="n">job_rdd</span><span class="o">.</span><span class="n">mapPartitions</span><span class="p">(</span>
        <span class="n">ray_cluster_job_mapper</span>
    <span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_task_reschedule_failure</span><span class="p">:</span>
        <span class="n">spark_job_server</span><span class="o">.</span><span class="n">last_worker_error</span> <span class="o">=</span> <span class="n">err_msg</span>
        <span class="k">return</span> <span class="n">err_msg</span>

    <span class="k">return</span> <span class="kc">None</span>


<div class="viewcode-block" id="shutdown_ray_cluster">
<a class="viewcode-back" href="../../../../cluster/vms/user-guides/community/spark.html#ray.util.spark.shutdown_ray_cluster">[docs]</a>
<span class="nd">@PublicAPI</span>
<span class="k">def</span> <span class="nf">shutdown_ray_cluster</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Shut down the active ray cluster.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_active_ray_cluster</span>

    <span class="k">with</span> <span class="n">_active_ray_cluster_rwlock</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">_active_ray_cluster</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;No active ray cluster to shut down.&quot;</span><span class="p">)</span>

        <span class="n">_active_ray_cluster</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
        <span class="n">_active_ray_cluster</span> <span class="o">=</span> <span class="kc">None</span></div>



<span class="n">_global_ray_cluster_cancel_event</span> <span class="o">=</span> <span class="kc">None</span>


<span class="nd">@DeveloperAPI</span>
<span class="k">class</span> <span class="nc">AutoscalingCluster</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create a ray on spark autoscaling cluster.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">head_resources</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
        <span class="n">worker_node_types</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
        <span class="n">extra_provider_config</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
        <span class="n">upscaling_speed</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">idle_timeout_minutes</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create the cluster.</span>

<span class="sd">        Args:</span>
<span class="sd">            head_resources: resources of the head node, including CPU.</span>
<span class="sd">            worker_node_types: autoscaler node types config for worker nodes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_head_resources</span> <span class="o">=</span> <span class="n">head_resources</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_head_resources</span><span class="p">[</span><span class="s2">&quot;NODE_ID_AS_RESOURCE&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">HEAD_NODE_ID</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_config</span><span class="p">(</span>
            <span class="n">head_resources</span><span class="p">,</span>
            <span class="n">worker_node_types</span><span class="p">,</span>
            <span class="n">extra_provider_config</span><span class="p">,</span>
            <span class="n">upscaling_speed</span><span class="p">,</span>
            <span class="n">idle_timeout_minutes</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_generate_config</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">head_resources</span><span class="p">,</span>
        <span class="n">worker_node_types</span><span class="p">,</span>
        <span class="n">extra_provider_config</span><span class="p">,</span>
        <span class="n">upscaling_speed</span><span class="p">,</span>
        <span class="n">idle_timeout_minutes</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">base_config</span> <span class="o">=</span> <span class="n">yaml</span><span class="o">.</span><span class="n">safe_load</span><span class="p">(</span>
            <span class="nb">open</span><span class="p">(</span>
                <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">ray</span><span class="o">.</span><span class="vm">__file__</span><span class="p">),</span>
                    <span class="s2">&quot;autoscaler/spark/defaults.yaml&quot;</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="n">custom_config</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">base_config</span><span class="p">)</span>
        <span class="n">custom_config</span><span class="p">[</span><span class="s2">&quot;available_node_types&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">worker_node_types</span>
        <span class="n">custom_config</span><span class="p">[</span><span class="s2">&quot;available_node_types&quot;</span><span class="p">][</span><span class="s2">&quot;ray.head.default&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;resources&quot;</span><span class="p">:</span> <span class="n">head_resources</span><span class="p">,</span>
            <span class="s2">&quot;node_config&quot;</span><span class="p">:</span> <span class="p">{},</span>
            <span class="s2">&quot;max_workers&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="n">custom_config</span><span class="p">[</span><span class="s2">&quot;max_workers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span>
            <span class="n">v</span><span class="p">[</span><span class="s2">&quot;max_workers&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">worker_node_types</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="n">custom_config</span><span class="p">[</span><span class="s2">&quot;provider&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">extra_provider_config</span><span class="p">)</span>

        <span class="n">custom_config</span><span class="p">[</span><span class="s2">&quot;upscaling_speed&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">upscaling_speed</span>
        <span class="n">custom_config</span><span class="p">[</span><span class="s2">&quot;idle_timeout_minutes&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">idle_timeout_minutes</span>

        <span class="k">return</span> <span class="n">custom_config</span>

    <span class="k">def</span> <span class="nf">start</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">ray_head_ip</span><span class="p">,</span>
        <span class="n">ray_head_port</span><span class="p">,</span>
        <span class="n">ray_client_server_port</span><span class="p">,</span>
        <span class="n">ray_temp_dir</span><span class="p">,</span>
        <span class="n">dashboard_options</span><span class="p">,</span>
        <span class="n">head_node_options</span><span class="p">,</span>
        <span class="n">collect_log_to_path</span><span class="p">,</span>
        <span class="n">ray_node_custom_env</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Start the cluster.</span>

<span class="sd">        After this call returns, you can connect to the cluster with</span>
<span class="sd">        ray.init(&quot;auto&quot;).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">ray.util.spark.cluster_init</span> <span class="kn">import</span> <span class="p">(</span>
            <span class="n">RAY_ON_SPARK_COLLECT_LOG_TO_PATH</span><span class="p">,</span>
            <span class="n">_append_resources_config</span><span class="p">,</span>
            <span class="n">_convert_ray_node_options</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">ray_temp_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">autoscale_config</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ray_temp_dir</span><span class="p">,</span> <span class="s2">&quot;autoscaling_config.json&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">autoscale_config</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                <span class="n">_get_default_ray_tmp_dir</span><span class="p">(),</span> <span class="s2">&quot;autoscaling_config.json&quot;</span>
            <span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">autoscale_config</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_config</span><span class="p">))</span>

        <span class="p">(</span>
            <span class="n">worker_port_range_begin</span><span class="p">,</span>
            <span class="n">worker_port_range_end</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="n">_preallocate_ray_worker_port_range</span><span class="p">()</span>

        <span class="n">ray_head_node_cmd</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">executable</span><span class="p">,</span>
            <span class="s2">&quot;-m&quot;</span><span class="p">,</span>
            <span class="s2">&quot;ray.util.spark.start_ray_node&quot;</span><span class="p">,</span>
            <span class="s2">&quot;--block&quot;</span><span class="p">,</span>
            <span class="s2">&quot;--head&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--node-ip-address=</span><span class="si">{</span><span class="n">ray_head_ip</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--port=</span><span class="si">{</span><span class="n">ray_head_port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--ray-client-server-port=</span><span class="si">{</span><span class="n">ray_client_server_port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--autoscaling-config=</span><span class="si">{</span><span class="n">autoscale_config</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--min-worker-port=</span><span class="si">{</span><span class="n">worker_port_range_begin</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;--max-worker-port=</span><span class="si">{</span><span class="n">worker_port_range_end</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="o">*</span><span class="n">dashboard_options</span><span class="p">,</span>
        <span class="p">]</span>

        <span class="k">if</span> <span class="n">ray_temp_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">ray_head_node_cmd</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;--temp-dir=</span><span class="si">{</span><span class="n">ray_temp_dir</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;CPU&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_head_resources</span><span class="p">:</span>
            <span class="n">ray_head_node_cmd</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="s2">&quot;--num-cpus=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_head_resources</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;CPU&quot;</span><span class="p">))</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;GPU&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_head_resources</span><span class="p">:</span>
            <span class="n">ray_head_node_cmd</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="s2">&quot;--num-gpus=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_head_resources</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;GPU&quot;</span><span class="p">))</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;memory&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_head_resources</span><span class="p">:</span>
            <span class="n">ray_head_node_cmd</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="s2">&quot;--memory=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_head_resources</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;memory&quot;</span><span class="p">))</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;object_store_memory&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_head_resources</span><span class="p">:</span>
            <span class="n">ray_head_node_cmd</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="s2">&quot;--object-store-memory=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_head_resources</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;object_store_memory&quot;</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="n">head_node_options</span> <span class="o">=</span> <span class="n">_append_resources_config</span><span class="p">(</span>
            <span class="n">head_node_options</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_head_resources</span>
        <span class="p">)</span>
        <span class="n">ray_head_node_cmd</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">_convert_ray_node_options</span><span class="p">(</span><span class="n">head_node_options</span><span class="p">))</span>

        <span class="n">extra_env</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;AUTOSCALER_UPDATE_INTERVAL_S&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">,</span>
            <span class="n">RAY_ON_SPARK_COLLECT_LOG_TO_PATH</span><span class="p">:</span> <span class="n">collect_log_to_path</span> <span class="ow">or</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
            <span class="n">RAY_ON_SPARK_START_RAY_PARENT_PID</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getpid</span><span class="p">()),</span>
            <span class="o">**</span><span class="n">ray_node_custom_env</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ray_head_node_cmd</span> <span class="o">=</span> <span class="n">ray_head_node_cmd</span>

        <span class="k">return</span> <span class="n">_start_ray_head_node</span><span class="p">(</span>
            <span class="n">ray_head_node_cmd</span><span class="p">,</span> <span class="n">synchronous</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">extra_env</span><span class="o">=</span><span class="n">extra_env</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">_start_ray_head_node</span><span class="p">(</span><span class="n">ray_head_node_cmd</span><span class="p">,</span> <span class="n">synchronous</span><span class="p">,</span> <span class="n">extra_env</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">preexec_function</span><span class="p">():</span>
        <span class="c1"># Make `start_ray_node` script and Ray node process run</span>
        <span class="c1"># in a separate group,</span>
        <span class="c1"># otherwise Ray node will be in the same group of parent process,</span>
        <span class="c1"># if parent process is a Jupyter notebook kernel, when user</span>
        <span class="c1"># clicks interrupt cell button, SIGINT signal is sent, then Ray node will</span>
        <span class="c1"># receive SIGINT signal, and it causes Ray node process dies.</span>
        <span class="c1"># `start_ray_node` script should also run in a separate group</span>
        <span class="c1"># because on Databricks Runtime, because if Databricks notebook</span>
        <span class="c1"># is detached, if the children processes don&#39;t exit within 1s,</span>
        <span class="c1"># they will receive SIGKILL, this behavior makes start_ray_node</span>
        <span class="c1"># doesn&#39;t have enough time to complete cleanup work like removing</span>
        <span class="c1"># temp directory and collecting logs.</span>
        <span class="n">os</span><span class="o">.</span><span class="n">setpgrp</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">exec_cmd</span><span class="p">(</span>
        <span class="n">ray_head_node_cmd</span><span class="p">,</span>
        <span class="n">synchronous</span><span class="o">=</span><span class="n">synchronous</span><span class="p">,</span>
        <span class="n">extra_env</span><span class="o">=</span><span class="n">extra_env</span><span class="p">,</span>
        <span class="n">preexec_fn</span><span class="o">=</span><span class="n">preexec_function</span><span class="p">,</span>
    <span class="p">)</span>


<span class="n">_sigterm_signal_installed</span> <span class="o">=</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">_install_sigterm_signal</span><span class="p">():</span>
    <span class="k">global</span> <span class="n">_sigterm_signal_installed</span>

    <span class="k">if</span> <span class="n">_sigterm_signal_installed</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">_origin_sigterm_handler</span> <span class="o">=</span> <span class="n">signal</span><span class="o">.</span><span class="n">getsignal</span><span class="p">(</span><span class="n">signal</span><span class="o">.</span><span class="n">SIGTERM</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">_sigterm_handler</span><span class="p">(</span><span class="n">signum</span><span class="p">,</span> <span class="n">frame</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">shutdown_ray_cluster</span><span class="p">()</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                <span class="c1"># swallow exception to continue executing the following code in the</span>
                <span class="c1"># handler</span>
                <span class="k">pass</span>
            <span class="n">signal</span><span class="o">.</span><span class="n">signal</span><span class="p">(</span>
                <span class="n">signal</span><span class="o">.</span><span class="n">SIGTERM</span><span class="p">,</span> <span class="n">_origin_sigterm_handler</span>
            <span class="p">)</span>  <span class="c1"># Reset to original signal</span>
            <span class="n">os</span><span class="o">.</span><span class="n">kill</span><span class="p">(</span>
                <span class="n">os</span><span class="o">.</span><span class="n">getpid</span><span class="p">(),</span> <span class="n">signal</span><span class="o">.</span><span class="n">SIGTERM</span>
            <span class="p">)</span>  <span class="c1"># Re-raise the signal to trigger original behavior</span>

        <span class="n">signal</span><span class="o">.</span><span class="n">signal</span><span class="p">(</span><span class="n">signal</span><span class="o">.</span><span class="n">SIGTERM</span><span class="p">,</span> <span class="n">_sigterm_handler</span><span class="p">)</span>
        <span class="n">_sigterm_signal_installed</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
        <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Install Ray-on-Spark SIGTERM handler failed.&quot;</span><span class="p">)</span>
</pre></div>

                </article>
              
              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="footer-content-items footer-content__inner">
  
    <div class="footer-content-item"><div id="csat">
  <div id="csat-feedback-received" class="csat-hidden">
    <span>Thanks for the feedback!</span>
  </div>
  <div id="csat-inputs">
    <span>Was this helpful?</span>
    <div id="csat-yes" class="csat-button">
      <svg id="csat-yes-icon" class="csat-hidden csat-icon" width="18" height="13" viewBox="0 0 18 13" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 10.172L16.1922 0.979004L17.6072 2.393L7.00023 13L0.63623 6.636L2.05023 5.222L7.00023 10.172Z" />
      </svg>
      <span>Yes<span>
    </div>
    <div id="csat-no" class="csat-button">
      <svg id="csat-no-icon" class="csat-hidden csat-icon" width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 5.58599L11.9502 0.635986L13.3642 2.04999L8.41423 6.99999L13.3642 11.95L11.9502 13.364L7.00023 8.41399L2.05023 13.364L0.63623 11.95L5.58623 6.99999L0.63623 2.04999L2.05023 0.635986L7.00023 5.58599Z" />
      </svg>
      <span>No<span>
    </div>
  </div>
  <div id="csat-textarea-group" class="csat-hidden">
    <span id="csat-feedback-label">Feedback</span>
    <textarea id="csat-textarea"></textarea>
    <div id="csat-submit">Submit</div>
  </div>
</div></div>
  
</div>

          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794"></script>
<script src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
  <p class="copyright">
    
      Â© Copyright 2025, The Ray Team.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">
  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.3.7.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.14.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>