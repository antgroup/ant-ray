<!-- prettier-ignore -->

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>GPT-J-6B Fine-Tuning with Ray Train and DeepSpeed &#8212; Ray 3.0.0.dev0</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=ac02cc09edc035673794" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/termynal.css?v=2fc3cb5e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/csat.css?v=c8f39c76" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/assistant.css?v=d345c55e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/custom.css?v=633d7681" />
    <link rel="stylesheet" type="text/css" href="../../../_static/docsearch.css?v=45fb5152" />
    <link rel="stylesheet" type="text/css" href="../../../_static/pydata-docsearch-custom.css?v=7fb92720" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794" />
  <script src="../../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=ac02cc09edc035673794"></script>

    <script src="../../../_static/documentation_options.js?v=d1493c90"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=9a29e97e"></script>
    <script defer="defer" src="../../../_static/js/termynal.js?v=67cfcf08"></script>
    <script defer="defer" src="../../../_static/js/custom.js?v=9e3b357f"></script>
    <script src="../../../_static/js/csat.js?v=8e649b1c"></script>
    <script defer="defer" src="../../../_static/js/assistant.js?v=73fdc522"></script>
    <script defer="defer" src="../../../_static/docsearch.js?v=77274085"></script>
    <script defer="defer" src="../../../_static/docsearch_config.js?v=d25523ed"></script>
    <script src="../../../_static/design-tabs.js?v=36754332"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'train/examples/deepspeed/gptj_deepspeed_fine_tuning';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.14.1';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://antgroup.github.io/ant-ray/versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'dev-flow-doc';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = false;
        </script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/train/examples/deepspeed/gptj_deepspeed_fine_tuning.html" />
    <link rel="icon" href="../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" /><!-- Extra header to include at the top of each template.
Kept separately so that it can easily be included in any templates
that need to be overridden for individual pages; e.g. included
both in the usual template (layout.html) as well as (index.html). -->

<link rel="preconnect" href="https://fonts.googleapis.com" />
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
<link
  href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;900&family=Roboto:wght@400;500;700&display=swap"
  rel="stylesheet"
/>
<link
  rel="stylesheet"
  title="light"
  href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css"
  disabled="disabled"
/>
<link
  rel="stylesheet"
  title="dark"
  href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css"
  disabled="disabled"
/>
<link
  href="https://cdn.jsdelivr.net/npm/remixicon@4.1.0/fonts/remixicon.css"
  rel="stylesheet"
/>

<!-- Used for text embedded in html on the index page  -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- Parser used to call hljs on responses from Ray Assistant -->
<script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

<!-- Sanitizer for Ray Assistant AI -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/dompurify/2.3.3/purify.min.js"></script>

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<!-- Google Tag Manager -->
<script>
  (function (w, d, s, l, i) {
    w[l] = w[l] || [];
    w[l].push({
      'gtm.start': new Date().getTime(),
      event: 'gtm.js',
    });
    var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s),
      dl = l != 'dataLayer' ? '&l=' + l : '';
    j.async = true;
    j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
    f.parentNode.insertBefore(j, f);
  })(window, document, 'script', 'dataLayer', 'GTM-P8H6KQG');
</script>
<!-- End Google Tag Manager -->
<!-- Data to be shared with JS on every page -->
<script>
  window.data = {
      copyIconSrc: "../../../_static/copy-button.svg"
  };
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  

<a class="navbar-brand logo" href="../../../index.html">
  <svg width="400" height="201" viewBox="0 0 400 201" xmlns="http://www.w3.org/2000/svg">
<path id="ray-text" d="M325.949 134.356V109.785L302.442 66.6406H314.244L330.495 97.3062H331.946L348.198 66.6406H360L336.493 109.785V134.356H325.949ZM253.043 134.364L272.391 66.648H290.771L310.021 134.364H299.283L294.834 118.402H268.328L263.878 134.364H253.043ZM270.94 108.728H292.222L282.354 73.1294H280.807L270.94 108.728ZM198.887 134.364V66.648H227.327C231.519 66.648 235.195 67.3896 238.355 68.8729C241.58 70.2918 244.063 72.3555 245.804 75.0641C247.61 77.7727 248.513 80.9973 248.513 84.7378V85.8019C248.513 90.0583 247.481 93.4763 245.417 96.0559C243.418 98.5711 240.967 100.345 238.065 101.376V102.924C240.516 103.053 242.483 103.892 243.966 105.439C245.449 106.923 246.191 109.083 246.191 111.921V134.364H235.647V113.372C235.647 111.631 235.195 110.244 234.292 109.212C233.39 108.18 231.938 107.664 229.939 107.664H209.334V134.364H198.887ZM209.334 98.1842H226.166C229.907 98.1842 232.809 97.249 234.873 95.3788C236.937 93.4441 237.968 90.8322 237.968 87.5431V86.7692C237.968 83.4802 236.937 80.9005 234.873 79.0303C232.874 77.0956 229.971 76.1282 226.166 76.1282H209.334V98.1842Z" fill="black"/>
<path fill-rule="evenodd" clip-rule="evenodd" d="M143.63 101.311L98.3087 146.632L94.9903 143.313L140.311 97.9925L143.63 101.311ZM141.953 102.334L51.4454 102.334V97.6409L141.953 97.6409V102.334ZM94.992 55.9863L140.313 101.307L143.631 97.9886L98.3105 52.6679L94.992 55.9863Z" fill="#02A0CF"/>
<path d="M40 88.3163H62.6604V110.977H40V88.3163ZM85.3207 88.3163H107.981V110.977H85.3207V88.3163ZM85.3207 43H107.981V65.6604H85.3207V43ZM85.3207 133.645H107.981V156.306H85.3207V133.645ZM130.641 88.3163H153.301V110.977H130.641V88.3163Z" fill="#02A0CF"/>
</svg>

</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <div class="navbar-content docutils container">
<ul class="navbar-toplevel">
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../ray-overview/getting-started.html" title="Get Started"><span class="navbar-link-title">Get Started</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../ray-overview/use-cases.html" title="Use Cases"><span class="navbar-link-title">Use Cases</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../ray-overview/examples.html" title="Example Gallery"><span class="navbar-link-title">Example Gallery</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../ray-overview/installation.html" title="Library"><span class="navbar-link-title">Library</span></a></p>
<i class="fa-solid fa-chevron-down"></i></div>
<div class="navbar-dropdown docutils container">
<ul class="navbar-sublevel">
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../ray-core/walkthrough.html" title="Ray Core"><span class="navbar-link-title">Ray Core</span>Scale general Python applications</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../data/data.html" title="Ray Data"><span class="navbar-link-title">Ray Data</span>Scale data ingest and preprocessing</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../train.html" title="Ray Train"><span class="navbar-link-title">Ray Train</span>Scale machine learning training</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../tune/index.html" title="Ray Tune"><span class="navbar-link-title">Ray Tune</span>Scale hyperparameter tuning</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../serve/index.html" title="Ray Serve"><span class="navbar-link-title">Ray Serve</span>Scale model serving</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../rllib/index.html" title="Ray RLlib"><span class="navbar-link-title">Ray RLlib</span>Scale reinforcement learning</a></p>
</div>
</li>
</ul>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../index.html" title="Docs"><span class="navbar-link-title">Docs</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://discuss.ray.io" title="Resources"><span class="navbar-link-title">Resources</span></a></p>
<i class="fa-solid fa-chevron-down"></i></div>
<div class="navbar-dropdown docutils container">
<ul class="navbar-sublevel">
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://discuss.ray.io" title="Discussion Forum"><span class="navbar-link-title">Discussion Forum</span>Get your Ray questions answered</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://github.com/ray-project/ray-educational-materials" title="Training"><span class="navbar-link-title">Training</span>Hands-on learning</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/blog" title="Blog"><span class="navbar-link-title">Blog</span>Updates, best practices, user-stories</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/events" title="Events"><span class="navbar-link-title">Events</span>Webinars, meetups, office hours</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/blog/how-ray-and-anyscale-make-it-easy-to-do-massive-scale-machine-learning-on" title="Success Stories"><span class="navbar-link-title">Success Stories</span>Real-world workload examples</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../ray-overview/ray-libraries.html" title="Ecosystem"><span class="navbar-link-title">Ecosystem</span>Libraries integrated with Ray</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.ray.io/community" title="Community"><span class="navbar-link-title">Community</span>Connect with us</a></p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>

</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          <div id="docsearch"></div>
        </div>
      
      
        <div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
      
        <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="versionswitcherbutton" type="button" role="button" class="version-switcher__button btn btn-sm navbar-btn dropdown-toggle" data-bs-toggle="dropdown" aria-haspopup="listbox" aria-controls="versionswitcherlist" aria-label="Version switcher list">
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="versionswitcherlist" class="version-switcher__menu dropdown-menu list-group-flush py-0" role="listbox" aria-labelledby="versionswitcherbutton">
    <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
      
        <div class="navbar-item"><a
  id="try-anyscale-href"
  href="https://www.anyscale.com/ray-on-anyscale/?utm_source=ray_docs&utm_medium=docs&utm_campaign=navbar"
>
  <div id="try-anyscale-text">
    <span>Try Ray on Anyscale</span>
  </div>
</a></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile"><div id="docsearch"></div>
    </div>
  

  
    <label class="sidebar-toggle secondary-toggle" for="__secondary" tabindex="0">
      <span class="fa-solid fa-outdent"></span>
    </label>
  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <div class="navbar-content docutils container">
<ul class="navbar-toplevel">
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../ray-overview/getting-started.html" title="Get Started"><span class="navbar-link-title">Get Started</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../ray-overview/use-cases.html" title="Use Cases"><span class="navbar-link-title">Use Cases</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../ray-overview/examples.html" title="Example Gallery"><span class="navbar-link-title">Example Gallery</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../ray-overview/installation.html" title="Library"><span class="navbar-link-title">Library</span></a></p>
<i class="fa-solid fa-chevron-down"></i></div>
<div class="navbar-dropdown docutils container">
<ul class="navbar-sublevel">
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../ray-core/walkthrough.html" title="Ray Core"><span class="navbar-link-title">Ray Core</span>Scale general Python applications</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../data/data.html" title="Ray Data"><span class="navbar-link-title">Ray Data</span>Scale data ingest and preprocessing</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../train.html" title="Ray Train"><span class="navbar-link-title">Ray Train</span>Scale machine learning training</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../tune/index.html" title="Ray Tune"><span class="navbar-link-title">Ray Tune</span>Scale hyperparameter tuning</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../serve/index.html" title="Ray Serve"><span class="navbar-link-title">Ray Serve</span>Scale model serving</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../rllib/index.html" title="Ray RLlib"><span class="navbar-link-title">Ray RLlib</span>Scale reinforcement learning</a></p>
</div>
</li>
</ul>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../index.html" title="Docs"><span class="navbar-link-title">Docs</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://discuss.ray.io" title="Resources"><span class="navbar-link-title">Resources</span></a></p>
<i class="fa-solid fa-chevron-down"></i></div>
<div class="navbar-dropdown docutils container">
<ul class="navbar-sublevel">
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://discuss.ray.io" title="Discussion Forum"><span class="navbar-link-title">Discussion Forum</span>Get your Ray questions answered</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://github.com/ray-project/ray-educational-materials" title="Training"><span class="navbar-link-title">Training</span>Hands-on learning</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/blog" title="Blog"><span class="navbar-link-title">Blog</span>Updates, best practices, user-stories</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/events" title="Events"><span class="navbar-link-title">Events</span>Webinars, meetups, office hours</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/blog/how-ray-and-anyscale-make-it-easy-to-do-massive-scale-machine-learning-on" title="Success Stories"><span class="navbar-link-title">Success Stories</span>Real-world workload examples</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../ray-overview/ray-libraries.html" title="Ecosystem"><span class="navbar-link-title">Ecosystem</span>Libraries integrated with Ray</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.ray.io/community" title="Community"><span class="navbar-link-title">Community</span>Connect with us</a></p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>

</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
        
          <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="versionswitcherbutton" type="button" role="button" class="version-switcher__button btn btn-sm navbar-btn dropdown-toggle" data-bs-toggle="dropdown" aria-haspopup="listbox" aria-controls="versionswitcherlist" aria-label="Version switcher list">
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="versionswitcherlist" class="version-switcher__menu dropdown-menu list-group-flush py-0" role="listbox" aria-labelledby="versionswitcherbutton">
    <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
        
          <div class="navbar-item"><a
  id="try-anyscale-href"
  href="https://www.anyscale.com/ray-on-anyscale/?utm_source=ray_docs&utm_medium=docs&utm_campaign=navbar"
>
  <div id="try-anyscale-text">
    <span>Try Ray on Anyscale</span>
  </div>
</a></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item"><nav id="main-sidebar" class="bd-docs-nav bd-links" aria-label="Section Navigation">
  <div class="bd-toc-item navbar-nav"><ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../ray-overview/index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ray-overview/getting-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ray-overview/installation.html">Installation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../ray-overview/use-cases.html">Use Cases</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../ray-air/getting-started.html">Ray for ML Infrastructure</a></li>

</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../ray-overview/examples.html">Example Gallery</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ray-overview/ray-libraries.html">Ecosystem</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../ray-core/walkthrough.html">Ray Core</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../ray-core/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../ray-core/user-guide.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../ray-core/tasks.html">Tasks</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/tasks/nested-tasks.html">Nested Remote Functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/tasks/generators.html">Dynamic generators</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../ray-core/actors.html">Actors</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/actors/named-actors.html">Named Actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/actors/terminating-actors.html">Terminating Actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/actors/async_api.html">AsyncIO / Concurrency for Actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/actors/concurrency_group_api.html">Limiting Concurrency Per-Method with Concurrency Groups</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/actors/actor-utils.html">Utility Classes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/actors/out-of-band-communication.html">Out-of-band Communication</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/actors/task-orders.html">Actor Task Execution Order</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../ray-core/objects.html">Objects</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/objects/serialization.html">Serialization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/objects/object-spilling.html">Object Spilling</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-core/handling-dependencies.html">Environment Dependencies</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../ray-core/scheduling/index.html">Scheduling</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/scheduling/resources.html">Resources</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/scheduling/accelerators.html">Accelerator Support</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/scheduling/placement-group.html">Placement Groups</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/scheduling/memory-management.html">Memory Management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/scheduling/ray-oom-prevention.html">Out-Of-Memory Prevention</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../ray-core/fault-tolerance.html">Fault Tolerance</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/fault_tolerance/tasks.html">Task Fault Tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/fault_tolerance/actors.html">Actor Fault Tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/fault_tolerance/objects.html">Object Fault Tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/fault_tolerance/nodes.html">Node Fault Tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/fault_tolerance/gcs.html">GCS Fault Tolerance</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../ray-core/patterns/index.html">Design Patterns &amp; Anti-patterns</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/patterns/nested-tasks.html">Pattern: Using nested tasks to achieve nested parallelism</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/patterns/generators.html">Pattern: Using generators to reduce heap memory usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/patterns/limit-pending-tasks.html">Pattern: Using ray.wait to limit the number of pending tasks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/patterns/limit-running-tasks.html">Pattern: Using resources to limit the number of concurrently running tasks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/patterns/concurrent-operations-async-actor.html">Pattern: Using asyncio to run actor methods concurrently</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/patterns/actor-sync.html">Pattern: Using an actor to synchronize other tasks and actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/patterns/tree-of-actors.html">Pattern: Using a supervisor actor to manage a tree of actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/patterns/pipelining.html">Pattern: Using pipelining to increase throughput</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/patterns/return-ray-put.html">Anti-pattern: Returning ray.put() ObjectRefs from a task harms performance and fault tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/patterns/ray-get-loop.html">Anti-pattern: Calling ray.get in a loop harms parallelism</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/patterns/unnecessary-ray-get.html">Anti-pattern: Calling ray.get unnecessarily harms performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/patterns/ray-get-submission-order.html">Anti-pattern: Processing results in submission order using ray.get increases runtime</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/patterns/ray-get-too-many-objects.html">Anti-pattern: Fetching too many objects at once with ray.get causes failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/patterns/too-fine-grained-tasks.html">Anti-pattern: Over-parallelizing with too fine-grained tasks harms speedup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/patterns/redefine-task-actor-loop.html">Anti-pattern: Redefining the same remote function or class harms performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/patterns/pass-large-arg-by-value.html">Anti-pattern: Passing the same large argument by value repeatedly harms performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/patterns/closure-capture-large-objects.html">Anti-pattern: Closure capturing large objects harms performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/patterns/global-variables.html">Anti-pattern: Using global variables to share state between tasks and actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/patterns/out-of-band-object-ref-serialization.html">Anti-pattern: Serialize ray.ObjectRef out of band</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/patterns/fork-new-processes.html">Anti-pattern: Forking new processes in application code</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../ray-core/compiled-graph/ray-compiled-graph.html">Ray Compiled Graph</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/compiled-graph/quickstart.html">Quickstart</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/compiled-graph/profiling.html">Profiling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/compiled-graph/overlap.html">Experimental: Overlapping communication and computation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/compiled-graph/compiled-graph-api.html">Compiled Graph API</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../ray-core/advanced-topics.html">Advanced Topics</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/tips-for-first-time.html">Tips for first-time users</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/starting-ray.html">Starting Ray</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/ray-generator.html">Ray Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/namespaces.html">Using Namespaces</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/cross-language.html">Cross-Language Programming</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/using-ray-with-jupyter.html">Working with Jupyter Notebooks &amp; JupyterLab</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/ray-dag.html">Lazy Computation Graphs with the Ray DAG API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/miscellaneous.html">Miscellaneous Topics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/runtime_env_auth.html">Authenticating Remote URIs in runtime_env</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-core/user-spawn-processes.html">Lifetimes of a User-Spawn Process</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../ray-core/examples/overview.html">Examples</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-core/examples/automl_for_time_series.html">Simple AutoML for time series with Ray Core</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../../ray-core/examples/batch_prediction.html">Batch Prediction with Ray Core</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-core/examples/gentle_walkthrough.html">A Gentle Introduction to Ray Core by Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-core/examples/highly_parallel.html">Using Ray for Highly Parallelizable Tasks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-core/examples/map_reduce.html">A Simple MapReduce Example with Ray Core</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-core/examples/monte_carlo_pi.html">Monte Carlo Estimation of Ï€</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-core/examples/plot_hyperparameter.html">Simple Parallel Model Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-core/examples/plot_parameter_server.html">Parameter Server</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-core/examples/plot_pong_example.html">Learning to Play Pong</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-core/examples/web-crawler.html">Speed up your web crawler by parallelizing it with Ray</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../ray-core/api/index.html">Ray Core API</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-core/api/core.html">Core API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-core/api/scheduling.html">Scheduling API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-core/api/runtime-env.html">Runtime Env API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-core/api/utility.html">Utility</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-core/api/exceptions.html">Exceptions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-core/api/cli.html">Ray Core CLI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-observability/reference/cli.html">State CLI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-observability/reference/api.html">State API</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../data/data.html">Ray Data</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../data/quickstart.html">Ray Data Quickstart</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../data/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../data/user-guide.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../data/loading-data.html">Loading Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../data/inspecting-data.html">Inspecting Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../data/transforming-data.html">Transforming Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../data/iterating-over-data.html">Iterating over Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../data/shuffling-data.html">Shuffling Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../data/saving-data.html">Saving Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../data/working-with-images.html">Working with Images</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../data/working-with-text.html">Working with Text</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../data/working-with-tensors.html">Working with Tensors / NumPy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../data/working-with-pytorch.html">Working with PyTorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../data/working-with-llms.html">Working with LLMs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../data/monitoring-your-workload.html">Monitoring Your Workload</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../data/execution-configurations.html">Execution Configurations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../data/batch_inference.html">End-to-end: Offline Batch Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../data/performance-tips.html">Advanced: Performance Tips and Tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../data/custom-datasource-example.html">Advanced: Read and Write Custom File Types</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../data/examples.html">Examples</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../data/api/api.html">Ray Data API</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../data/api/input_output.html">Input/Output</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../data/api/dataset.html">Dataset API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../data/api/data_iterator.html">DataIterator API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../data/api/execution_options.html">ExecutionOptions API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../data/api/aggregate.html">Aggregation API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../data/api/grouped_data.html">GroupedData API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../data/api/data_context.html">Global configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../data/api/preprocessor.html">Preprocessor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../data/api/llm.html">Large Language Model (LLM) API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../data/api/from_other_data_libs.html">API Guide for Users from Other Data Libraries</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../data/comparisons.html">Comparing Ray Data to other systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../data/data-internals.html">Ray Data Internals</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../train.html">Ray Train</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../overview.html">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../getting-started-pytorch.html">PyTorch Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../getting-started-pytorch-lightning.html">PyTorch Lightning Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../getting-started-transformers.html">Hugging Face Transformers Guide</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../more-frameworks.html">More Frameworks</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../huggingface-accelerate.html">Hugging Face Accelerate Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../deepspeed.html">DeepSpeed Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../distributed-tensorflow-keras.html">TensorFlow and Keras Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../xgboost/distributed-xgboost-lightgbm.html">XGBoost and LightGBM Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../horovod.html">Horovod Guide</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../user-guides.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user-guides/data-loading-preprocessing.html">Data Loading and Preprocessing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user-guides/using-gpus.html">Configuring Scale and GPUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user-guides/persistent-storage.html">Configuring Persistent Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user-guides/monitoring-logging.html">Monitoring and Logging Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user-guides/checkpoints.html">Saving and Loading Checkpoints</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user-guides/experiment-tracking.html">Experiment Tracking</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user-guides/results.html">Inspecting Training Results</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user-guides/fault-tolerance.html">Handling Failures and Node Preemption</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user-guides/reproducibility.html">Reproducibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user-guides/hyperparameter-optimization.html">Hyperparameter Optimization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../examples.html">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../benchmarks.html">Benchmarks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../api/api.html">Ray Train API</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../tune/index.html">Ray Tune</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-20"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../tune/getting-started.html">Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../tune/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../tune/tutorials/overview.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-21"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/tutorials/tune-run.html">Running Basic Experiments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/tutorials/tune-output.html">Logging and Outputs in Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/tutorials/tune-resources.html">Setting Trial Resources</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/tutorials/tune-search-spaces.html">Using Search Spaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/tutorials/tune-stopping.html">How to Define Stopping Criteria for a Ray Tune Experiment</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/tutorials/tune-trial-checkpoints.html">How to Save and Load Trial Checkpoints</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../../tune/tutorials/tune-storage.html">How to Configure Persistent Storage in Ray Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/tutorials/tune-fault-tolerance.html">How to Enable Fault Tolerance in Ray Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/tutorials/tune-metrics.html">Using Callbacks and Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/tutorials/tune_get_data_in_and_out.html">Getting Data in and out of Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/examples/tune_analyze_results.html">Analyzing Tune Experiment Results</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../tune/examples/pbt_guide.html">A Guide to Population Based Training with Tune</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-22"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tune/examples/pbt_visualization/pbt_visualization.html">Visualizing and Understanding PBT</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/tutorials/tune-distributed.html">Deploying Tune in the Cloud</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/tutorials/tune-lifecycle.html">Tune Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/tutorials/tune-scalability.html">Scalability Benchmarks</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../tune/examples/index.html">Ray Tune Examples</a><input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-23"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../tune/examples/ml-frameworks.html">Examples using Ray Tune with ML Frameworks</a><input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-24"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tune/examples/tune_mnist_keras.html">Keras Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tune/examples/tune-pytorch-cifar.html">PyTorch Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tune/examples/tune-pytorch-lightning.html">PyTorch Lightning Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tune/examples/pbt_ppo_example.html">Ray RLlib Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tune/examples/tune-xgboost.html">XGBoost Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tune/examples/lightgbm_example.html">LightGBM Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tune/examples/horovod_simple.html">Horovod Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tune/examples/pbt_transformers.html">Hugging Face Transformers Example</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../tune/examples/experiment-tracking.html">Tune Experiment Tracking Examples</a><input class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-25"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tune/examples/tune-wandb.html">Weights &amp; Biases Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tune/examples/tune-mlflow.html">MLflow Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tune/examples/tune-aim.html">Aim Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tune/examples/tune-comet.html">Comet Example</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../tune/examples/hpo-frameworks.html">Tune Hyperparameter Optimization Framework Examples</a><input class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-26"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tune/examples/ax_example.html">Ax Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tune/examples/hyperopt_example.html">HyperOpt Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tune/examples/bayesopt_example.html">Bayesopt Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tune/examples/bohb_example.html">BOHB Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tune/examples/nevergrad_example.html">Nevergrad Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tune/examples/optuna_example.html">Optuna Example</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/examples/other-examples.html">Other Examples</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/examples/exercises.html">Exercises</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../tune/faq.html">Ray Tune FAQ</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../tune/api/api.html">Ray Tune API</a><input class="toctree-checkbox" id="toctree-checkbox-27" name="toctree-checkbox-27" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-27"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/api/execution.html">Tune Execution (tune.Tuner)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/api/result_grid.html">Tune Experiment Results (tune.ResultGrid)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/api/trainable.html">Training in Tune (tune.Trainable, tune.report)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/api/search_space.html">Tune Search Space API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/api/suggestion.html">Tune Search Algorithms (tune.search)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/api/schedulers.html">Tune Trial Schedulers (tune.schedulers)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/api/stoppers.html">Tune Stopping Mechanisms (tune.stopper)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/api/reporters.html">Tune Console Output (Reporters)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/api/syncing.html">Syncing in Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/api/logging.html">Tune Loggers (tune.logger)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/api/callbacks.html">Tune Callbacks (tune.Callback)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/api/env.html">Environment variables used by Ray Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/api/integration.html">External library integrations for Ray Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/api/internals.html">Tune Internals</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tune/api/cli.html">Tune CLI (Experimental)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../serve/index.html">Ray Serve</a><input class="toctree-checkbox" id="toctree-checkbox-28" name="toctree-checkbox-28" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-28"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../serve/getting_started.html">Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../serve/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../serve/develop-and-deploy.html">Develop and Deploy an ML Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../serve/model_composition.html">Deploy Compositions of Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../serve/multi-app.html">Deploy Multiple Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../serve/model-multiplexing.html">Model Multiplexing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../serve/configure-serve-deployment.html">Configure Ray Serve deployments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../serve/http-guide.html">Set Up FastAPI and HTTP</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../serve/llm/index.html">Serving LLMs</a><input class="toctree-checkbox" id="toctree-checkbox-29" name="toctree-checkbox-29" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-29"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../serve/llm/overview.html">Overview</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../serve/llm/api.html">Ray Serve LLM API</a><input class="toctree-checkbox" id="toctree-checkbox-30" name="toctree-checkbox-30" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-30"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../serve/llm/doc/ray.serve.llm.builders.build_vllm_deployment.html">ray.serve.llm.builders.build_vllm_deployment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../serve/llm/doc/ray.serve.llm.builders.build_openai_app.html">ray.serve.llm.builders.build_openai_app</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../serve/llm/doc/ray.serve.llm.configs.LLMConfig.html">ray.serve.llm.configs.LLMConfig</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../serve/llm/doc/ray.serve.llm.configs.LLMServingArgs.html">ray.serve.llm.configs.LLMServingArgs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../serve/llm/doc/ray.serve.llm.configs.ModelLoadingConfig.html">ray.serve.llm.configs.ModelLoadingConfig</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../serve/llm/doc/ray.serve.llm.configs.CloudMirrorConfig.html">ray.serve.llm.configs.CloudMirrorConfig</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../serve/llm/doc/ray.serve.llm.configs.LoraConfig.html">ray.serve.llm.configs.LoraConfig</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../serve/llm/doc/ray.serve.llm.deployments.VLLMService.html">ray.serve.llm.deployments.VLLMService</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../serve/llm/doc/ray.serve.llm.deployments.LLMRouter.html">ray.serve.llm.deployments.LLMRouter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../serve/llm/doc/ray.serve.llm.openai_api_models.ChatCompletionRequest.html">ray.serve.llm.openai_api_models.ChatCompletionRequest</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../serve/llm/doc/ray.serve.llm.openai_api_models.CompletionRequest.html">ray.serve.llm.openai_api_models.CompletionRequest</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../serve/llm/doc/ray.serve.llm.openai_api_models.ChatCompletionStreamResponse.html">ray.serve.llm.openai_api_models.ChatCompletionStreamResponse</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../serve/llm/doc/ray.serve.llm.openai_api_models.ChatCompletionResponse.html">ray.serve.llm.openai_api_models.ChatCompletionResponse</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../serve/llm/doc/ray.serve.llm.openai_api_models.CompletionStreamResponse.html">ray.serve.llm.openai_api_models.CompletionStreamResponse</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../serve/llm/doc/ray.serve.llm.openai_api_models.CompletionResponse.html">ray.serve.llm.openai_api_models.CompletionResponse</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../serve/llm/doc/ray.serve.llm.openai_api_models.ErrorResponse.html">ray.serve.llm.openai_api_models.ErrorResponse</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../serve/production-guide/index.html">Production Guide</a><input class="toctree-checkbox" id="toctree-checkbox-31" name="toctree-checkbox-31" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-31"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../serve/production-guide/config.html">Serve Config Files</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../serve/production-guide/kubernetes.html">Deploy on Kubernetes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../serve/production-guide/docker.html">Custom Docker Images</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../serve/production-guide/fault-tolerance.html">Add End-to-End Fault Tolerance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../serve/production-guide/handling-dependencies.html">Handle Dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../serve/production-guide/best-practices.html">Best practices in production</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../serve/monitoring.html">Monitor Your Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../serve/resource-allocation.html">Resource Allocation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../serve/autoscaling-guide.html">Ray Serve Autoscaling</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../serve/advanced-guides/index.html">Advanced Guides</a><input class="toctree-checkbox" id="toctree-checkbox-32" name="toctree-checkbox-32" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-32"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../serve/advanced-guides/app-builder-guide.html">Pass Arguments to Applications</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../serve/advanced-guides/advanced-autoscaling.html">Advanced Ray Serve Autoscaling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../serve/advanced-guides/performance.html">Performance Tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../serve/advanced-guides/dyn-req-batch.html">Dynamic Request Batching</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../serve/advanced-guides/inplace-updates.html">Updating Applications In-Place</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../serve/advanced-guides/dev-workflow.html">Development Workflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../serve/advanced-guides/grpc-guide.html">Set Up a gRPC Service</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../serve/advanced-guides/managing-java-deployments.html">Experimental Java API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../serve/advanced-guides/deploy-vm.html">Deploy on VM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../serve/advanced-guides/multi-app-container.html">Run Multiple Applications in Different Containers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../serve/architecture.html">Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../serve/examples.html">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../serve/api/index.html">Ray Serve API</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../rllib/index.html">Ray RLlib</a><input class="toctree-checkbox" id="toctree-checkbox-33" name="toctree-checkbox-33" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-33"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../rllib/getting-started.html">Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rllib/key-concepts.html">Key concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../rllib/rllib-env.html">Environments</a><input class="toctree-checkbox" id="toctree-checkbox-34" name="toctree-checkbox-34" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-34"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rllib/multi-agent-envs.html">Multi-Agent Environments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rllib/hierarchical-envs.html">Hierarchical Environments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rllib/external-envs.html">External Environments and Applications</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rllib/algorithm-config.html">AlgorithmConfig API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rllib/rllib-algorithms.html">Algorithms</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../rllib/user-guides.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-35" name="toctree-checkbox-35" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-35"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rllib/rllib-advanced-api.html">Advanced Python APIs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rllib/rllib-callback.html">Callbacks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rllib/checkpoints.html">Checkpointing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rllib/metrics-logger.html">MetricsLogger API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rllib/single-agent-episode.html">Episodes</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../../rllib/rllib-replay-buffers.html">Replay Buffers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rllib/rllib-offline.html">Working with offline data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rllib/rl-modules.html">RL Modules</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rllib/rllib-learner.html">Learner (Alpha)</a></li>



<li class="toctree-l3"><a class="reference internal" href="../../../rllib/rllib-torch2x.html">Using RLlib with torch 2.x compile</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../../rllib/rllib-fault-tolerance.html">Fault Tolerance And Elastic Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rllib/rllib-dev.html">Install RLlib for Development</a></li>




<li class="toctree-l3"><a class="reference internal" href="../../../rllib/scaling-guide.html">RLlib scaling guide</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rllib/rllib-examples.html">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rllib/new-api-stack-migration-guide.html">New API stack migration guide</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../rllib/package_ref/index.html">Ray RLlib API</a><input class="toctree-checkbox" id="toctree-checkbox-36" name="toctree-checkbox-36" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-36"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../rllib/package_ref/algorithm-config.html">Algorithm Configuration API</a><input class="toctree-checkbox" id="toctree-checkbox-37" name="toctree-checkbox-37" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-37"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_algo.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_algo</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner_group.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner_group</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.is_multi_agent.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.is_multi_agent</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.is_offline.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.is_offline</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learner_class.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learner_class</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.model_config.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.model_config</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.rl_module_spec.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.rl_module_spec</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.total_train_batch_size.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.total_train_batch_size</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_learner_class.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_learner_class</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_rl_module_spec.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_rl_module_spec</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_evaluation_config_object.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_evaluation_config_object</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_multi_rl_module_spec.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_multi_rl_module_spec</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_multi_agent_setup.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_multi_agent_setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_rollout_fragment_length.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_rollout_fragment_length</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.copy.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.copy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.validate.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.validate</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.freeze.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.freeze</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../rllib/package_ref/algorithm.html">Algorithms</a><input class="toctree-checkbox" id="toctree-checkbox-38" name="toctree-checkbox-38" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-38"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.html">ray.rllib.algorithms.algorithm.Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.setup.html">ray.rllib.algorithms.algorithm.Algorithm.setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_default_config.html">ray.rllib.algorithms.algorithm.Algorithm.get_default_config</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.env_runner.html">ray.rllib.algorithms.algorithm.Algorithm.env_runner</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.eval_env_runner.html">ray.rllib.algorithms.algorithm.Algorithm.eval_env_runner</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.train.html">ray.rllib.algorithms.algorithm.Algorithm.train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.training_step.html">ray.rllib.algorithms.algorithm.Algorithm.training_step</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.save_to_path.html">ray.rllib.algorithms.algorithm.Algorithm.save_to_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.restore_from_path.html">ray.rllib.algorithms.algorithm.Algorithm.restore_from_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.from_checkpoint.html">ray.rllib.algorithms.algorithm.Algorithm.from_checkpoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_state.html">ray.rllib.algorithms.algorithm.Algorithm.get_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.set_state.html">ray.rllib.algorithms.algorithm.Algorithm.set_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.evaluate.html">ray.rllib.algorithms.algorithm.Algorithm.evaluate</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_module.html">ray.rllib.algorithms.algorithm.Algorithm.get_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.add_policy.html">ray.rllib.algorithms.algorithm.Algorithm.add_policy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.remove_policy.html">ray.rllib.algorithms.algorithm.Algorithm.remove_policy</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../rllib/package_ref/callback.html">Callback APIs</a><input class="toctree-checkbox" id="toctree-checkbox-39" name="toctree-checkbox-39" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-39"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.html">ray.rllib.callbacks.callbacks.RLlibCallback</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_algorithm_init.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_algorithm_init</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_sample_end.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_sample_end</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_train_result.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_train_result</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_evaluate_start.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_evaluate_start</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_evaluate_end.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_evaluate_end</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_env_runners_recreated.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_env_runners_recreated</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_checkpoint_loaded.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_checkpoint_loaded</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_environment_created.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_environment_created</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_created.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_created</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_start.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_start</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_step.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_step</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_end.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_end</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../rllib/package_ref/env.html">Environments</a><input class="toctree-checkbox" id="toctree-checkbox-40" name="toctree-checkbox-40" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-40"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/env/env_runner.html">EnvRunner API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/env/single_agent_env_runner.html">SingleAgentEnvRunner API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/env/single_agent_episode.html">SingleAgentEpisode API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/env/multi_agent_env.html">MultiAgentEnv API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/env/multi_agent_env_runner.html">MultiAgentEnvRunner API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/env/multi_agent_episode.html">MultiAgentEpisode API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/env/utils.html">Env Utils</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../rllib/package_ref/rl_modules.html">RLModule APIs</a><input class="toctree-checkbox" id="toctree-checkbox-41" name="toctree-checkbox-41" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-41"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.build.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.build</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.module_class.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.module_class</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.observation_space.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.observation_space</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.action_space.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.action_space</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.inference_only.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.inference_only</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.learner_only.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.learner_only</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.model_config.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.model_config</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec.build.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec.build</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.default_model_config.DefaultModelConfig.html">ray.rllib.core.rl_module.default_model_config.DefaultModelConfig</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.html">ray.rllib.core.rl_module.rl_module.RLModule</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.observation_space.html">ray.rllib.core.rl_module.rl_module.RLModule.observation_space</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.action_space.html">ray.rllib.core.rl_module.rl_module.RLModule.action_space</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.inference_only.html">ray.rllib.core.rl_module.rl_module.RLModule.inference_only</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.model_config.html">ray.rllib.core.rl_module.rl_module.RLModule.model_config</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.setup.html">ray.rllib.core.rl_module.rl_module.RLModule.setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.as_multi_rl_module.html">ray.rllib.core.rl_module.rl_module.RLModule.as_multi_rl_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.forward_exploration.html">ray.rllib.core.rl_module.rl_module.RLModule.forward_exploration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.forward_inference.html">ray.rllib.core.rl_module.rl_module.RLModule.forward_inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.forward_train.html">ray.rllib.core.rl_module.rl_module.RLModule.forward_train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule._forward.html">ray.rllib.core.rl_module.rl_module.RLModule._forward</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule._forward_exploration.html">ray.rllib.core.rl_module.rl_module.RLModule._forward_exploration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule._forward_inference.html">ray.rllib.core.rl_module.rl_module.RLModule._forward_inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule._forward_train.html">ray.rllib.core.rl_module.rl_module.RLModule._forward_train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.save_to_path.html">ray.rllib.core.rl_module.rl_module.RLModule.save_to_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.restore_from_path.html">ray.rllib.core.rl_module.rl_module.RLModule.restore_from_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.from_checkpoint.html">ray.rllib.core.rl_module.rl_module.RLModule.from_checkpoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.get_state.html">ray.rllib.core.rl_module.rl_module.RLModule.get_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.set_state.html">ray.rllib.core.rl_module.rl_module.RLModule.set_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.setup.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.as_multi_rl_module.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.as_multi_rl_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.add_module.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.add_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.remove_module.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.remove_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.save_to_path.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.save_to_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.restore_from_path.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.restore_from_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.from_checkpoint.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.from_checkpoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.get_state.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.get_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.set_state.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.set_state</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../rllib/package_ref/distributions.html">Distribution API</a><input class="toctree-checkbox" id="toctree-checkbox-42" name="toctree-checkbox-42" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-42"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.models.distributions.Distribution.html">ray.rllib.models.distributions.Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.models.distributions.Distribution.from_logits.html">ray.rllib.models.distributions.Distribution.from_logits</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.models.distributions.Distribution.sample.html">ray.rllib.models.distributions.Distribution.sample</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.models.distributions.Distribution.rsample.html">ray.rllib.models.distributions.Distribution.rsample</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.models.distributions.Distribution.logp.html">ray.rllib.models.distributions.Distribution.logp</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.models.distributions.Distribution.kl.html">ray.rllib.models.distributions.Distribution.kl</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../rllib/package_ref/learner.html">LearnerGroup API</a><input class="toctree-checkbox" id="toctree-checkbox-43" name="toctree-checkbox-43" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-43"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learners.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learners</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner_group.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner_group</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.core.learner.learner_group.LearnerGroup.html">ray.rllib.core.learner.learner_group.LearnerGroup</a></li>
</ul>
</li>

<li class="toctree-l3 has-children"><a class="reference internal" href="../../../rllib/package_ref/offline.html">Offline RL API</a><input class="toctree-checkbox" id="toctree-checkbox-44" name="toctree-checkbox-44" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-44"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.offline_data.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.offline_data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learners.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learners</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.env_runners.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.env_runners</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner.html">ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.offline.offline_data.OfflineData.html">ray.rllib.offline.offline_data.OfflineData</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.offline.offline_data.OfflineData.__init__.html">ray.rllib.offline.offline_data.OfflineData.__init__</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.offline.offline_data.OfflineData.sample.html">ray.rllib.offline.offline_data.OfflineData.sample</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.offline.offline_data.OfflineData.default_map_batches_kwargs.html">ray.rllib.offline.offline_data.OfflineData.default_map_batches_kwargs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.offline.offline_data.OfflineData.default_iter_batches_kwargs.html">ray.rllib.offline.offline_data.OfflineData.default_iter_batches_kwargs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.__init__.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner.__init__</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.SCHEMA.html">ray.rllib.offline.offline_prelearner.SCHEMA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.__call__.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner.__call__</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner._map_to_episodes.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner._map_to_episodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner._map_sample_batch_to_episode.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner._map_sample_batch_to_episode</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner._should_module_be_updated.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner._should_module_be_updated</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.default_prelearner_buffer_class.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner.default_prelearner_buffer_class</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.default_prelearner_buffer_kwargs.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner.default_prelearner_buffer_kwargs</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../rllib/package_ref/replay-buffers.html">Replay Buffer API</a><input class="toctree-checkbox" id="toctree-checkbox-45" name="toctree-checkbox-45" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-45"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.StorageUnit.html">ray.rllib.utils.replay_buffers.replay_buffer.StorageUnit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.prioritized_replay_buffer.PrioritizedReplayBuffer.html">ray.rllib.utils.replay_buffers.prioritized_replay_buffer.PrioritizedReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.reservoir_replay_buffer.ReservoirReplayBuffer.html">ray.rllib.utils.replay_buffers.reservoir_replay_buffer.ReservoirReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.sample.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.sample</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.add.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.add</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.get_state.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.get_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.set_state.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.set_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.multi_agent_replay_buffer.MultiAgentReplayBuffer.html">ray.rllib.utils.replay_buffers.multi_agent_replay_buffer.MultiAgentReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.multi_agent_prioritized_replay_buffer.MultiAgentPrioritizedReplayBuffer.html">ray.rllib.utils.replay_buffers.multi_agent_prioritized_replay_buffer.MultiAgentPrioritizedReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.utils.update_priorities_in_replay_buffer.html">ray.rllib.utils.replay_buffers.utils.update_priorities_in_replay_buffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.utils.sample_min_n_steps_from_buffer.html">ray.rllib.utils.replay_buffers.utils.sample_min_n_steps_from_buffer</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../rllib/package_ref/utils.html">RLlib Utilities</a><input class="toctree-checkbox" id="toctree-checkbox-46" name="toctree-checkbox-46" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-46"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.peek.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger.peek</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_value.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_value</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_dict.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_dict</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.merge_and_log_n_dicts.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger.merge_and_log_n_dicts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_time.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_time</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.schedules.scheduler.Scheduler.html">ray.rllib.utils.schedules.scheduler.Scheduler</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.schedules.scheduler.Scheduler.validate.html">ray.rllib.utils.schedules.scheduler.Scheduler.validate</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.schedules.scheduler.Scheduler.get_current_value.html">ray.rllib.utils.schedules.scheduler.Scheduler.get_current_value</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.schedules.scheduler.Scheduler.update.html">ray.rllib.utils.schedules.scheduler.Scheduler.update</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.schedules.scheduler.Scheduler._create_tensor_variable.html">ray.rllib.utils.schedules.scheduler.Scheduler._create_tensor_variable</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.framework.try_import_torch.html">ray.rllib.utils.framework.try_import_torch</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.clip_gradients.html">ray.rllib.utils.torch_utils.clip_gradients</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.compute_global_norm.html">ray.rllib.utils.torch_utils.compute_global_norm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.convert_to_torch_tensor.html">ray.rllib.utils.torch_utils.convert_to_torch_tensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.explained_variance.html">ray.rllib.utils.torch_utils.explained_variance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.flatten_inputs_to_1d_tensor.html">ray.rllib.utils.torch_utils.flatten_inputs_to_1d_tensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.global_norm.html">ray.rllib.utils.torch_utils.global_norm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.one_hot.html">ray.rllib.utils.torch_utils.one_hot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.reduce_mean_ignore_inf.html">ray.rllib.utils.torch_utils.reduce_mean_ignore_inf</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.sequence_mask.html">ray.rllib.utils.torch_utils.sequence_mask</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.set_torch_seed.html">ray.rllib.utils.torch_utils.set_torch_seed</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.softmax_cross_entropy_with_logits.html">ray.rllib.utils.torch_utils.softmax_cross_entropy_with_logits</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.update_target_network.html">ray.rllib.utils.torch_utils.update_target_network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.numpy.aligned_array.html">ray.rllib.utils.numpy.aligned_array</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.numpy.concat_aligned.html">ray.rllib.utils.numpy.concat_aligned</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.numpy.convert_to_numpy.html">ray.rllib.utils.numpy.convert_to_numpy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.numpy.fc.html">ray.rllib.utils.numpy.fc</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.numpy.flatten_inputs_to_1d_tensor.html">ray.rllib.utils.numpy.flatten_inputs_to_1d_tensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.numpy.make_action_immutable.html">ray.rllib.utils.numpy.make_action_immutable</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.numpy.huber_loss.html">ray.rllib.utils.numpy.huber_loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.numpy.l2_loss.html">ray.rllib.utils.numpy.l2_loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.numpy.lstm.html">ray.rllib.utils.numpy.lstm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.numpy.one_hot.html">ray.rllib.utils.numpy.one_hot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.numpy.relu.html">ray.rllib.utils.numpy.relu</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.numpy.sigmoid.html">ray.rllib.utils.numpy.sigmoid</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.numpy.softmax.html">ray.rllib.utils.numpy.softmax</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.checkpoints.try_import_msgpack.html">ray.rllib.utils.checkpoints.try_import_msgpack</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rllib/package_ref/doc/ray.rllib.utils.checkpoints.Checkpointable.html">ray.rllib.utils.checkpoints.Checkpointable</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../ray-more-libs/index.html">More Libraries</a><input class="toctree-checkbox" id="toctree-checkbox-47" name="toctree-checkbox-47" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-47"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../ray-more-libs/joblib.html">Distributed Scikit-learn / Joblib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ray-more-libs/multiprocessing.html">Distributed multiprocessing.Pool</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ray-more-libs/ray-collective.html">Ray Collective Communication Lib</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../ray-more-libs/dask-on-ray.html">Using Dask on Ray</a><input class="toctree-checkbox" id="toctree-checkbox-48" name="toctree-checkbox-48" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-48"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../ray-more-libs/doc/ray.util.dask.RayDaskCallback.html">ray.util.dask.RayDaskCallback</a><input class="toctree-checkbox" id="toctree-checkbox-49" name="toctree-checkbox-49" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-49"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-more-libs/doc/ray.util.dask.RayDaskCallback.ray_active.html">ray.util.dask.RayDaskCallback.ray_active</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_presubmit.html">ray.util.dask.callbacks.RayDaskCallback._ray_presubmit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_postsubmit.html">ray.util.dask.callbacks.RayDaskCallback._ray_postsubmit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_pretask.html">ray.util.dask.callbacks.RayDaskCallback._ray_pretask</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_posttask.html">ray.util.dask.callbacks.RayDaskCallback._ray_posttask</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_postsubmit_all.html">ray.util.dask.callbacks.RayDaskCallback._ray_postsubmit_all</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_finish.html">ray.util.dask.callbacks.RayDaskCallback._ray_finish</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../ray-more-libs/raydp.html">Using Spark on Ray (RayDP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ray-more-libs/mars-on-ray.html">Using Mars on Ray</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ray-more-libs/modin/index.html">Using Pandas on Ray (Modin)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../workflows/index.html">Ray Workflows (Alpha)</a><input class="toctree-checkbox" id="toctree-checkbox-50" name="toctree-checkbox-50" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-50"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../workflows/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../workflows/basics.html">Getting Started</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../workflows/management.html">Workflow Management</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../workflows/metadata.html">Workflow Metadata</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../workflows/events.html">Events</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../workflows/comparison.html">API Comparisons</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../workflows/advanced.html">Advanced Topics</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../workflows/api/api.html">Ray Workflows API</a><input class="toctree-checkbox" id="toctree-checkbox-51" name="toctree-checkbox-51" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-51"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../workflows/api/execution.html">Workflow Execution API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../workflows/api/management.html">Workflow Management API</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../cluster/getting-started.html">Ray Clusters</a><input class="toctree-checkbox" id="toctree-checkbox-52" name="toctree-checkbox-52" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-52"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../cluster/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../cluster/kubernetes/index.html">Deploying on Kubernetes</a><input class="toctree-checkbox" id="toctree-checkbox-53" name="toctree-checkbox-53" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-53"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../cluster/kubernetes/getting-started.html">Getting Started with KubeRay</a><input class="toctree-checkbox" id="toctree-checkbox-54" name="toctree-checkbox-54" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-54"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/getting-started/raycluster-quick-start.html">RayCluster Quickstart</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/getting-started/rayjob-quick-start.html">RayJob Quickstart</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/getting-started/rayservice-quick-start.html">RayService Quickstart</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../cluster/kubernetes/user-guides.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-55" name="toctree-checkbox-55" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-55"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/user-guides/rayservice.html">Deploy Ray Serve Apps</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/user-guides/rayservice-high-availability.html">RayService high availability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/user-guides/observability.html">KubeRay Observability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/user-guides/upgrade-guide.html">KubeRay upgrade guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/user-guides/k8s-cluster-setup.html">Managed Kubernetes services</a></li>



<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/user-guides/storage.html">Best Practices for Storage and Dependencies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/user-guides/config.html">RayCluster Configuration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/user-guides/configuring-autoscaling.html">KubeRay Autoscaling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/user-guides/kuberay-gcs-ft.html">GCS fault tolerance in KubeRay</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/user-guides/kuberay-gcs-persistent-ft.html">Tuning Redis for a Persistent Fault Tolerant GCS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/user-guides/gke-gcs-bucket.html">Configuring KubeRay to use Google Cloud Storage Buckets in GKE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/user-guides/persist-kuberay-custom-resource-logs.html">Persist KubeRay custom resource logs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/user-guides/persist-kuberay-operator-logs.html">Persist KubeRay Operator Logs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/user-guides/gpu.html">Using GPUs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/user-guides/tpu.html">Use TPUs with KubeRay</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/user-guides/rayserve-dev-doc.html">Developing Ray Serve Python scripts on a RayCluster</a></li>









<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/user-guides/pod-command.html">Specify container commands for Ray head/worker Pods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/user-guides/helm-chart-rbac.html">Helm Chart RBAC</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/user-guides/tls.html">TLS Authentication</a></li>






<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/user-guides/k8s-autoscaler.html">(Advanced) Understanding the Ray Autoscaler in the Context of Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.html">(Advanced) Deploying a static Ray cluster without KubeRay</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/user-guides/kubectl-plugin.html">Use kubectl plugin (beta)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/user-guides/kuberay-auth.html">Configure Ray clusters with authentication and access control using KubeRay</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/user-guides/reduce-image-pull-latency.html">Reducing image pull latency on Kubernetes</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../cluster/kubernetes/examples.html">Examples</a><input class="toctree-checkbox" id="toctree-checkbox-56" name="toctree-checkbox-56" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-56"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/examples/ml-example.html">Ray Train XGBoostTrainer on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/examples/gpu-training-example.html">Train PyTorch ResNet model with GPUs on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/examples/mnist-training-example.html">Train a PyTorch model on Fashion MNIST with CPUs on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/examples/stable-diffusion-rayservice.html">Serve a StableDiffusion text-to-image model on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/examples/tpu-serve-stable-diffusion.html">Serve a Stable Diffusion model on GKE with TPUs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/examples/mobilenet-rayservice.html">Serve a MobileNet image classifier on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/examples/text-summarizer-rayservice.html">Serve a text summarizer on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/examples/rayjob-batch-inference-example.html">RayJob Batch Inference Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/examples/rayjob-kueue-priority-scheduling.html">Priority Scheduling with RayJob and Kueue</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/examples/rayjob-kueue-gang-scheduling.html">Gang Scheduling with RayJob and Kueue</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/examples/distributed-checkpointing-with-gcsfuse.html">Distributed checkpointing with KubeRay and GCSFuse</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/examples/modin-example.html">Use Modin with Ray on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/examples/vllm-rayservice.html">Serve a Large Language Model with vLLM on Kubernetes</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../cluster/kubernetes/k8s-ecosystem.html">KubeRay Ecosystem</a><input class="toctree-checkbox" id="toctree-checkbox-57" name="toctree-checkbox-57" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-57"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/k8s-ecosystem/ingress.html">Ingress</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/k8s-ecosystem/prometheus-grafana.html">Using Prometheus and Grafana</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/k8s-ecosystem/pyspy.html">Profiling with py-spy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/k8s-ecosystem/volcano.html">KubeRay integration with Volcano</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/k8s-ecosystem/yunikorn.html">KubeRay integration with Apache YuniKorn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/k8s-ecosystem/kueue.html">Gang scheduling and priority scheduling for RayJob with Kueue</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/k8s-ecosystem/istio.html">mTLS and L7 observability with Istio</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../cluster/kubernetes/benchmarks.html">KubeRay Benchmarks</a><input class="toctree-checkbox" id="toctree-checkbox-58" name="toctree-checkbox-58" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-58"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/benchmarks/memory-scalability-benchmark.html">KubeRay memory and scalability benchmark</a></li>

</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../cluster/kubernetes/troubleshooting.html">KubeRay Troubleshooting</a><input class="toctree-checkbox" id="toctree-checkbox-59" name="toctree-checkbox-59" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-59"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/troubleshooting/troubleshooting.html">Troubleshooting guide</a></li>

<li class="toctree-l4"><a class="reference internal" href="../../../cluster/kubernetes/troubleshooting/rayservice-troubleshooting.html">RayService troubleshooting</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../cluster/kubernetes/references.html">API Reference</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../cluster/vms/index.html">Deploying on VMs</a><input class="toctree-checkbox" id="toctree-checkbox-60" name="toctree-checkbox-60" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-60"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../cluster/vms/getting-started.html">Getting Started</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../cluster/vms/user-guides/index.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-61" name="toctree-checkbox-61" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-61"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/vms/user-guides/launching-clusters/index.html">Launching Ray Clusters on AWS, GCP, Azure, vSphere, On-Prem</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/vms/user-guides/large-cluster-best-practices.html">Best practices for deploying large clusters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/vms/user-guides/configuring-autoscaling.html">Configuring Autoscaling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/vms/user-guides/logging.html">Log Persistence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/vms/user-guides/community/index.html">Community Supported Cluster Managers</a></li>

</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../cluster/vms/examples/index.html">Examples</a><input class="toctree-checkbox" id="toctree-checkbox-62" name="toctree-checkbox-62" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-62"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/vms/examples/ml-example.html">Ray Train XGBoostTrainer on VMs</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../cluster/vms/references/index.html">API References</a><input class="toctree-checkbox" id="toctree-checkbox-63" name="toctree-checkbox-63" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-63"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/vms/references/ray-cluster-cli.html">Cluster Launcher Commands</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/vms/references/ray-cluster-configuration.html">Cluster YAML Configuration Options</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../cluster/metrics.html">Collecting and monitoring metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../cluster/configure-manage-dashboard.html">Configuring and Managing Ray Dashboard</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../cluster/running-applications/index.html">Applications Guide</a><input class="toctree-checkbox" id="toctree-checkbox-64" name="toctree-checkbox-64" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-64"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../cluster/running-applications/job-submission/index.html">Ray Jobs Overview</a><input class="toctree-checkbox" id="toctree-checkbox-65" name="toctree-checkbox-65" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-65"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/running-applications/job-submission/quickstart.html">Quickstart using the Ray Jobs CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/running-applications/job-submission/sdk.html">Python SDK Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/running-applications/job-submission/jobs-package-ref.html">Python SDK API Reference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/running-applications/job-submission/cli.html">Ray Jobs CLI API Reference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/running-applications/job-submission/rest.html">Ray Jobs REST API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../cluster/running-applications/job-submission/ray-client.html">Ray Client</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../cluster/running-applications/autoscaling/reference.html">Programmatic Cluster Scaling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../cluster/faq.html">FAQ</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../cluster/package-overview.html">Ray Cluster Management API</a><input class="toctree-checkbox" id="toctree-checkbox-66" name="toctree-checkbox-66" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-66"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../cluster/cli.html">Cluster Management CLI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../cluster/running-applications/job-submission/jobs-package-ref.html">Python SDK API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../cluster/running-applications/job-submission/cli.html">Ray Jobs CLI API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../cluster/running-applications/autoscaling/reference.html">Programmatic Cluster Scaling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../cluster/usage-stats.html">Usage Stats Collection</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../virtual-cluster/getting-started.html">Ray Virtual Clusters</a><input class="toctree-checkbox" id="toctree-checkbox-67" name="toctree-checkbox-67" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-67"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../virtual-cluster/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../virtual-cluster/design-overview.html">Design Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../virtual-cluster/management.html">Virtual Cluster Management API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../virtual-cluster/cli.html">Virtual Cluster CLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../virtual-cluster/examples.html">Examples</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../ray-observability/index.html">Monitoring and Debugging</a><input class="toctree-checkbox" id="toctree-checkbox-68" name="toctree-checkbox-68" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-68"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../ray-observability/getting-started.html">Ray Dashboard</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ray-observability/ray-distributed-debugger.html">Ray Distributed Debugger</a></li>



<li class="toctree-l2"><a class="reference internal" href="../../../ray-observability/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../ray-observability/user-guides/index.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-69" name="toctree-checkbox-69" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-69"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../ray-observability/user-guides/debug-apps/index.html">Debugging Applications</a><input class="toctree-checkbox" id="toctree-checkbox-70" name="toctree-checkbox-70" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-70"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-observability/user-guides/debug-apps/general-debugging.html">General Debugging</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-observability/user-guides/debug-apps/debug-memory.html">Debugging Memory Issues</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-observability/user-guides/debug-apps/debug-hangs.html">Debugging Hangs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-observability/user-guides/debug-apps/debug-failures.html">Debugging Failures</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-observability/user-guides/debug-apps/optimize-performance.html">Optimizing Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ray-observability/ray-distributed-debugger.html">Ray Distributed Debugger</a></li>



<li class="toctree-l4"><a class="reference internal" href="../../../ray-observability/user-guides/debug-apps/ray-debugging.html">Using the Ray Debugger</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-observability/user-guides/cli-sdk.html">Monitoring with the CLI or SDK</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-observability/user-guides/configure-logging.html">Configuring Logging</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-observability/user-guides/profiling.html">Profiling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-observability/user-guides/add-app-metrics.html">Adding Application-Level Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-observability/user-guides/ray-tracing.html">Tracing</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../ray-observability/reference/index.html">Reference</a><input class="toctree-checkbox" id="toctree-checkbox-71" name="toctree-checkbox-71" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-71"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-observability/reference/api.html">State API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-observability/reference/cli.html">State CLI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-observability/reference/system-metrics.html">System Metrics</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../ray-contribute/index.html">Developer Guides</a><input class="toctree-checkbox" id="toctree-checkbox-72" name="toctree-checkbox-72" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-72"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../ray-contribute/stability.html">API Stability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ray-contribute/api-policy.html">API Policy</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../ray-contribute/getting-involved.html">Getting Involved / Contributing</a><input class="toctree-checkbox" id="toctree-checkbox-73" name="toctree-checkbox-73" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-73"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-contribute/development.html">Building Ray from Source</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-contribute/ci.html">CI Testing Workflow on PRs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-contribute/docs.html">Contributing to the Ray Documentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-contribute/writing-code-snippets.html">How to write code snippets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-contribute/fake-autoscaler.html">Testing Autoscaling Locally</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../../ray-contribute/testing-tips.html">Tips for testing Ray programs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-contribute/debugging.html">Debugging for Ray Developers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ray-contribute/profiling.html">Profiling for Ray Developers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../ray-core/configure.html">Configuring Ray</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ray-contribute/whitepaper.html">Architecture Whitepapers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../ray-references/glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ray-security/index.html">Security</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">GPT-J-6B...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="gpt-j-6b-fine-tuning-with-ray-train-and-deepspeed">
<h1>GPT-J-6B Fine-Tuning with Ray Train and DeepSpeed<a class="headerlink" href="#gpt-j-6b-fine-tuning-with-ray-train-and-deepspeed" title="Link to this heading">#</a></h1>
<a id="try-anyscale-quickstart-gptj_deepspeed_fine_tuning" href="https://www.anyscale.com/ray-on-anyscale?utm_source=ray_docs&utm_medium=docs&utm_campaign=gptj_deepspeed_fine_tuning">
    <img src="../../../_static/img/run-on-anyscale.svg" alt="try-anyscale-quickstart">
</a>
<br></br>
<p>This example showcases how to use Ray Train for <strong>GPT-J fine-tuning</strong>. GPT-J is a GPT-2-like causal language model trained on the Pile dataset. This particular model has 6 billion parameters. For more information, see <a class="reference external" href="https://huggingface.co/docs/transformers/model_doc/gptj">GPT-J</a>.</p>
<p>This example uses the Ray Train ðŸ¤— Transformers integration and a pre-trained model from the Hugging Face Hub. Note that this example is adaptable to other similar models.</p>
<p>This is an advanced example that focuses on the performance and distributed computing aspects of Ray Train. For a beginner-friendly introduction to the Ray Train ðŸ¤— Transformers integration, see <a class="reference internal" href="../transformers/transformers_torch_trainer_basic.html#transformers-torch-trainer-basic-example"><span class="std std-ref">Basic Example for HuggingFace Transformers</span></a>.</p>
<p>Read <a class="reference internal" href="../../overview.html#train-key-concepts"><span class="std std-ref">Ray Train Key Concepts</span></a> and <a class="reference internal" href="../../user-guides/data-loading-preprocessing.html#data-ingest-torch"><span class="std std-ref">Ray Data Integration User Guides</span></a> before starting this example.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To run this example, make sure your Ray cluster has access to at least one GPU with 16 or more GBs of memory. The required amount of memory depends on the model. This notebook is tested with 16 g4dn.4xlarge instances (including the head node).</p>
</div>
<p>This notebook has the following steps:</p>
<ol class="arabic simple">
<li><p><a class="reference internal" href="#gptj-setup">Set up Ray</a></p></li>
<li><p><a class="reference internal" href="#gptj-load">Load the dataset</a></p></li>
<li><p><a class="reference internal" href="#gptj-preprocess">Preprocess the dataset with Ray Data</a></p></li>
<li><p><a class="reference internal" href="#gptj-train">Run the training with Ray Train</a></p></li>
<li><p><a class="reference internal" href="#gptj-predict">Generate text from prompt</a></p></li>
</ol>
<p>Uncomment and run the following line in order to install all the necessary dependencies (this notebook was tested with <code class="docutils literal notranslate"><span class="pre">accelerate=0.18.0</span></code>, <code class="docutils literal notranslate"><span class="pre">transformers==4.26.0</span></code>, <code class="docutils literal notranslate"><span class="pre">deepspeed==0.12.3</span></code>):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span><span class="s2">&quot;datasets&quot;</span><span class="w"> </span><span class="s2">&quot;evaluate&quot;</span><span class="w"> </span><span class="s2">&quot;accelerate==0.18.0&quot;</span><span class="w"> </span><span class="s2">&quot;transformers==4.26.0&quot;</span><span class="w"> </span><span class="s2">&quot;torch&gt;=1.12.0&quot;</span><span class="w"> </span><span class="s2">&quot;deepspeed==0.12.3&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">os</span>
</pre></div>
</div>
</div>
</div>
<section id="set-up-ray">
<span id="gptj-setup"></span><h2>Set up Ray<a class="headerlink" href="#set-up-ray" title="Link to this heading">#</a></h2>
<p>First, letâ€™s set some global variables. We will use 16 workers, each being assigned 1 GPU and 8 CPUs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;EleutherAI/gpt-j-6B&quot;</span>
<span class="n">use_gpu</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">num_workers</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">cpus_per_worker</span> <span class="o">=</span> <span class="mi">8</span>
</pre></div>
</div>
</div>
</div>
<p>We will use <code class="docutils literal notranslate"><span class="pre">ray.init()</span></code> to initialize a local cluster. By default, this cluster will be comprised of only the machine you are running this notebook on. You can also run this notebook on an Anyscale cluster.</p>
<p>We define a <a class="reference internal" href="../../../ray-core/handling-dependencies.html#runtime-environments"><span class="std std-ref">runtime environment</span></a> to ensure that the Ray workers have access to all the necessary packages. You can omit the <code class="docutils literal notranslate"><span class="pre">runtime_env</span></code> argument if you have all of the packages already installed on each node in your cluster.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ray</span>

<span class="n">ray</span><span class="o">.</span><span class="n">init</span><span class="p">(</span>
    <span class="n">runtime_env</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;pip&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;datasets&quot;</span><span class="p">,</span>
            <span class="s2">&quot;evaluate&quot;</span><span class="p">,</span>
            <span class="c1"># The latest combination accelerate==0.25.0, transformers==4.36.0, deepspeed==0.12.4</span>
            <span class="c1"># has issues with DeepSpeed process group initialization,</span>
            <span class="c1"># and will result in a batch_size validation problem.</span>
            <span class="c1"># TODO(ml-team): get rid of the pins once the issue is fixed.</span>
            <span class="s2">&quot;accelerate==0.18.0&quot;</span><span class="p">,</span>
            <span class="s2">&quot;transformers==4.26.0&quot;</span><span class="p">,</span>
            <span class="s2">&quot;torch&gt;=1.12.0&quot;</span><span class="p">,</span>
            <span class="s2">&quot;deepspeed==0.12.3&quot;</span><span class="p">,</span>
        <span class="p">],</span>
    <span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># THIS SHOULD BE HIDDEN IN DOCS AND ONLY RAN IN CI</span>
<span class="c1"># Download the model from our S3 mirror as it&#39;s faster</span>

<span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">import</span> <span class="nn">subprocess</span>
<span class="kn">import</span> <span class="nn">ray.util.scheduling_strategies</span>


<span class="k">def</span> <span class="nf">force_on_node</span><span class="p">(</span><span class="n">node_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">remote_func_or_actor_class</span><span class="p">):</span>
    <span class="n">scheduling_strategy</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">scheduling_strategies</span><span class="o">.</span><span class="n">NodeAffinitySchedulingStrategy</span><span class="p">(</span>
        <span class="n">node_id</span><span class="o">=</span><span class="n">node_id</span><span class="p">,</span> <span class="n">soft</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
    <span class="n">options</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;scheduling_strategy&quot;</span><span class="p">:</span> <span class="n">scheduling_strategy</span><span class="p">}</span>
    <span class="k">return</span> <span class="n">remote_func_or_actor_class</span><span class="o">.</span><span class="n">options</span><span class="p">(</span><span class="o">**</span><span class="n">options</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">run_on_every_node</span><span class="p">(</span><span class="n">remote_func_or_actor_class</span><span class="p">,</span> <span class="o">**</span><span class="n">remote_kwargs</span><span class="p">):</span>
    <span class="n">refs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">ray</span><span class="o">.</span><span class="n">nodes</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">node</span><span class="p">[</span><span class="s2">&quot;Alive&quot;</span><span class="p">]</span> <span class="ow">and</span> <span class="n">node</span><span class="p">[</span><span class="s2">&quot;Resources&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;GPU&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">refs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">force_on_node</span><span class="p">(</span><span class="n">node</span><span class="p">[</span><span class="s2">&quot;NodeID&quot;</span><span class="p">],</span> <span class="n">remote_func_or_actor_class</span><span class="p">)</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span>
                    <span class="o">**</span><span class="n">remote_kwargs</span>
                <span class="p">)</span>
            <span class="p">)</span>
    <span class="k">return</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">refs</span><span class="p">)</span>


<span class="nd">@ray</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="n">num_gpus</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">download_model</span><span class="p">():</span>
    <span class="kn">from</span> <span class="nn">transformers.utils.hub</span> <span class="kn">import</span> <span class="n">TRANSFORMERS_CACHE</span>

    <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span>
        <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">TRANSFORMERS_CACHE</span><span class="p">,</span> <span class="s2">&quot;models--EleutherAI--gpt-j-6B&quot;</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="s2">&quot;mkdir&quot;</span><span class="p">,</span> <span class="s2">&quot;-p&quot;</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;snapshots&quot;</span><span class="p">,</span> <span class="s2">&quot;main&quot;</span><span class="p">)])</span>
    <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="s2">&quot;mkdir&quot;</span><span class="p">,</span> <span class="s2">&quot;-p&quot;</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;refs&quot;</span><span class="p">)])</span>
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;refs&quot;</span><span class="p">,</span> <span class="s2">&quot;main&quot;</span><span class="p">)):</span>
        <span class="k">return</span>
    <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="s2">&quot;aws&quot;</span><span class="p">,</span>
            <span class="s2">&quot;s3&quot;</span><span class="p">,</span>
            <span class="s2">&quot;sync&quot;</span><span class="p">,</span>
            <span class="s2">&quot;--no-sign-request&quot;</span><span class="p">,</span>
            <span class="s2">&quot;s3://large-dl-models-mirror/models--EleutherAI--gpt-j-6B/main/&quot;</span><span class="p">,</span>
            <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;snapshots&quot;</span><span class="p">,</span> <span class="s2">&quot;main&quot;</span><span class="p">),</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;snapshots&quot;</span><span class="p">,</span> <span class="s2">&quot;main&quot;</span><span class="p">,</span> <span class="s2">&quot;hash&quot;</span><span class="p">),</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f_hash</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;refs&quot;</span><span class="p">,</span> <span class="s2">&quot;main&quot;</span><span class="p">),</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">f_hash</span><span class="p">)</span>
    <span class="n">os</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span>
        <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;snapshots&quot;</span><span class="p">,</span> <span class="s2">&quot;main&quot;</span><span class="p">),</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;snapshots&quot;</span><span class="p">,</span> <span class="n">f_hash</span><span class="p">)</span>
    <span class="p">)</span>


<span class="n">_</span> <span class="o">=</span> <span class="n">run_on_every_node</span><span class="p">(</span><span class="n">download_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="loading-the-dataset">
<span id="gptj-load"></span><h2>Loading the dataset<a class="headerlink" href="#loading-the-dataset" title="Link to this heading">#</a></h2>
<p>We will be fine-tuning the model on the <a class="reference external" href="https://huggingface.co/datasets/tiny_shakespeare"><code class="docutils literal notranslate"><span class="pre">tiny_shakespeare</span></code> dataset</a>, comprised of 40,000 lines of Shakespeare from a variety of Shakespeareâ€™s plays. The aim will be to make the GPT-J model better at generating text in the style of Shakespeare.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loading tiny_shakespeare dataset&quot;</span><span class="p">)</span>
<span class="n">current_dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;tiny_shakespeare&quot;</span><span class="p">)</span>
<span class="n">current_dataset</span>
</pre></div>
</div>
</div>
</div>
<p>We will use <a class="reference internal" href="../../../data/data.html#data"><span class="std std-ref">Ray Data</span></a> for distributed preprocessing and data ingestion. We can easily convert the dataset obtained from Hugging Face Hub to Ray Data by using <a class="reference internal" href="../../../data/api/doc/ray.data.from_huggingface.html#ray.data.from_huggingface" title="ray.data.from_huggingface"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ray.data.from_huggingface()</span></code></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ray.data</span>

<span class="n">ray_datasets</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">from_huggingface</span><span class="p">(</span><span class="n">current_dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]),</span>
    <span class="s2">&quot;validation&quot;</span><span class="p">:</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">from_huggingface</span><span class="p">(</span><span class="n">current_dataset</span><span class="p">[</span><span class="s2">&quot;validation&quot;</span><span class="p">]),</span>
<span class="p">}</span>

<span class="n">ray_datasets</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;train&#39;: MaterializedDataset(num_blocks=1, num_rows=1, schema={text: string}),
 &#39;validation&#39;: MaterializedDataset(num_blocks=1, num_rows=1, schema={text: string})}
</pre></div>
</div>
</div>
</div>
<p id="gptj-preprocess">Note that the dataset is represented by a single line of large string, and needs some preprocessing. To do this, use the <a class="reference internal" href="../../../data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches" title="ray.data.Dataset.map_batches"><code class="xref py py-meth docutils literal notranslate"><span class="pre">map_batches()</span></code></a> API to apply transformation functions to batches of data.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">split_text</span></code> function takes the single string and splits it into separate lines, removing empty lines and character names ending with â€˜:â€™ (eg. â€˜ROMEO:â€™). The <code class="docutils literal notranslate"><span class="pre">tokenize</span></code> function takes the lines and tokenizes them using the ðŸ¤— Tokenizer associated with the model, ensuring each entry has the same length (<code class="docutils literal notranslate"><span class="pre">block_size</span></code>) by padding and truncating. This preprocessing is necessary for training.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This preprocessing can be done in other ways. A common pattern is to tokenize first, and then split the obtained tokens into equally-sized blocks.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">block_size</span> <span class="o">=</span> <span class="mi">512</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>


<span class="k">def</span> <span class="nf">split_text</span><span class="p">(</span><span class="n">batch</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
    <span class="n">text</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
    <span class="n">flat_text</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">split_text</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">x</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">flat_text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">x</span><span class="o">.</span><span class="n">strip</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;:&quot;</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">split_text</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">batch</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">use_fast</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
        <span class="nb">list</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]),</span>
        <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;np&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">ret</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ret</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>


<span class="n">processed_datasets</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">key</span><span class="p">:</span> <span class="p">(</span>
        <span class="n">ds</span><span class="o">.</span><span class="n">map_batches</span><span class="p">(</span><span class="n">split_text</span><span class="p">,</span> <span class="n">batch_format</span><span class="o">=</span><span class="s2">&quot;pandas&quot;</span><span class="p">)</span>
        <span class="o">.</span><span class="n">map_batches</span><span class="p">(</span><span class="n">tokenize</span><span class="p">,</span> <span class="n">batch_format</span><span class="o">=</span><span class="s2">&quot;pandas&quot;</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">ds</span> <span class="ow">in</span> <span class="n">ray_datasets</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
<span class="p">}</span>
<span class="n">processed_datasets</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;train&#39;: MapBatches(tokenize)
 +- MapBatches(split_text)
    +- Dataset(num_blocks=1, num_rows=1, schema={text: string}),
 &#39;validation&#39;: MapBatches(tokenize)
 +- MapBatches(split_text)
    +- Dataset(num_blocks=1, num_rows=1, schema={text: string})}
</pre></div>
</div>
</div>
</div>
<section id="fine-tuning-the-model-with-ray-train">
<span id="gptj-train"></span><h3>Fine-tuning the model with Ray Train<a class="headerlink" href="#fine-tuning-the-model-with-ray-train" title="Link to this heading">#</a></h3>
<p>Configure Ray Trainâ€™s <a class="reference internal" href="../../api/doc/ray.train.torch.TorchTrainer.html#ray.train.torch.TorchTrainer" title="ray.train.torch.TorchTrainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TorchTrainer</span></code></a> to perform distributed fine-tuning of the model. Specify a <code class="docutils literal notranslate"><span class="pre">train_loop_per_worker</span></code> function, which defines the training logic to be distributed by Ray using Distributed Data Parallelism, which uses the PyTorch Distributed backend internally. Each worker has its own copy of the model, but operates on different data. At the end of each step, all the workers sync gradients.</p>
<p>Because GPT-J is a relatively large model, it may not be possible to fit it on smaller GPU types (&lt;=16 GB GRAM). To deal with that issue, this example uses <a class="reference external" href="https://github.com/microsoft/DeepSpeed">DeepSpeed</a>, a library to optimize the training process and to offload and partition optimizer and parameter states, reducing GRAM usage. Furthermore, DeepSpeed ZeRO Stage 3 can load large models without running out of memory.</p>
<p>ðŸ¤— Transformers and Ray Trainâ€™s <a class="reference internal" href="../../api/api.html#train-transformers-integration"><span class="std std-ref">integrations</span></a> allow you to easily configure and use DDP and DeepSpeed. All you need to do is specify the DeepSpeed configuration in the <a class="reference external" href="https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments"><code class="docutils literal notranslate"><span class="pre">TrainingArguments</span></code></a> object.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>There are many DeepSpeed settings that allow you to trade-off speed for memory usage. The settings used below are tailored to the cluster setup used (16 g4dn.4xlarge nodes) and per device batch size of 16. Some things to keep in mind:</p>
<ul class="simple">
<li><p>If your GPUs support bfloat16, use that instead of float16 mixed precision to get better performance and prevent overflows. Replace <code class="docutils literal notranslate"><span class="pre">fp16=True</span></code> with <code class="docutils literal notranslate"><span class="pre">bf16=True</span></code> in <code class="docutils literal notranslate"><span class="pre">TrainingArguments</span></code>.</p></li>
<li><p>If you are running out of GRAM: try reducing batch size (defined in the cell below the next one), set <code class="docutils literal notranslate"><span class="pre">&quot;overlap_comm&quot;:</span> <span class="pre">False</span></code> in DeepSpeed config.</p></li>
<li><p>If you are running out of RAM, add more nodes to your cluster, use nodes with more RAM, set <code class="docutils literal notranslate"><span class="pre">&quot;pin_memory&quot;:</span> <span class="pre">False</span></code> in the DeepSpeed config, reduce the batch size, and remove <code class="docutils literal notranslate"><span class="pre">&quot;offload_param&quot;</span></code> from the DeepSpeed config.</p></li>
</ul>
<p>For more information on DeepSpeed configuration, refer to <a class="reference external" href="https://huggingface.co/docs/transformers/main_classes/deepspeed">Hugging Face documentation</a> and <a class="reference external" href="https://www.deepspeed.ai/docs/config-json/">DeepSpeed documentation</a>.</p>
<p>Additionally, if you prefer a lower-level API, the logic below can be expressed as an <a class="reference external" href="https://github.com/huggingface/accelerate/blob/main/examples/by_feature/deepspeed_with_config_support.py">Accelerate training loop</a> distributed by a Ray Train <a class="reference internal" href="../../api/doc/ray.train.torch.TorchTrainer.html#ray.train.torch.TorchTrainer" title="ray.train.torch.torch_trainer.TorchTrainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TorchTrainer</span></code></a>.</p>
</div>
<section id="training-speed">
<h4>Training speed<a class="headerlink" href="#training-speed" title="Link to this heading">#</a></h4>
<p>As this example uses data parallelism, each worker operates on its own shard of the data. The batch size set in <code class="docutils literal notranslate"><span class="pre">train_ds.iter_torch_batches</span></code> is the <strong>per device batch size</strong> (per worker batch size). By changing the number of workers, you can change the <strong>effective batch size</strong> and thus the time needed for training to complete. Calculate the effective batch size as <code class="docutils literal notranslate"><span class="pre">per</span> <span class="pre">device</span> <span class="pre">batch</span> <span class="pre">size</span> <span class="pre">*</span> <span class="pre">number</span> <span class="pre">of</span> <span class="pre">workers</span> <span class="pre">*</span> <span class="pre">number</span> <span class="pre">of</span> <span class="pre">gradient</span> <span class="pre">accumulation</span> <span class="pre">steps</span></code>. As you add more workers, the effective batch size rises and thus less time is needed to complete a full epoch. While the speedup is not exactly linear due to extra communication overheads, in many cases it can be close to linear.</p>
<p>The preprocessed dataset has 1348 examples. We have set per device batch size to 16.</p>
<ul class="simple">
<li><p>With 16 g4dn.4xlarge nodes, the effective batch size was 256, which equals to 85 steps per epoch. One epoch took <strong>~2440 seconds</strong> (including initialization time).</p></li>
<li><p>With 32 g4dn.4xlarge nodes, the effective batch size was 512, which equals to 43 steps per epoch. One epoch took <strong>~1280 seconds</strong> (including initialization time).</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">evaluate</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Trainer</span><span class="p">,</span>
    <span class="n">TrainingArguments</span><span class="p">,</span>
    <span class="n">GPTJForCausalLM</span><span class="p">,</span>
    <span class="n">AutoTokenizer</span><span class="p">,</span>
    <span class="n">default_data_collator</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">transformers.utils.logging</span> <span class="kn">import</span> <span class="n">disable_progress_bar</span><span class="p">,</span> <span class="n">enable_progress_bar</span>

<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">train</span>
<span class="kn">from</span> <span class="nn">ray.train.huggingface.transformers</span> <span class="kn">import</span> <span class="n">prepare_trainer</span><span class="p">,</span> <span class="n">RayTrainReportCallback</span>


<span class="k">def</span> <span class="nf">train_func</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="c1"># Use the actual number of CPUs assigned by Ray</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;OMP_NUM_THREADS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span>
        <span class="n">train</span><span class="o">.</span><span class="n">get_context</span><span class="p">()</span><span class="o">.</span><span class="n">get_trial_resources</span><span class="p">()</span><span class="o">.</span><span class="n">bundles</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;CPU&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="c1"># Enable tf32 for better performance</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">matmul</span><span class="o">.</span><span class="n">allow_tf32</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">epochs</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;epochs&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;warmup_steps&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">,</span> <span class="mf">0.00002</span><span class="p">)</span>
    <span class="n">weight_decay</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;weight_decay&quot;</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
    <span class="n">steps_per_epoch</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;steps_per_epoch&quot;</span><span class="p">)</span>

    <span class="n">deepspeed</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;fp16&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;enabled&quot;</span><span class="p">:</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
            <span class="s2">&quot;initial_scale_power&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
            <span class="s2">&quot;hysteresis&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
            <span class="s2">&quot;consecutive_hysteresis&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="s2">&quot;bf16&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;enabled&quot;</span><span class="p">:</span> <span class="s2">&quot;auto&quot;</span><span class="p">},</span>
        <span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;AdamW&quot;</span><span class="p">,</span>
            <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
                <span class="s2">&quot;betas&quot;</span><span class="p">:</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
                <span class="s2">&quot;eps&quot;</span><span class="p">:</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
            <span class="p">},</span>
        <span class="p">},</span>
        <span class="s2">&quot;zero_optimization&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
            <span class="s2">&quot;offload_optimizer&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
                <span class="s2">&quot;pin_memory&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
            <span class="p">},</span>
            <span class="s2">&quot;overlap_comm&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
            <span class="s2">&quot;contiguous_gradients&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
            <span class="s2">&quot;reduce_bucket_size&quot;</span><span class="p">:</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
            <span class="s2">&quot;stage3_prefetch_bucket_size&quot;</span><span class="p">:</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
            <span class="s2">&quot;stage3_param_persistence_threshold&quot;</span><span class="p">:</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
            <span class="s2">&quot;gather_16bit_weights_on_model_save&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
            <span class="s2">&quot;round_robin_gradients&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="s2">&quot;gradient_accumulation_steps&quot;</span><span class="p">:</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="s2">&quot;gradient_clipping&quot;</span><span class="p">:</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="s2">&quot;steps_per_print&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
        <span class="s2">&quot;train_batch_size&quot;</span><span class="p">:</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="s2">&quot;train_micro_batch_size_per_gpu&quot;</span><span class="p">:</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="s2">&quot;wall_clock_breakdown&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Preparing training arguments&quot;</span><span class="p">)</span>
    <span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
        <span class="s2">&quot;output&quot;</span><span class="p">,</span>
        <span class="n">logging_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">save_strategy</span><span class="o">=</span><span class="s2">&quot;steps&quot;</span><span class="p">,</span>
        <span class="n">save_steps</span><span class="o">=</span><span class="n">steps_per_epoch</span><span class="p">,</span>
        <span class="n">max_steps</span><span class="o">=</span><span class="n">steps_per_epoch</span> <span class="o">*</span> <span class="n">epochs</span><span class="p">,</span>
        <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span>
        <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">,</span>
        <span class="n">label_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">,</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">],</span>
        <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">report_to</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span>
        <span class="n">disable_tqdm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># declutter the output a little</span>
        <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">gradient_checkpointing</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">deepspeed</span><span class="o">=</span><span class="n">deepspeed</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">disable_progress_bar</span><span class="p">()</span>

    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loading model&quot;</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">GPTJForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">resize_token_embeddings</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">))</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model loaded&quot;</span><span class="p">)</span>

    <span class="n">enable_progress_bar</span><span class="p">()</span>

    <span class="n">metric</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>

    <span class="n">train_ds</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
    <span class="n">eval_ds</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;validation&quot;</span><span class="p">)</span>

    <span class="n">train_ds_iterable</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">iter_torch_batches</span><span class="p">(</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">local_shuffle_buffer_size</span><span class="o">=</span><span class="n">train</span><span class="o">.</span><span class="n">get_context</span><span class="p">()</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">eval_ds_iterable</span> <span class="o">=</span> <span class="n">eval_ds</span><span class="o">.</span><span class="n">iter_torch_batches</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>
        <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
        <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_ds_iterable</span><span class="p">,</span>
        <span class="n">eval_dataset</span><span class="o">=</span><span class="n">eval_ds_iterable</span><span class="p">,</span>
        <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
        <span class="n">data_collator</span><span class="o">=</span><span class="n">default_data_collator</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Add callback to report checkpoints to Ray Train</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">add_callback</span><span class="p">(</span><span class="n">RayTrainReportCallback</span><span class="p">())</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">prepare_trainer</span><span class="p">(</span><span class="n">trainer</span><span class="p">)</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>After defining the training function, instantiate the <a class="reference internal" href="../../api/doc/ray.train.torch.TorchTrainer.html#ray.train.torch.TorchTrainer" title="ray.train.torch.TorchTrainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TorchTrainer</span></code></a>. Aside from the function, set the <code class="docutils literal notranslate"><span class="pre">scaling_config</span></code> to control the number of workers and amount of resources to use, and <code class="docutils literal notranslate"><span class="pre">datasets</span></code>(the preprocessed Ray Datasets) to use for training and evaluation.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Running with multiple nodes necessitates the persistence of checkpoints
and other outputs to some external storage for access after training has completed.
<strong>You should set up cloud storage or NFS, then replace <code class="docutils literal notranslate"><span class="pre">storage_path</span></code> with your own cloud bucket URI or NFS path.</strong></p>
<p>See <a class="reference internal" href="../../user-guides/persistent-storage.html#persistent-storage-guide"><span class="std std-ref">Configuration and Persistent Storage</span></a> for more details.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">storage_path</span> <span class="o">=</span> <span class="s2">&quot;s3://your-bucket-here&quot;</span>  <span class="c1"># TODO: Set up cloud storage</span>
<span class="c1"># storage_path=&quot;/mnt/path/to/nfs&quot;     # TODO: Alternatively, set up NFS</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">train_ds_size</span> <span class="o">=</span> <span class="n">processed_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="n">steps_per_epoch</span> <span class="o">=</span> <span class="n">train_ds_size</span> <span class="o">//</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_workers</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>
<span class="kn">from</span> <span class="nn">ray.train</span> <span class="kn">import</span> <span class="n">RunConfig</span><span class="p">,</span> <span class="n">ScalingConfig</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_loop_per_worker</span><span class="o">=</span><span class="n">train_func</span><span class="p">,</span>
    <span class="n">train_loop_config</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;epochs&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="n">batch_size</span><span class="p">,</span>  <span class="c1"># per device</span>
        <span class="s2">&quot;steps_per_epoch&quot;</span><span class="p">:</span> <span class="n">steps_per_epoch</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
        <span class="n">use_gpu</span><span class="o">=</span><span class="n">use_gpu</span><span class="p">,</span>
        <span class="n">resources_per_worker</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;GPU&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="n">cpus_per_worker</span><span class="p">},</span>
    <span class="p">),</span>
    <span class="n">datasets</span><span class="o">=</span><span class="n">processed_datasets</span><span class="p">,</span>
    <span class="n">run_config</span><span class="o">=</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">storage_path</span><span class="o">=</span><span class="n">storage_path</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Finally, call the <a class="reference internal" href="../../api/doc/ray.train.torch.TorchTrainer.fit.html#ray.train.torch.TorchTrainer.fit" title="ray.train.torch.TorchTrainer.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a> method to start training with Ray Train. Save the <a class="reference internal" href="../../api/doc/ray.train.Result.html#ray.train.Result" title="ray.train.Result"><code class="xref py py-class docutils literal notranslate"><span class="pre">Result</span></code></a> object to a variable to access metrics and checkpoints.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div class="tuneStatus">
  <div style="display: flex;flex-direction: row">
    <div style="display: flex;flex-direction: column;">
      <h3>Tune Status</h3>
      <table>
<tbody>
<tr><td>Current time:</td><td>2023-08-18 18:54:02</td></tr>
<tr><td>Running for: </td><td>00:44:50.37        </td></tr>
<tr><td>Memory:      </td><td>10.2/62.0 GiB      </td></tr>
</tbody>
</table>
    </div>
    <div class="vDivider"></div>
    <div class="systemInfo">
      <h3>System Info</h3>
      Using FIFO scheduling algorithm.<br>Logical resource usage: 129.0/256 CPUs, 16.0/16 GPUs
    </div>
    
  </div>
  <div class="hDivider"></div>
  <div class="trialStatus">
    <h3>Trial Status</h3>
    <table>
<thead>
<tr><th>Trial name              </th><th>status    </th><th>loc            </th><th style="text-align: right;">  iter</th><th style="text-align: right;">  total time (s)</th><th style="text-align: right;">  loss</th><th style="text-align: right;">  learning_rate</th><th style="text-align: right;">  epoch</th></tr>
</thead>
<tbody>
<tr><td>TorchTrainer_01ea5_00000</td><td>TERMINATED</td><td>10.0.60.59:8839</td><td style="text-align: right;">     1</td><td style="text-align: right;">         2663.78</td><td style="text-align: right;"> 0.069</td><td style="text-align: right;">    2.38095e-07</td><td style="text-align: right;">      1</td></tr>
</tbody>
</table>
  </div>
</div>
<style>
.tuneStatus {
  color: var(--jp-ui-font-color1);
}
.tuneStatus .systemInfo {
  display: flex;
  flex-direction: column;
}
.tuneStatus td {
  white-space: nowrap;
}
.tuneStatus .trialStatus {
  display: flex;
  flex-direction: column;
}
.tuneStatus h3 {
  font-weight: bold;
}
.tuneStatus .hDivider {
  border-bottom-width: var(--jp-border-width);
  border-bottom-color: var(--jp-border-color0);
  border-bottom-style: solid;
}
.tuneStatus .vDivider {
  border-left-width: var(--jp-border-width);
  border-left-color: var(--jp-border-color0);
  border-left-style: solid;
  margin: 0.5em 1em 0.5em 1em;
}
</style>
</div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(TrainTrainable pid=8839, ip=10.0.60.59)</span> 2023-08-18 18:09:16.315108: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(TrainTrainable pid=8839, ip=10.0.60.59)</span> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(TrainTrainable pid=8839, ip=10.0.60.59)</span> 2023-08-18 18:09:16.462944: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(TrainTrainable pid=8839, ip=10.0.60.59)</span> 2023-08-18 18:09:17.336229: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libnvinfer.so.7&#39;; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(TrainTrainable pid=8839, ip=10.0.60.59)</span> 2023-08-18 18:09:17.336299: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libnvinfer_plugin.so.7&#39;; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(TrainTrainable pid=8839, ip=10.0.60.59)</span> 2023-08-18 18:09:17.336306: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(TrainTrainable pid=8839, ip=10.0.60.59)</span> --------------------------------------------------------------------------
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(TrainTrainable pid=8839, ip=10.0.60.59)</span>                  Aim collects anonymous usage analytics.                 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(TrainTrainable pid=8839, ip=10.0.60.59)</span>                         Read how to opt-out here:                         
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(TrainTrainable pid=8839, ip=10.0.60.59)</span>     https://aimstack.readthedocs.io/en/latest/community/telemetry.html    
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(TrainTrainable pid=8839, ip=10.0.60.59)</span> --------------------------------------------------------------------------
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(TrainTrainable pid=8839, ip=10.0.60.59)</span> comet_ml is installed but `COMET_API_KEY` is not set.
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(TorchTrainer pid=8839, ip=10.0.60.59)</span> Starting distributed worker processes: [&#39;8911 (10.0.60.59)&#39;, &#39;36675 (10.0.13.222)&#39;, &#39;8880 (10.0.63.99)&#39;, &#39;8867 (10.0.49.236)&#39;, &#39;49329 (10.0.40.253)&#39;, &#39;8845 (10.0.18.195)&#39;, &#39;36249 (10.0.11.26)&#39;, &#39;8858 (10.0.0.119)&#39;, &#39;8857 (10.0.44.114)&#39;, &#39;8885 (10.0.47.209)&#39;, &#39;36311 (10.0.27.53)&#39;, &#39;8830 (10.0.30.35)&#39;, &#39;8875 (10.0.0.80)&#39;, &#39;8851 (10.0.43.240)&#39;, &#39;9631 (10.0.57.153)&#39;, &#39;36262 (10.0.52.191)&#39;]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> Setting up process group for: env:// [rank=0, world_size=16]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> 2023-08-18 18:09:25.209122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> 2023-08-18 18:09:25.358493: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> 2023-08-18 18:09:26.095161: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libnvinfer.so.7&#39;; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> 2023-08-18 18:09:26.095229: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libnvinfer_plugin.so.7&#39;; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> 2023-08-18 18:09:26.095236: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(SplitCoordinator pid=8980, ip=10.0.60.59)</span> Auto configuring locality_with_output=[&#39;6002ded0aaa53ce9a0351d22a72b344ef411a422919132f41d9f937a&#39;, &#39;d3bbd390b6fe73f26202f96d75998946cf3e8b457528d426db0c6e07&#39;, &#39;fe6aaf54317ee630a02d23e0d49581b57b5cd51316eaf769e28bb045&#39;, &#39;f7de4694a4f764c05a9c51a6a4bd40ac33f3fced3b25127b25cd4ac3&#39;, &#39;42866a2fba4ce2ab4b6645c4d731d486b762e2b23ac24cafccba7096&#39;, &#39;8a7272830662c7e756a656de0a9b433a3a1f9b990768f692b6fe11a7&#39;, &#39;bba62e8b57552509c62a6b6b7fd67c1a2280b9d81b3d9c41eb4d1b9b&#39;, &#39;b40764f303538c24bc439106f2e7b2144d382bfed6c9fdec15ab828e&#39;, &#39;d1de4d4b6d44eff93857026df4ef0f70e24e3dc91e15d87015f2ed32&#39;, &#39;4d6a9dc1aa7bfc80cb73d9f66f4e28041807f12769391f5643bce143&#39;, &#39;8bcc7235f459b61be21fe158d0bae4fef2ec6de013ec60e7aaf7897a&#39;, &#39;73c50b995811afa0ece70fd3d4466b7fd0dc85a97d6807128b2c47da&#39;, &#39;03bf3d374a9f857b1cd1aebdbe028208f7904b077fb151790e03e9fe&#39;, &#39;9f7fc101a7d6b3e17b72e57ca1c92f91d13aa385a6740f99d58ec016&#39;, &#39;867844d104a8e9351a1dcc8bbd61d99906a8dc5b53e220c2ae2efbe1&#39;, &#39;7677b344c59d6b30c3db451f48e346d61bb60cc798e5567aa4e0a1ea&#39;]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=49329)</span> comet_ml is installed but `COMET_API_KEY` is not set.
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8867, ip=10.0.49.236)</span> --------------------------------------------------------------------------
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8867, ip=10.0.49.236)</span>                  Aim collects anonymous usage analytics.                 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8867, ip=10.0.49.236)</span>                         Read how to opt-out here:                         
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8867, ip=10.0.49.236)</span>     https://aimstack.readthedocs.io/en/latest/community/telemetry.html    
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8867, ip=10.0.49.236)</span> --------------------------------------------------------------------------
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(SplitCoordinator pid=8980, ip=10.0.60.59)</span> 2023-08-18 18:09:26.534936: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA<span class=" -Color -Color-Green"> [repeated 16x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(SplitCoordinator pid=8980, ip=10.0.60.59)</span> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(SplitCoordinator pid=8980, ip=10.0.60.59)</span> 2023-08-18 18:09:26.667181: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8885, ip=10.0.47.209)</span> Preparing training arguments
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> Loading model
<span class=" -Color -Color-Bold -Color-Bold-Faint -Color-Bold-Faint-Cyan">(autoscaler +3m53s)</span> [workspace snapshot] New snapshot created successfully (size: 172.52 MB).
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:12:01,852] [INFO] [partition_parameters.py:454:__exit__] finished initializing model with 6.05B parameters
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> Preparing training arguments<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8880, ip=10.0.63.99)</span> Loading model<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8851, ip=10.0.43.240)</span> Model loaded
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.20k/4.20k [00:00&lt;00:00, 22.1MB/s]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(SplitCoordinator pid=8980, ip=10.0.60.59)</span> 2023-08-18 18:09:27.424862: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libnvinfer_plugin.so.7&#39;; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64<span class=" -Color -Color-Green"> [repeated 32x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(SplitCoordinator pid=8980, ip=10.0.60.59)</span> 2023-08-18 18:09:27.424869: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36311, ip=10.0.27.53)</span> comet_ml is installed but `COMET_API_KEY` is not set.<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36262, ip=10.0.52.191)</span> --------------------------------------------------------------------------<span class=" -Color -Color-Green"> [repeated 26x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36262, ip=10.0.52.191)</span>                  Aim collects anonymous usage analytics.                 <span class=" -Color -Color-Green"> [repeated 13x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36262, ip=10.0.52.191)</span>                         Read how to opt-out here:                         <span class=" -Color -Color-Green"> [repeated 13x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36262, ip=10.0.52.191)</span>     https://aimstack.readthedocs.io/en/latest/community/telemetry.html    <span class=" -Color -Color-Green"> [repeated 13x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> max_steps is given, it will override any value given in num_train_epochs
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> Using cuda_amp half precision backend
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:12:36,256] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.2, git-hash=unknown, git-branch=unknown
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:12:36,373] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8858, ip=10.0.0.119)</span> Using /home/ray/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8858, ip=10.0.0.119)</span> Creating extension directory /home/ray/.cache/torch_extensions/py39_cu118/cpu_adam...
Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.20k/4.20k [00:00&lt;00:00, 19.8MB/s]<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8857, ip=10.0.44.114)</span> max_steps is given, it will override any value given in num_train_epochs<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8857, ip=10.0.44.114)</span> Using cuda_amp half precision backend<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=49329)</span> Detected CUDA files, patching ldflags
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=49329)</span> Emitting ninja build file /home/ray/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=49329)</span> Building extension module cpu_adam...
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=49329)</span> Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8858, ip=10.0.0.119)</span> [1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\&quot;_gcc\&quot; -DPYBIND11_STDLIB=\&quot;_libstdcpp\&quot; -DPYBIND11_BUILD_ABI=\&quot;_cxxabi1011\&quot; -I/home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/TH -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ray/anaconda3/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options &#39;-fPIC&#39; -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75 -c /home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8830, ip=10.0.30.35)</span> Model loaded<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=49329)</span> [2/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\&quot;_gcc\&quot; -DPYBIND11_STDLIB=\&quot;_libstdcpp\&quot; -DPYBIND11_BUILD_ABI=\&quot;_cxxabi1011\&quot; -I/home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/TH -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ray/anaconda3/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++14 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -c /home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> [1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\&quot;_gcc\&quot; -DPYBIND11_STDLIB=\&quot;_libstdcpp\&quot; -DPYBIND11_BUILD_ABI=\&quot;_cxxabi1011\&quot; -I/home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/TH -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ray/anaconda3/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options &#39;-fPIC&#39; -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75 -c /home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o <span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=49329)</span> [3/3] c++ cpu_adam.o custom_cuda_kernel.cuda.o -shared -lcurand -L/home/ray/anaconda3/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=49329)</span> Time to load cpu_adam op: 31.202290058135986 seconds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=49329)</span> Loading extension module cpu_adam...
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> Using /home/ray/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> Creating extension directory /home/ray/.cache/torch_extensions/py39_cu118/cpu_adam...<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> Detected CUDA files, patching ldflags<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> Emitting ninja build file /home/ray/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> Building extension module cpu_adam...<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=49329)</span> Adam Optimizer #0 is created with AVX512 arithmetic capability.
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=49329)</span> Config: alpha=0.000020, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=49329)</span> Building extension module utils...
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:13,196] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:13,212] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:13,212] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=&lt;class &#39;deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam&#39;&gt;
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:13,212] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:13,212] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:13,520] [INFO] [utils.py:785:see_memory_usage] Stage 3 initialize beginning
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:13,521] [INFO] [utils.py:786:see_memory_usage] MA 0.11 GB         Max_MA 1.26 GB         CA 1.54 GB         Max_CA 2 GB 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:13,521] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 8.96 GB, percent = 14.4%
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:13,523] [INFO] [stage3.py:113:__init__] Reduce bucket size 16777216
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:13,523] [INFO] [stage3.py:114:__init__] Prefetch bucket size 15099494
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=49329)</span> [1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\&quot;_gcc\&quot; -DPYBIND11_STDLIB=\&quot;_libstdcpp\&quot; -DPYBIND11_BUILD_ABI=\&quot;_cxxabi1011\&quot; -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/TH -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/THC -isystem /home/ray/anaconda3/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> [2/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\&quot;_gcc\&quot; -DPYBIND11_STDLIB=\&quot;_libstdcpp\&quot; -DPYBIND11_BUILD_ABI=\&quot;_cxxabi1011\&quot; -I/home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/TH -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ray/anaconda3/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++14 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -c /home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o <span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> [3/3] c++ cpu_adam.o custom_cuda_kernel.cuda.o -shared -lcurand -L/home/ray/anaconda3/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> Time to load cpu_adam op: 34.29589319229126 seconds<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> Adam Optimizer #0 is created with AVX512 arithmetic capability.<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> Config: alpha=0.000020, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=49329)</span> [2/2] c++ flatten_unflatten.o -shared -L/home/ray/anaconda3/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=49329)</span> Time to load utils op: 15.381849527359009 seconds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=49329)</span> Loading extension module utils...
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> Loading extension module cpu_adam...<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> Using /home/ray/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> Creating extension directory /home/ray/.cache/torch_extensions/py39_cu118/utils...<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> Emitting ninja build file /home/ray/.cache/torch_extensions/py39_cu118/utils/build.ninja...<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> Building extension module utils...<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:29,490] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:29,491] [INFO] [utils.py:786:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 1.54 GB         Max_CA 2 GB 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:29,491] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 8.96 GB, percent = 14.5%
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> Parameter Offload: Total persistent parameters: 811008 in 114 params
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:29,763] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:29,764] [INFO] [utils.py:786:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 1.54 GB         Max_CA 2 GB 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:29,764] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 8.96 GB, percent = 14.5%
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:30,012] [INFO] [utils.py:785:see_memory_usage] Before creating fp16 partitions
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:30,013] [INFO] [utils.py:786:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 1.54 GB         Max_CA 2 GB 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:30,013] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 8.96 GB, percent = 14.5%
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> [1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\&quot;_gcc\&quot; -DPYBIND11_STDLIB=\&quot;_libstdcpp\&quot; -DPYBIND11_BUILD_ABI=\&quot;_cxxabi1011\&quot; -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/TH -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/THC -isystem /home/ray/anaconda3/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o <span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> Loading extension module utils...<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> [2/2] c++ flatten_unflatten.o -shared -L/home/ray/anaconda3/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> Time to load utils op: 16.94431161880493 seconds<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:31,872] [INFO] [utils.py:785:see_memory_usage] After creating fp16 partitions: 1
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:31,873] [INFO] [utils.py:786:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 1.54 GB         Max_CA 2 GB 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:31,873] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 9.98 GB, percent = 16.1%
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:32,120] [INFO] [utils.py:785:see_memory_usage] Before creating fp32 partitions
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:32,121] [INFO] [utils.py:786:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 1.54 GB         Max_CA 2 GB 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:32,121] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 9.98 GB, percent = 16.1%
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:32,624] [INFO] [utils.py:785:see_memory_usage] After creating fp32 partitions
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:32,624] [INFO] [utils.py:786:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 1.54 GB         Max_CA 2 GB 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:32,625] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 11.39 GB, percent = 18.4%
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:32,870] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:32,870] [INFO] [utils.py:786:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 1.54 GB         Max_CA 2 GB 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:32,871] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 11.39 GB, percent = 18.4%
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:34,834] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:34,835] [INFO] [utils.py:786:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 1.54 GB         Max_CA 2 GB 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:34,835] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 16.25 GB, percent = 26.2%
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:34,835] [INFO] [stage3.py:392:_setup_for_real_optimizer] optimizer state initialized
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8830, ip=10.0.30.35)</span> Using /home/ray/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8830, ip=10.0.30.35)</span> No modifications detected for re-loaded extension module utils, skipping build step...
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8830, ip=10.0.30.35)</span> Loading extension module utils...
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=9631, ip=10.0.57.153)</span> Loading extension module utils...
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=9631, ip=10.0.57.153)</span> ***** Running training *****
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=9631, ip=10.0.57.153)</span>   Num examples = 10752
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=9631, ip=10.0.57.153)</span>   Num Epochs = 9223372036854775807
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=9631, ip=10.0.57.153)</span>   Instantaneous batch size per device = 8
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=9631, ip=10.0.57.153)</span>   Total train batch size (w. parallel, distributed &amp; accumulation) = 128
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=9631, ip=10.0.57.153)</span>   Gradient Accumulation steps = 1
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=9631, ip=10.0.57.153)</span>   Total optimization steps = 84
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=9631, ip=10.0.57.153)</span>   Number of trainable parameters = 0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8830, ip=10.0.30.35)</span> Time to load utils op: 0.0005006790161132812 seconds
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=9631, ip=10.0.57.153)</span> Time to load utils op: 0.0005137920379638672 seconds
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,692] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,693] [INFO] [utils.py:786:see_memory_usage] MA 0.14 GB         Max_MA 0.91 GB         CA 1.54 GB         Max_CA 2 GB 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,693] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 17.3 GB, percent = 27.9%
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,694] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,694] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client callable to create LR scheduler
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,694] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = &lt;torch.optim.lr_scheduler.LambdaLR object at 0x7f50b45fbfd0&gt;
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,694] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[2e-05], mom=[[0.9, 0.999]]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,695] [INFO] [config.py:955:print] DeepSpeedEngine configuration:
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,696] [INFO] [config.py:959:print]   activation_checkpointing_config  {
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;partition_activations&quot;: false, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;contiguous_memory_optimization&quot;: false, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;cpu_checkpointing&quot;: false, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;number_checkpoints&quot;: null, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;synchronize_checkpoint_boundary&quot;: false, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;profile&quot;: false
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> }
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,696] [INFO] [config.py:959:print]   aio_config ................... {&#39;block_size&#39;: 1048576, &#39;queue_depth&#39;: 8, &#39;thread_count&#39;: 1, &#39;single_submit&#39;: False, &#39;overlap_events&#39;: True}
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,696] [INFO] [config.py:959:print]   amp_enabled .................. False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,696] [INFO] [config.py:959:print]   amp_params ................... False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,696] [INFO] [config.py:959:print]   autotuning_config ............ {
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;enabled&quot;: false, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;start_step&quot;: null, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;end_step&quot;: null, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;metric_path&quot;: null, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;arg_mappings&quot;: null, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;metric&quot;: &quot;throughput&quot;, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;model_info&quot;: null, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;results_dir&quot;: &quot;autotuning_results&quot;, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;exps_dir&quot;: &quot;autotuning_exps&quot;, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;overwrite&quot;: true, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;fast&quot;: true, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;start_profile_step&quot;: 3, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;end_profile_step&quot;: 5, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;tuner_type&quot;: &quot;gridsearch&quot;, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;tuner_early_stopping&quot;: 5, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;tuner_num_trials&quot;: 50, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;model_info_path&quot;: null, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;mp_size&quot;: 1, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;max_train_batch_size&quot;: null, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;min_train_batch_size&quot;: 1, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;max_train_micro_batch_size_per_gpu&quot;: 1.024000e+03, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;min_train_micro_batch_size_per_gpu&quot;: 1, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;num_tuning_micro_batch_sizes&quot;: 3
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> }
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,696] [INFO] [config.py:959:print]   bfloat16_enabled ............. False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,696] [INFO] [config.py:959:print]   checkpoint_parallel_write_pipeline  False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,696] [INFO] [config.py:959:print]   checkpoint_tag_validation_enabled  True
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,696] [INFO] [config.py:959:print]   checkpoint_tag_validation_fail  False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,696] [INFO] [config.py:959:print]   comms_config ................. &lt;deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f50c6da2370&gt;
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,696] [INFO] [config.py:959:print]   communication_data_type ...... None
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,696] [INFO] [config.py:959:print]   compression_config ........... {&#39;weight_quantization&#39;: {&#39;shared_parameters&#39;: {&#39;enabled&#39;: False, &#39;quantizer_kernel&#39;: False, &#39;schedule_offset&#39;: 0, &#39;quantize_groups&#39;: 1, &#39;quantize_verbose&#39;: False, &#39;quantization_type&#39;: &#39;symmetric&#39;, &#39;quantize_weight_in_forward&#39;: False, &#39;rounding&#39;: &#39;nearest&#39;, &#39;fp16_mixed_quantize&#39;: False, &#39;quantize_change_ratio&#39;: 0.001}, &#39;different_groups&#39;: {}}, &#39;activation_quantization&#39;: {&#39;shared_parameters&#39;: {&#39;enabled&#39;: False, &#39;quantization_type&#39;: &#39;symmetric&#39;, &#39;range_calibration&#39;: &#39;dynamic&#39;, &#39;schedule_offset&#39;: 1000}, &#39;different_groups&#39;: {}}, &#39;sparse_pruning&#39;: {&#39;shared_parameters&#39;: {&#39;enabled&#39;: False, &#39;method&#39;: &#39;l1&#39;, &#39;schedule_offset&#39;: 1000}, &#39;different_groups&#39;: {}}, &#39;row_pruning&#39;: {&#39;shared_parameters&#39;: {&#39;enabled&#39;: False, &#39;method&#39;: &#39;l1&#39;, &#39;schedule_offset&#39;: 1000}, &#39;different_groups&#39;: {}}, &#39;head_pruning&#39;: {&#39;shared_parameters&#39;: {&#39;enabled&#39;: False, &#39;method&#39;: &#39;topk&#39;, &#39;schedule_offset&#39;: 1000}, &#39;different_groups&#39;: {}}, &#39;channel_pruning&#39;: {&#39;shared_parameters&#39;: {&#39;enabled&#39;: False, &#39;method&#39;: &#39;l1&#39;, &#39;schedule_offset&#39;: 1000}, &#39;different_groups&#39;: {}}, &#39;layer_reduction&#39;: {&#39;enabled&#39;: False}}
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,696] [INFO] [config.py:959:print]   curriculum_enabled_legacy .... False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,696] [INFO] [config.py:959:print]   curriculum_params_legacy ..... False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,696] [INFO] [config.py:959:print]   data_efficiency_config ....... {&#39;enabled&#39;: False, &#39;seed&#39;: 1234, &#39;data_sampling&#39;: {&#39;enabled&#39;: False, &#39;num_epochs&#39;: 1000, &#39;num_workers&#39;: 0, &#39;curriculum_learning&#39;: {&#39;enabled&#39;: False}}, &#39;data_routing&#39;: {&#39;enabled&#39;: False, &#39;random_ltd&#39;: {&#39;enabled&#39;: False, &#39;layer_token_lr_schedule&#39;: {&#39;enabled&#39;: False}}}}
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,696] [INFO] [config.py:959:print]   data_efficiency_enabled ...... False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,696] [INFO] [config.py:959:print]   dataloader_drop_last ......... False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   disable_allgather ............ False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   dump_state ................... False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   dynamic_loss_scale_args ...... {&#39;init_scale&#39;: 256, &#39;scale_window&#39;: 1000, &#39;delayed_shift&#39;: 2, &#39;min_scale&#39;: 1}
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   eigenvalue_enabled ........... False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   eigenvalue_gas_boundary_resolution  1
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   eigenvalue_layer_name ........ bert.encoder.layer
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   eigenvalue_layer_num ......... 0
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   eigenvalue_max_iter .......... 100
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   eigenvalue_stability ......... 1e-06
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   eigenvalue_tol ............... 0.01
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   eigenvalue_verbose ........... False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   elasticity_enabled ........... False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   flops_profiler_config ........ {
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;enabled&quot;: false, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;profile_step&quot;: 1, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;module_depth&quot;: -1, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;top_modules&quot;: 1, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;detailed&quot;: true, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;output_file&quot;: null
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> }
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   fp16_auto_cast ............... False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   fp16_enabled ................. True
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   fp16_master_weights_and_gradients  False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   global_rank .................. 0
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   grad_accum_dtype ............. None
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   gradient_accumulation_steps .. 1
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   gradient_clipping ............ 1.0
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   gradient_predivide_factor .... 1.0
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   initial_dynamic_scale ........ 256
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   load_universal_checkpoint .... False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   loss_scale ................... 0
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   memory_breakdown ............. False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   mics_hierarchial_params_gather  False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   mics_shard_size .............. -1
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path=&#39;&#39;, job_name=&#39;DeepSpeedJobName&#39;) wandb=WandbConfig(enabled=False, group=None, team=None, project=&#39;deepspeed&#39;) csv_monitor=CSVConfig(enabled=False, output_path=&#39;&#39;, job_name=&#39;DeepSpeedJobName&#39;) enabled=False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   nebula_config ................ {
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;enabled&quot;: false, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;persistent_storage_path&quot;: null, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;persistent_time_interval&quot;: 100, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;num_of_version_in_retention&quot;: 2, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;enable_nebula_load&quot;: true, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;load_path&quot;: null
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> }
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   optimizer_legacy_fusion ...... False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   optimizer_name ............... adamw
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   optimizer_params ............. {&#39;lr&#39;: 2e-05, &#39;betas&#39;: [0.9, 0.999], &#39;eps&#39;: 1e-08}
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,697] [INFO] [config.py:959:print]   pipeline ..................... {&#39;stages&#39;: &#39;auto&#39;, &#39;partition&#39;: &#39;best&#39;, &#39;seed_layers&#39;: False, &#39;activation_checkpoint_interval&#39;: 0}
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,698] [INFO] [config.py:959:print]   pld_enabled .................. False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,698] [INFO] [config.py:959:print]   pld_params ................... False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,698] [INFO] [config.py:959:print]   prescale_gradients ........... False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,698] [INFO] [config.py:959:print]   scheduler_name ............... None
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,698] [INFO] [config.py:959:print]   scheduler_params ............. None
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,698] [INFO] [config.py:959:print]   sparse_attention ............. None
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,698] [INFO] [config.py:959:print]   sparse_gradients_enabled ..... False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,698] [INFO] [config.py:959:print]   steps_per_print .............. 10
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,698] [INFO] [config.py:959:print]   train_batch_size ............. 128
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,698] [INFO] [config.py:959:print]   train_micro_batch_size_per_gpu  8
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,698] [INFO] [config.py:959:print]   use_node_local_storage ....... False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,698] [INFO] [config.py:959:print]   wall_clock_breakdown ......... False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,698] [INFO] [config.py:959:print]   world_size ................... 16
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,698] [INFO] [config.py:959:print]   zero_allow_untested_optimizer  False
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,698] [INFO] [config.py:959:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=16777216 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device=&#39;cpu&#39;, nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device=&#39;cpu&#39;, nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=15099494 param_persistence_threshold=40960 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=True mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,698] [INFO] [config.py:959:print]   zero_enabled ................. True
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,698] [INFO] [config.py:959:print]   zero_force_ds_cpu_optimizer .. True
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,698] [INFO] [config.py:959:print]   zero_optimization_stage ...... 3
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:13:40,698] [INFO] [config.py:945:print_user_config]   json = {
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;fp16&quot;: {
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>         &quot;enabled&quot;: true, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>         &quot;initial_scale_power&quot;: 8
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     }, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;bf16&quot;: {
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>         &quot;enabled&quot;: false
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     }, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;optimizer&quot;: {
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>         &quot;type&quot;: &quot;AdamW&quot;, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>         &quot;params&quot;: {
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>             &quot;lr&quot;: 2e-05, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>             &quot;betas&quot;: [0.9, 0.999], 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>             &quot;eps&quot;: 1e-08
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>         }
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     }, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;zero_optimization&quot;: {
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>         &quot;stage&quot;: 3, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>         &quot;offload_optimizer&quot;: {
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>             &quot;device&quot;: &quot;cpu&quot;, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>             &quot;pin_memory&quot;: true
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>         }, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>         &quot;offload_param&quot;: {
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>             &quot;device&quot;: &quot;cpu&quot;, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>             &quot;pin_memory&quot;: true
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>         }, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>         &quot;overlap_comm&quot;: true, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>         &quot;contiguous_gradients&quot;: true, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>         &quot;reduce_bucket_size&quot;: 1.677722e+07, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>         &quot;stage3_prefetch_bucket_size&quot;: 1.509949e+07, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>         &quot;stage3_param_persistence_threshold&quot;: 4.096000e+04, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>         &quot;gather_16bit_weights_on_model_save&quot;: true, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>         &quot;round_robin_gradients&quot;: true
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     }, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;gradient_accumulation_steps&quot;: 1, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;gradient_clipping&quot;: 1.0, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;steps_per_print&quot;: 10, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;train_batch_size&quot;: 128, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;train_micro_batch_size_per_gpu&quot;: 8, 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>     &quot;wall_clock_breakdown&quot;: false
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> }
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(SplitCoordinator pid=8980, ip=10.0.60.59)</span> Executing DAG InputDataBuffer[Input] -&gt; TaskPoolMapOperator[MapBatches(split_text)-&gt;MapBatches(tokenize)] -&gt; OutputSplitter[split(16, equal=True)]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(SplitCoordinator pid=8980, ip=10.0.60.59)</span> Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=2000000000.0), locality_with_output=[&#39;6002ded0aaa53ce9a0351d22a72b344ef411a422919132f41d9f937a&#39;, &#39;d3bbd390b6fe73f26202f96d75998946cf3e8b457528d426db0c6e07&#39;, &#39;fe6aaf54317ee630a02d23e0d49581b57b5cd51316eaf769e28bb045&#39;, &#39;f7de4694a4f764c05a9c51a6a4bd40ac33f3fced3b25127b25cd4ac3&#39;, &#39;42866a2fba4ce2ab4b6645c4d731d486b762e2b23ac24cafccba7096&#39;, &#39;8a7272830662c7e756a656de0a9b433a3a1f9b990768f692b6fe11a7&#39;, &#39;bba62e8b57552509c62a6b6b7fd67c1a2280b9d81b3d9c41eb4d1b9b&#39;, &#39;b40764f303538c24bc439106f2e7b2144d382bfed6c9fdec15ab828e&#39;, &#39;d1de4d4b6d44eff93857026df4ef0f70e24e3dc91e15d87015f2ed32&#39;, &#39;4d6a9dc1aa7bfc80cb73d9f66f4e28041807f12769391f5643bce143&#39;, &#39;8bcc7235f459b61be21fe158d0bae4fef2ec6de013ec60e7aaf7897a&#39;, &#39;73c50b995811afa0ece70fd3d4466b7fd0dc85a97d6807128b2c47da&#39;, &#39;03bf3d374a9f857b1cd1aebdbe028208f7904b077fb151790e03e9fe&#39;, &#39;9f7fc101a7d6b3e17b72e57ca1c92f91d13aa385a6740f99d58ec016&#39;, &#39;867844d104a8e9351a1dcc8bbd61d99906a8dc5b53e220c2ae2efbe1&#39;, &#39;7677b344c59d6b30c3db451f48e346d61bb60cc798e5567aa4e0a1ea&#39;], preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(SplitCoordinator pid=8980, ip=10.0.60.59)</span> Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(MapBatches(split_text)-&gt;MapBatches(tokenize) pid=10097, ip=10.0.60.59)</span> 2023-08-18 18:13:42.547741: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(MapBatches(split_text)-&gt;MapBatches(tokenize) pid=10097, ip=10.0.60.59)</span> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(MapBatches(split_text)-&gt;MapBatches(tokenize) pid=10097, ip=10.0.60.59)</span> 2023-08-18 18:13:42.685843: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(MapBatches(split_text)-&gt;MapBatches(tokenize) pid=10097, ip=10.0.60.59)</span> 2023-08-18 18:13:43.506819: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libnvinfer.so.7&#39;; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(MapBatches(split_text)-&gt;MapBatches(tokenize) pid=10097, ip=10.0.60.59)</span> 2023-08-18 18:13:43.506880: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libnvinfer_plugin.so.7&#39;; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(MapBatches(split_text)-&gt;MapBatches(tokenize) pid=10097, ip=10.0.60.59)</span> 2023-08-18 18:13:43.506887: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> Time to load utils op: 0.0003864765167236328 seconds<span class=" -Color -Color-Green"> [repeated 14x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8851, ip=10.0.43.240)</span> {&#39;loss&#39;: 12.1235, &#39;learning_rate&#39;: 1.9761904761904763e-05, &#39;epoch&#39;: 0.01}
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8851, ip=10.0.43.240)</span> {&#39;loss&#39;: 6.7834, &#39;learning_rate&#39;: 1.9523809523809524e-05, &#39;epoch&#39;: 0.02}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8857, ip=10.0.44.114)</span> {&#39;loss&#39;: 2.2151, &#39;learning_rate&#39;: 1.928571428571429e-05, &#39;epoch&#39;: 0.04}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=9631, ip=10.0.57.153)</span> {&#39;loss&#39;: 0.1739, &#39;learning_rate&#39;: 1.904761904761905e-05, &#39;epoch&#39;: 0.05}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Bold -Color-Bold-Faint -Color-Bold-Faint-Cyan">(autoscaler +8m53s)</span> [workspace snapshot] New snapshot created successfully (size: 172.58 MB).
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8858, ip=10.0.0.119)</span> {&#39;loss&#39;: 0.121, &#39;learning_rate&#39;: 1.880952380952381e-05, &#39;epoch&#39;: 0.06}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> {&#39;loss&#39;: 0.1422, &#39;learning_rate&#39;: 1.8571428571428575e-05, &#39;epoch&#39;: 0.07}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36249, ip=10.0.11.26)</span> {&#39;loss&#39;: 0.1007, &#39;learning_rate&#39;: 1.8333333333333333e-05, &#39;epoch&#39;: 0.08}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8867, ip=10.0.49.236)</span> {&#39;loss&#39;: 0.1082, &#39;learning_rate&#39;: 1.8095238095238097e-05, &#39;epoch&#39;: 0.1}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8858, ip=10.0.0.119)</span> {&#39;loss&#39;: 0.094, &#39;learning_rate&#39;: 1.785714285714286e-05, &#39;epoch&#39;: 0.11}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8880, ip=10.0.63.99)</span> {&#39;loss&#39;: 0.0936, &#39;learning_rate&#39;: 1.761904761904762e-05, &#39;epoch&#39;: 0.12}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:18:36,553] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[1.761904761904762e-05], mom=[[0.9, 0.999]]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:18:36,554] [INFO] [timer.py:199:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=4.768458258762969, CurrSamplesPerSec=4.833942877725304, MemAllocated=0.16GB, MaxMemAllocated=8.93GB
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8857, ip=10.0.44.114)</span> {&#39;loss&#39;: 0.0921, &#39;learning_rate&#39;: 1.7380952380952384e-05, &#39;epoch&#39;: 0.13}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8851, ip=10.0.43.240)</span> {&#39;loss&#39;: 0.0915, &#39;learning_rate&#39;: 1.7142857142857142e-05, &#39;epoch&#39;: 0.14}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=49329)</span> {&#39;loss&#39;: 0.0883, &#39;learning_rate&#39;: 1.6904761904761906e-05, &#39;epoch&#39;: 0.15}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36311, ip=10.0.27.53)</span> {&#39;loss&#39;: 0.0868, &#39;learning_rate&#39;: 1.6666666666666667e-05, &#39;epoch&#39;: 0.17}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> {&#39;loss&#39;: 0.0815, &#39;learning_rate&#39;: 1.642857142857143e-05, &#39;epoch&#39;: 0.18}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Bold -Color-Bold-Faint -Color-Bold-Faint-Cyan">(autoscaler +13m58s)</span> [workspace snapshot] New snapshot created successfully (size: 172.58 MB).
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8875, ip=10.0.0.80)</span> {&#39;loss&#39;: 0.0825, &#39;learning_rate&#39;: 1.6190476190476193e-05, &#39;epoch&#39;: 0.19}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=49329)</span> {&#39;loss&#39;: 0.0813, &#39;learning_rate&#39;: 1.5952380952380954e-05, &#39;epoch&#39;: 0.2}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8880, ip=10.0.63.99)</span> {&#39;loss&#39;: 0.0816, &#39;learning_rate&#39;: 1.5714285714285715e-05, &#39;epoch&#39;: 0.21}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8851, ip=10.0.43.240)</span> {&#39;loss&#39;: 0.0813, &#39;learning_rate&#39;: 1.5476190476190476e-05, &#39;epoch&#39;: 0.23}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8885, ip=10.0.47.209)</span> {&#39;loss&#39;: 0.0765, &#39;learning_rate&#39;: 1.523809523809524e-05, &#39;epoch&#39;: 0.24}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:23:03,756] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[1.523809523809524e-05], mom=[[0.9, 0.999]]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:23:03,756] [INFO] [timer.py:199:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=4.781402482813706, CurrSamplesPerSec=4.7832870646183325, MemAllocated=0.16GB, MaxMemAllocated=8.93GB
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8858, ip=10.0.0.119)</span> {&#39;loss&#39;: 0.0833, &#39;learning_rate&#39;: 1.5000000000000002e-05, &#39;epoch&#39;: 0.25}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8857, ip=10.0.44.114)</span> {&#39;loss&#39;: 0.084, &#39;learning_rate&#39;: 1.4761904761904763e-05, &#39;epoch&#39;: 0.26}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=49329)</span> {&#39;loss&#39;: 0.0839, &#39;learning_rate&#39;: 1.4523809523809524e-05, &#39;epoch&#39;: 0.27}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8867, ip=10.0.49.236)</span> {&#39;loss&#39;: 0.0825, &#39;learning_rate&#39;: 1.4285714285714287e-05, &#39;epoch&#39;: 0.29}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36262, ip=10.0.52.191)</span> {&#39;loss&#39;: 0.0838, &#39;learning_rate&#39;: 1.4047619047619048e-05, &#39;epoch&#39;: 0.3}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8867, ip=10.0.49.236)</span> {&#39;loss&#39;: 0.0847, &#39;learning_rate&#39;: 1.3809523809523811e-05, &#39;epoch&#39;: 0.31}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Bold -Color-Bold-Faint -Color-Bold-Faint-Cyan">(autoscaler +18m58s)</span> [workspace snapshot] New snapshot created successfully (size: 172.58 MB).
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8851, ip=10.0.43.240)</span> {&#39;loss&#39;: 0.0788, &#39;learning_rate&#39;: 1.3571428571428574e-05, &#39;epoch&#39;: 0.32}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8875, ip=10.0.0.80)</span> {&#39;loss&#39;: 0.0832, &#39;learning_rate&#39;: 1.3333333333333333e-05, &#39;epoch&#39;: 0.33}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8875, ip=10.0.0.80)</span> {&#39;loss&#39;: 0.0811, &#39;learning_rate&#39;: 1.3095238095238096e-05, &#39;epoch&#39;: 0.35}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> {&#39;loss&#39;: 0.0759, &#39;learning_rate&#39;: 1.2857142857142859e-05, &#39;epoch&#39;: 0.36}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:27:35,516] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[1.2857142857142859e-05], mom=[[0.9, 0.999]]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:27:35,517] [INFO] [timer.py:199:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=4.756191577689035, CurrSamplesPerSec=4.775146730091594, MemAllocated=0.16GB, MaxMemAllocated=8.93GB
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8875, ip=10.0.0.80)</span> {&#39;loss&#39;: 0.0774, &#39;learning_rate&#39;: 1.261904761904762e-05, &#39;epoch&#39;: 0.37}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8858, ip=10.0.0.119)</span> {&#39;loss&#39;: 0.0751, &#39;learning_rate&#39;: 1.2380952380952383e-05, &#39;epoch&#39;: 0.38}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36311, ip=10.0.27.53)</span> {&#39;loss&#39;: 0.0744, &#39;learning_rate&#39;: 1.2142857142857142e-05, &#39;epoch&#39;: 0.39}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8845, ip=10.0.18.195)</span> {&#39;loss&#39;: 0.0722, &#39;learning_rate&#39;: 1.1904761904761905e-05, &#39;epoch&#39;: 0.4}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8880, ip=10.0.63.99)</span> {&#39;loss&#39;: 0.0742, &#39;learning_rate&#39;: 1.1666666666666668e-05, &#39;epoch&#39;: 0.42}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8857, ip=10.0.44.114)</span> {&#39;loss&#39;: 0.0764, &#39;learning_rate&#39;: 1.1428571428571429e-05, &#39;epoch&#39;: 0.43}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8851, ip=10.0.43.240)</span> {&#39;loss&#39;: 0.0786, &#39;learning_rate&#39;: 1.1190476190476192e-05, &#39;epoch&#39;: 0.44}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Bold -Color-Bold-Faint -Color-Bold-Faint-Cyan">(autoscaler +24m4s)</span> [workspace snapshot] New snapshot created successfully (size: 172.58 MB).
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=9631, ip=10.0.57.153)</span> {&#39;loss&#39;: 0.0738, &#39;learning_rate&#39;: 1.0952380952380955e-05, &#39;epoch&#39;: 0.45}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> {&#39;loss&#39;: 0.0784, &#39;learning_rate&#39;: 1.0714285714285714e-05, &#39;epoch&#39;: 0.46}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8885, ip=10.0.47.209)</span> {&#39;loss&#39;: 0.0786, &#39;learning_rate&#39;: 1.0476190476190477e-05, &#39;epoch&#39;: 0.48}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:32:06,009] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[1.0476190476190477e-05], mom=[[0.9, 0.999]]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:32:06,009] [INFO] [timer.py:199:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=4.750214082000028, CurrSamplesPerSec=4.781755388354574, MemAllocated=0.16GB, MaxMemAllocated=8.93GB
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8845, ip=10.0.18.195)</span> {&#39;loss&#39;: 0.0714, &#39;learning_rate&#39;: 1.0238095238095238e-05, &#39;epoch&#39;: 0.49}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36311, ip=10.0.27.53)</span> {&#39;loss&#39;: 0.0739, &#39;learning_rate&#39;: 1e-05, &#39;epoch&#39;: 0.5}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8851, ip=10.0.43.240)</span> {&#39;loss&#39;: 0.0767, &#39;learning_rate&#39;: 9.761904761904762e-06, &#39;epoch&#39;: 0.51}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> {&#39;loss&#39;: 0.0827, &#39;learning_rate&#39;: 9.523809523809525e-06, &#39;epoch&#39;: 0.52}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36262, ip=10.0.52.191)</span> {&#39;loss&#39;: 0.0751, &#39;learning_rate&#39;: 9.285714285714288e-06, &#39;epoch&#39;: 0.54}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> {&#39;loss&#39;: 0.0737, &#39;learning_rate&#39;: 9.047619047619049e-06, &#39;epoch&#39;: 0.55}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8885, ip=10.0.47.209)</span> {&#39;loss&#39;: 0.0755, &#39;learning_rate&#39;: 8.80952380952381e-06, &#39;epoch&#39;: 0.56}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=49329)</span> {&#39;loss&#39;: 0.0745, &#39;learning_rate&#39;: 8.571428571428571e-06, &#39;epoch&#39;: 0.57}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=9631, ip=10.0.57.153)</span> {&#39;loss&#39;: 0.0753, &#39;learning_rate&#39;: 8.333333333333334e-06, &#39;epoch&#39;: 0.58}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Bold -Color-Bold-Faint -Color-Bold-Faint-Cyan">(autoscaler +29m9s)</span> [workspace snapshot] New snapshot created successfully (size: 172.59 MB).
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8845, ip=10.0.18.195)</span> {&#39;loss&#39;: 0.0739, &#39;learning_rate&#39;: 8.095238095238097e-06, &#39;epoch&#39;: 0.6}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:36:34,033] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[8.095238095238097e-06], mom=[[0.9, 0.999]]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:36:34,033] [INFO] [timer.py:199:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=4.75579745222066, CurrSamplesPerSec=4.705258125568294, MemAllocated=0.16GB, MaxMemAllocated=8.93GB
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8885, ip=10.0.47.209)</span> {&#39;loss&#39;: 0.073, &#39;learning_rate&#39;: 7.857142857142858e-06, &#39;epoch&#39;: 0.61}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8830, ip=10.0.30.35)</span> {&#39;loss&#39;: 0.0721, &#39;learning_rate&#39;: 7.61904761904762e-06, &#39;epoch&#39;: 0.62}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36311, ip=10.0.27.53)</span> {&#39;loss&#39;: 0.0729, &#39;learning_rate&#39;: 7.380952380952382e-06, &#39;epoch&#39;: 0.63}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8880, ip=10.0.63.99)</span> {&#39;loss&#39;: 0.0714, &#39;learning_rate&#39;: 7.1428571428571436e-06, &#39;epoch&#39;: 0.64}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> {&#39;loss&#39;: 0.0745, &#39;learning_rate&#39;: 6.9047619047619055e-06, &#39;epoch&#39;: 0.65}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=9631, ip=10.0.57.153)</span> {&#39;loss&#39;: 0.0726, &#39;learning_rate&#39;: 6.666666666666667e-06, &#39;epoch&#39;: 0.67}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> {&#39;loss&#39;: 0.0699, &#39;learning_rate&#39;: 6.4285714285714295e-06, &#39;epoch&#39;: 0.68}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36262, ip=10.0.52.191)</span> {&#39;loss&#39;: 0.0732, &#39;learning_rate&#39;: 6.1904761904761914e-06, &#39;epoch&#39;: 0.69}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=49329)</span> {&#39;loss&#39;: 0.0714, &#39;learning_rate&#39;: 5.9523809523809525e-06, &#39;epoch&#39;: 0.7}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8851, ip=10.0.43.240)</span> {&#39;loss&#39;: 0.0709, &#39;learning_rate&#39;: 5.7142857142857145e-06, &#39;epoch&#39;: 0.71}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:41:07,338] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[5.7142857142857145e-06], mom=[[0.9, 0.999]]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:41:07,338] [INFO] [timer.py:199:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=4.74341422313603, CurrSamplesPerSec=4.640637786972311, MemAllocated=0.16GB, MaxMemAllocated=8.93GB
<span class=" -Color -Color-Bold -Color-Bold-Faint -Color-Bold-Faint-Cyan">(autoscaler +34m9s)</span> [workspace snapshot] New snapshot created successfully (size: 172.59 MB).
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8875, ip=10.0.0.80)</span> {&#39;loss&#39;: 0.071, &#39;learning_rate&#39;: 5.476190476190477e-06, &#39;epoch&#39;: 0.73}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36311, ip=10.0.27.53)</span> {&#39;loss&#39;: 0.0714, &#39;learning_rate&#39;: 5.2380952380952384e-06, &#39;epoch&#39;: 0.74}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8875, ip=10.0.0.80)</span> {&#39;loss&#39;: 0.0703, &#39;learning_rate&#39;: 5e-06, &#39;epoch&#39;: 0.75}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> {&#39;loss&#39;: 0.0733, &#39;learning_rate&#39;: 4.761904761904762e-06, &#39;epoch&#39;: 0.76}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8845, ip=10.0.18.195)</span> {&#39;loss&#39;: 0.0686, &#39;learning_rate&#39;: 4.523809523809524e-06, &#39;epoch&#39;: 0.77}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8851, ip=10.0.43.240)</span> {&#39;loss&#39;: 0.068, &#39;learning_rate&#39;: 4.2857142857142855e-06, &#39;epoch&#39;: 0.79}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36311, ip=10.0.27.53)</span> {&#39;loss&#39;: 0.071, &#39;learning_rate&#39;: 4.047619047619048e-06, &#39;epoch&#39;: 0.8}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> {&#39;loss&#39;: 0.0708, &#39;learning_rate&#39;: 3.80952380952381e-06, &#39;epoch&#39;: 0.81}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36311, ip=10.0.27.53)</span> {&#39;loss&#39;: 0.0766, &#39;learning_rate&#39;: 3.5714285714285718e-06, &#39;epoch&#39;: 0.82}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8858, ip=10.0.0.119)</span> {&#39;loss&#39;: 0.0743, &#39;learning_rate&#39;: 3.3333333333333333e-06, &#39;epoch&#39;: 0.83}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:45:31,965] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[3.3333333333333333e-06], mom=[[0.9, 0.999]]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:45:31,965] [INFO] [timer.py:199:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=4.757168325507401, CurrSamplesPerSec=4.8146031804109555, MemAllocated=0.16GB, MaxMemAllocated=8.93GB
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8830, ip=10.0.30.35)</span> {&#39;loss&#39;: 0.0752, &#39;learning_rate&#39;: 3.3333333333333333e-06, &#39;epoch&#39;: 0.85}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:45:58,184] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 256, but hysteresis is 2. Reducing hysteresis to 1
<span class=" -Color -Color-Bold -Color-Bold-Faint -Color-Bold-Faint-Cyan">(autoscaler +39m14s)</span> [workspace snapshot] New snapshot created successfully (size: 172.59 MB).
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8845, ip=10.0.18.195)</span> {&#39;loss&#39;: 0.0717, &#39;learning_rate&#39;: 3.0952380952380957e-06, &#39;epoch&#39;: 0.86}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:46:26,433] [WARNING] [stage3.py:1826:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> {&#39;loss&#39;: 0.0695, &#39;learning_rate&#39;: 2.8571428571428573e-06, &#39;epoch&#39;: 0.87}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36249, ip=10.0.11.26)</span> {&#39;loss&#39;: 0.0709, &#39;learning_rate&#39;: 2.6190476190476192e-06, &#39;epoch&#39;: 0.88}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8885, ip=10.0.47.209)</span> {&#39;loss&#39;: 0.0729, &#39;learning_rate&#39;: 2.380952380952381e-06, &#39;epoch&#39;: 0.89}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8880, ip=10.0.63.99)</span> {&#39;loss&#39;: 0.0752, &#39;learning_rate&#39;: 2.1428571428571427e-06, &#39;epoch&#39;: 0.9}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8845, ip=10.0.18.195)</span> {&#39;loss&#39;: 0.0712, &#39;learning_rate&#39;: 1.904761904761905e-06, &#39;epoch&#39;: 0.92}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=9631, ip=10.0.57.153)</span> {&#39;loss&#39;: 0.0708, &#39;learning_rate&#39;: 1.6666666666666667e-06, &#39;epoch&#39;: 0.93}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36249, ip=10.0.11.26)</span> {&#39;loss&#39;: 0.0723, &#39;learning_rate&#39;: 1.4285714285714286e-06, &#39;epoch&#39;: 0.94}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8845, ip=10.0.18.195)</span> {&#39;loss&#39;: 0.0689, &#39;learning_rate&#39;: 1.1904761904761906e-06, &#39;epoch&#39;: 0.95}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:50:01,494] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=1, lr=[1.1904761904761906e-06], mom=[[0.9, 0.999]]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:50:01,494] [INFO] [timer.py:199:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=4.756310378443122, CurrSamplesPerSec=4.758170892979721, MemAllocated=0.16GB, MaxMemAllocated=8.93GB
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> {&#39;loss&#39;: 0.0715, &#39;learning_rate&#39;: 9.523809523809525e-07, &#39;epoch&#39;: 0.96}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8880, ip=10.0.63.99)</span> {&#39;loss&#39;: 0.07, &#39;learning_rate&#39;: 7.142857142857143e-07, &#39;epoch&#39;: 0.98}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=9631, ip=10.0.57.153)</span> {&#39;loss&#39;: 0.0716, &#39;learning_rate&#39;: 4.7619047619047623e-07, &#39;epoch&#39;: 0.99}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
<span class=" -Color -Color-Bold -Color-Bold-Faint -Color-Bold-Faint-Cyan">(autoscaler +44m19s)</span> [workspace snapshot] New snapshot created successfully (size: 172.60 MB).
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8880, ip=10.0.63.99)</span> {&#39;loss&#39;: 0.069, &#39;learning_rate&#39;: 2.3809523809523811e-07, &#39;epoch&#39;: 1.0}<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> Saving model checkpoint to output/checkpoint-84
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> Configuration saved in output/checkpoint-84/config.json
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> Configuration saved in output/checkpoint-84/generation_config.json
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> Using /home/ray/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> No modifications detected for re-loaded extension module utils, skipping build step...<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> Loading extension module utils...<span class=" -Color -Color-Green"> [repeated 14x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> ***** Running training *****<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>   Num examples = 10752<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>   Num Epochs = 9223372036854775807<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>   Instantaneous batch size per device = 8<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>   Total train batch size (w. parallel, distributed &amp; accumulation) = 128<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>   Gradient Accumulation steps = 1<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>   Total optimization steps = 84<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span>   Number of trainable parameters = 0<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> Model weights saved in output/checkpoint-84/pytorch_model.bin
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> tokenizer config file saved in output/checkpoint-84/tokenizer_config.json
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> Special tokens file saved in output/checkpoint-84/special_tokens_map.json
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=49329)</span> [2023-08-18 18:52:12,213] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step84 is ready now!
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36249, ip=10.0.11.26)</span> {&#39;loss&#39;: 0.069, &#39;learning_rate&#39;: 2.3809523809523811e-07, &#39;epoch&#39;: 1.0}<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:52:12,213] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step84 is about to be saved!
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:52:12,213] [INFO] [engine.py:3337:save_16bit_model] Saving model weights to output/checkpoint-84/pytorch_model.bin, tag: global_step84
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:52:12,213] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/checkpoint-84/pytorch_model.bin...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=49329)</span> /home/ray/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=49329)</span>   warnings.warn(
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:52:27,660] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/checkpoint-84/pytorch_model.bin.
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:52:27,673] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step84 is about to be saved!
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:52:27,684] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output/checkpoint-84/global_step84/zero_pp_rank_0_mp_rank_00_model_states.pt
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:52:27,685] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/checkpoint-84/global_step84/zero_pp_rank_0_mp_rank_00_model_states.pt...
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:52:27,660] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step84 is ready now!<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36262, ip=10.0.52.191)</span> [2023-08-18 18:52:27,685] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/checkpoint-84/global_step84/zero_pp_rank_15_mp_rank_00_model_states.pt...
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=9631, ip=10.0.57.153)</span> [2023-08-18 18:52:32,337] [INFO] [engine.py:3228:_save_zero_checkpoint] zero checkpoint saved output/checkpoint-84/global_step84/zero_pp_rank_14_mp_rank_00_optim_states.pt
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:52:36,011] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/checkpoint-84/global_step84/zero_pp_rank_0_mp_rank_00_optim_states.pt.<span class=" -Color -Color-Green"> [repeated 32x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36675, ip=10.0.13.222)</span> [2023-08-18 18:52:27,684] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: output/checkpoint-84/global_step84/zero_pp_rank_1_mp_rank_00_model_states.pt
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8867, ip=10.0.49.236)</span> [2023-08-18 18:52:27,873] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/checkpoint-84/global_step84/zero_pp_rank_3_mp_rank_00_optim_states.pt...<span class=" -Color -Color-Green"> [repeated 30x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36311, ip=10.0.27.53)</span> [2023-08-18 18:52:36,193] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step84 is ready now!
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8885, ip=10.0.47.209)</span> 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8885, ip=10.0.47.209)</span> 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8885, ip=10.0.47.209)</span> Training completed. Do not forget to share your model on huggingface.co/models =)
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8885, ip=10.0.47.209)</span> 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8885, ip=10.0.47.209)</span> 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8867, ip=10.0.49.236)</span> /home/ray/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8867, ip=10.0.49.236)</span>   warnings.warn(<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
2023-08-18 18:53:44,782	WARNING syncer.py:853 -- Ray AIR no longer supports the synchronization of checkpoints and other artifacts from worker nodes to the head node. This means that the checkpoints and artifacts saved by trials scheduled on worker nodes will not be accessible during the run (e.g., resuming from a checkpoint after a failure) or after the run (e.g., loading the checkpoint of a trial that ran on an already terminated worker node).

To fix this issue, configure AIR to use either:
(1) Cloud storage: `RunConfig(storage_path=&#39;s3://your/bucket&#39;)`
(2) A network filesystem mounted on all nodes: `RunConfig(storage_path=&#39;/mnt/path/to/nfs_storage&#39;)`
See this Github issue for more details on transitioning to cloud storage/NFS as well as an explanation on why this functionality is being removed: https://github.com/ray-project/ray/issues/37177
If you are already using NFS, you can ignore this warning message.

Other temporary workarounds:
- If you want to avoid errors/warnings and continue running with syncing explicitly turned off, set `RunConfig(SyncConfig(syncer=None))`
- Or, to re-enable the head node syncing behavior, set the environment variable RAY_AIR_REENABLE_DEPRECATED_SYNC_TO_HEAD_NODE=1
  - **Note that this functionality will tentatively be hard-deprecated in Ray 2.7.** See the linked issue for the latest information.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=36262, ip=10.0.52.191)</span> {&#39;train_runtime&#39;: 2355.3551, &#39;train_samples_per_second&#39;: 4.565, &#39;train_steps_per_second&#39;: 0.036, &#39;train_loss&#39;: 0.32820896875290645, &#39;epoch&#39;: 1.0}
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> [2023-08-18 18:52:36,012] [INFO] [engine.py:3228:_save_zero_checkpoint] zero checkpoint saved output/checkpoint-84/global_step84/zero_pp_rank_0_mp_rank_00_optim_states.pt<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8875, ip=10.0.0.80)</span> [2023-08-18 18:52:36,193] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step84 is ready now!<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> <span class=" -Color -Color-Green"> [repeated 60x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=8911, ip=10.0.60.59)</span> Training completed. Do not forget to share your model on huggingface.co/models =)<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
2023-08-18 18:54:02,594	INFO tune.py:1146 -- Total run time: 2691.03 seconds (2676.82 seconds for the tuning loop).
</pre></div>
</div>
</div>
</div>
<p>Use the returned <a class="reference internal" href="../../api/doc/ray.train.Result.html#ray.train.Result" title="ray.train.Result"><code class="xref py py-class docutils literal notranslate"><span class="pre">Result</span></code></a> object to access metrics and the Ray Train <a class="reference internal" href="../../api/doc/ray.train.Checkpoint.html#ray.train.Checkpoint" title="ray.train.Checkpoint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Checkpoint</span></code></a> associated with the last iteration.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">checkpoint</span>
<span class="n">checkpoint</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Checkpoint(filesystem=&lt;pyarrow._s3fs.S3FileSystem object at 0x7f8c59d311b0&gt;, path=anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/org_7c1Kalm9WcX2bNIjW53GUT/cld_kvedZWag2qA8i5BjxUevf5i7/artifact_storage/yunxuan__xiao/gptj-deepspeed-finetune/TorchTrainer_2023-08-18_18-09-11/TorchTrainer_01ea5_00000_0_2023-08-18_18-09-12/checkpoint_000000)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="generate-text-from-prompt">
<span id="gptj-predict"></span><h3>Generate text from prompt<a class="headerlink" href="#generate-text-from-prompt" title="Link to this heading">#</a></h3>
<p>First, download the persistent Ray Train checkpoint locally and load the fine-tuned model weights and tokenizer from the checkpoint. Then use ðŸ¤— Transformers <a class="reference external" href="https://huggingface.co/docs/transformers/en/main_classes/pipelines"><code class="docutils literal notranslate"><span class="pre">pipeline</span></code></a> to generate predictions from the fine-tuned model.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>For large scale batch inference, see <a class="reference internal" href="../../../data/batch_inference.html#batch-inference-home"><span class="std std-ref">End-to-end: Offline Batch Inference</span></a>.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="n">os</span><span class="o">.</span><span class="n">system</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;aws s3 sync s3://</span><span class="si">{</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">path</span><span class="si">}</span><span class="s2"> /mnt/local_storage/&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Set the <code class="docutils literal notranslate"><span class="pre">task</span></code> to <code class="docutils literal notranslate"><span class="pre">&quot;text-generation&quot;</span></code>, and also set <code class="docutils literal notranslate"><span class="pre">device_map=&quot;auto&quot;</span></code> for Ray Train to automatically place the model on the right device.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">GPTJForCausalLM</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">GPTJForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;/mnt/local_storage/checkpoint&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;/mnt/local_storage/checkpoint&quot;</span><span class="p">)</span>

<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate from prompts!</span>
<span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">pipe</span><span class="p">(</span>
    <span class="p">[</span><span class="s2">&quot;Romeo and Juliet&quot;</span><span class="p">,</span> <span class="s2">&quot;Romeo&quot;</span><span class="p">,</span> <span class="s2">&quot;Juliet&quot;</span><span class="p">],</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">min_length</span><span class="o">=</span><span class="mi">20</span>
<span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[{&#39;generated_text&#39;: &#39;Romeo and Juliet. This very night shall they come. A word with you, sir.&#39;}]
[{&#39;generated_text&#39;: &#39;Romeo! I know thee not. Lord Mercutio, is it you! Signior Montague.&#39;}]
[{&#39;generated_text&#39;: &#39;Juliet, look up in the vault, and there shalt find a grave; within the monument there is a table:&#39;}]
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#set-up-ray">Set up Ray</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-the-dataset">Loading the dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-the-model-with-ray-train">Fine-tuning the model with Ray Train</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#training-speed">Training speed</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generate-text-from-prompt">Generate text from prompt</a></li>
</ul>
</li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">  
<div class="tocsection editthispage">
  <a href="https://github.com/ray-project/ray/edit/master/doc/source/train/examples/deepspeed/gptj_deepspeed_fine_tuning.ipynb">
    <i class="fa-solid fa-pencil"></i>
       Edit
    on GitHub  
  </a>
</div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="footer-content-items footer-content__inner">
  
    <div class="footer-content-item"><div id="csat">
  <div id="csat-feedback-received" class="csat-hidden">
    <span>Thanks for the feedback!</span>
  </div>
  <div id="csat-inputs">
    <span>Was this helpful?</span>
    <div id="csat-yes" class="csat-button">
      <svg id="csat-yes-icon" class="csat-hidden csat-icon" width="18" height="13" viewBox="0 0 18 13" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 10.172L16.1922 0.979004L17.6072 2.393L7.00023 13L0.63623 6.636L2.05023 5.222L7.00023 10.172Z" />
      </svg>
      <span>Yes<span>
    </div>
    <div id="csat-no" class="csat-button">
      <svg id="csat-no-icon" class="csat-hidden csat-icon" width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 5.58599L11.9502 0.635986L13.3642 2.04999L8.41423 6.99999L13.3642 11.95L11.9502 13.364L7.00023 8.41399L2.05023 13.364L0.63623 11.95L5.58623 6.99999L0.63623 2.04999L2.05023 0.635986L7.00023 5.58599Z" />
      </svg>
      <span>No<span>
    </div>
  </div>
  <div id="csat-textarea-group" class="csat-hidden">
    <span id="csat-feedback-label">Feedback</span>
    <textarea id="csat-textarea"></textarea>
    <div id="csat-submit">Submit</div>
  </div>
</div></div>
  
</div>

          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
  <p class="copyright">
    
      Â© Copyright 2025, The Ray Team.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">
  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.3.7.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.14.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>