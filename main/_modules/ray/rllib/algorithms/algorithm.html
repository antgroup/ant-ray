<!-- prettier-ignore -->

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>ray.rllib.algorithms.algorithm &#8212; Ray 3.0.0.dev0</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../../../../_static/styles/bootstrap.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />

  
  <link href="../../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=ac02cc09edc035673794" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/termynal.css?v=2fc3cb5e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/csat.css?v=c8f39c76" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/assistant.css?v=d345c55e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/custom.css?v=4676c12b" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/docsearch.css?v=45fb5152" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/pydata-docsearch-custom.css?v=7fb92720" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794" />
<link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794" />
  <script src="../../../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=ac02cc09edc035673794"></script>

    <script src="../../../../_static/documentation_options.js?v=d1493c90"></script>
    <script src="../../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../../_static/copybutton.js?v=9a29e97e"></script>
    <script defer="defer" src="../../../../_static/js/termynal.js?v=67cfcf08"></script>
    <script defer="defer" src="../../../../_static/js/custom.js?v=9e3b357f"></script>
    <script src="../../../../_static/js/csat.js?v=8e649b1c"></script>
    <script defer="defer" src="../../../../_static/js/assistant.js?v=73fdc522"></script>
    <script defer="defer" src="../../../../_static/docsearch.js?v=77274085"></script>
    <script defer="defer" src="../../../../_static/docsearch_config.js?v=d25523ed"></script>
    <script src="../../../../_static/design-tabs.js?v=36754332"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/ray/rllib/algorithms/algorithm';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.14.1';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://antgroup.github.io/ant-ray/versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'main';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = false;
        </script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/_modules/ray/rllib/algorithms/algorithm.html" />
    <link rel="icon" href="../../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" /><!-- Extra header to include at the top of each template.
Kept separately so that it can easily be included in any templates
that need to be overridden for individual pages; e.g. included
both in the usual template (layout.html) as well as (index.html). -->

<link rel="preconnect" href="https://fonts.googleapis.com" />
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
<link
  href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;900&family=Roboto:wght@400;500;700&display=swap"
  rel="stylesheet"
/>
<link
  rel="stylesheet"
  title="light"
  href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css"
  disabled="disabled"
/>
<link
  rel="stylesheet"
  title="dark"
  href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css"
  disabled="disabled"
/>
<link
  href="https://cdn.jsdelivr.net/npm/remixicon@4.1.0/fonts/remixicon.css"
  rel="stylesheet"
/>

<!-- Used for text embedded in html on the index page  -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- Parser used to call hljs on responses from Ray Assistant -->
<script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

<!-- Sanitizer for Ray Assistant AI -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/dompurify/2.3.3/purify.min.js"></script>

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<!-- Google Tag Manager -->
<script>
  (function (w, d, s, l, i) {
    w[l] = w[l] || [];
    w[l].push({
      'gtm.start': new Date().getTime(),
      event: 'gtm.js',
    });
    var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s),
      dl = l != 'dataLayer' ? '&l=' + l : '';
    j.async = true;
    j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
    f.parentNode.insertBefore(j, f);
  })(window, document, 'script', 'dataLayer', 'GTM-P8H6KQG');
</script>
<!-- End Google Tag Manager -->
<!-- Data to be shared with JS on every page -->
<script>
  window.data = {
      copyIconSrc: "../../../../_static/copy-button.svg"
  };
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>


  <div class="bd-header-announcement container-fluid bd-header-announcement">
    <div class="bd-header-announcement__content">Try Ray with $100 credit â€” <a target="_blank" href="https://console.anyscale.com/register/ha?render_flow=ray&utm_source=ray_docs&utm_medium=docs&utm_campaign=banner">Start now</a>.</div>
  </div>

  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  

<a class="navbar-brand logo" href="../../../../index.html">
  <svg width="400" height="201" viewBox="0 0 400 201" xmlns="http://www.w3.org/2000/svg">
<path id="ray-text" d="M325.949 134.356V109.785L302.442 66.6406H314.244L330.495 97.3062H331.946L348.198 66.6406H360L336.493 109.785V134.356H325.949ZM253.043 134.364L272.391 66.648H290.771L310.021 134.364H299.283L294.834 118.402H268.328L263.878 134.364H253.043ZM270.94 108.728H292.222L282.354 73.1294H280.807L270.94 108.728ZM198.887 134.364V66.648H227.327C231.519 66.648 235.195 67.3896 238.355 68.8729C241.58 70.2918 244.063 72.3555 245.804 75.0641C247.61 77.7727 248.513 80.9973 248.513 84.7378V85.8019C248.513 90.0583 247.481 93.4763 245.417 96.0559C243.418 98.5711 240.967 100.345 238.065 101.376V102.924C240.516 103.053 242.483 103.892 243.966 105.439C245.449 106.923 246.191 109.083 246.191 111.921V134.364H235.647V113.372C235.647 111.631 235.195 110.244 234.292 109.212C233.39 108.18 231.938 107.664 229.939 107.664H209.334V134.364H198.887ZM209.334 98.1842H226.166C229.907 98.1842 232.809 97.249 234.873 95.3788C236.937 93.4441 237.968 90.8322 237.968 87.5431V86.7692C237.968 83.4802 236.937 80.9005 234.873 79.0303C232.874 77.0956 229.971 76.1282 226.166 76.1282H209.334V98.1842Z" fill="black"/>
<path fill-rule="evenodd" clip-rule="evenodd" d="M143.63 101.311L98.3087 146.632L94.9903 143.313L140.311 97.9925L143.63 101.311ZM141.953 102.334L51.4454 102.334V97.6409L141.953 97.6409V102.334ZM94.992 55.9863L140.313 101.307L143.631 97.9886L98.3105 52.6679L94.992 55.9863Z" fill="#02A0CF"/>
<path d="M40 88.3163H62.6604V110.977H40V88.3163ZM85.3207 88.3163H107.981V110.977H85.3207V88.3163ZM85.3207 43H107.981V65.6604H85.3207V43ZM85.3207 133.645H107.981V156.306H85.3207V133.645ZM130.641 88.3163H153.301V110.977H130.641V88.3163Z" fill="#02A0CF"/>
</svg>

</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <div class="navbar-content docutils container">
<ul class="navbar-toplevel">
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/getting-started.html" title="Get Started"><span class="navbar-link-title">Get Started</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/use-cases.html" title="Use Cases"><span class="navbar-link-title">Use Cases</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/examples.html" title="Example Gallery"><span class="navbar-link-title">Example Gallery</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/installation.html" title="Library"><span class="navbar-link-title">Library</span></a></p>
<i class="fa-solid fa-chevron-down"></i></div>
<div class="navbar-dropdown docutils container">
<ul class="navbar-sublevel">
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-core/walkthrough.html" title="Ray Core"><span class="navbar-link-title">Ray Core</span>Scale general Python applications</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../data/data.html" title="Ray Data"><span class="navbar-link-title">Ray Data</span>Scale data ingest and preprocessing</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../train/train.html" title="Ray Train"><span class="navbar-link-title">Ray Train</span>Scale machine learning training</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../tune/index.html" title="Ray Tune"><span class="navbar-link-title">Ray Tune</span>Scale hyperparameter tuning</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../serve/index.html" title="Ray Serve"><span class="navbar-link-title">Ray Serve</span>Scale model serving</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../rllib/index.html" title="Ray RLlib"><span class="navbar-link-title">Ray RLlib</span>Scale reinforcement learning</a></p>
</div>
</li>
</ul>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../index.html" title="Docs"><span class="navbar-link-title">Docs</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://discuss.ray.io" title="Resources"><span class="navbar-link-title">Resources</span></a></p>
<i class="fa-solid fa-chevron-down"></i></div>
<div class="navbar-dropdown docutils container">
<ul class="navbar-sublevel">
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://discuss.ray.io" title="Discussion Forum"><span class="navbar-link-title">Discussion Forum</span>Get your Ray questions answered</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://github.com/ray-project/ray-educational-materials" title="Training"><span class="navbar-link-title">Training</span>Hands-on learning</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/blog" title="Blog"><span class="navbar-link-title">Blog</span>Updates, best practices, user-stories</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/events" title="Events"><span class="navbar-link-title">Events</span>Webinars, meetups, office hours</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/blog/how-ray-and-anyscale-make-it-easy-to-do-massive-scale-machine-learning-on" title="Success Stories"><span class="navbar-link-title">Success Stories</span>Real-world workload examples</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/ray-libraries.html" title="Ecosystem"><span class="navbar-link-title">Ecosystem</span>Libraries integrated with Ray</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.ray.io/community" title="Community"><span class="navbar-link-title">Community</span>Connect with us</a></p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>

</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          <div id="docsearch"></div>
        </div>
      
      
        <div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
      
        <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="versionswitcherbutton" type="button" role="button" class="version-switcher__button btn btn-sm navbar-btn dropdown-toggle" data-bs-toggle="dropdown" aria-haspopup="listbox" aria-controls="versionswitcherlist" aria-label="Version switcher list">
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="versionswitcherlist" class="version-switcher__menu dropdown-menu list-group-flush py-0" role="listbox" aria-labelledby="versionswitcherbutton">
    <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile"><div id="docsearch"></div>
    </div>
  

  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <div class="navbar-content docutils container">
<ul class="navbar-toplevel">
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/getting-started.html" title="Get Started"><span class="navbar-link-title">Get Started</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/use-cases.html" title="Use Cases"><span class="navbar-link-title">Use Cases</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/examples.html" title="Example Gallery"><span class="navbar-link-title">Example Gallery</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/installation.html" title="Library"><span class="navbar-link-title">Library</span></a></p>
<i class="fa-solid fa-chevron-down"></i></div>
<div class="navbar-dropdown docutils container">
<ul class="navbar-sublevel">
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-core/walkthrough.html" title="Ray Core"><span class="navbar-link-title">Ray Core</span>Scale general Python applications</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../data/data.html" title="Ray Data"><span class="navbar-link-title">Ray Data</span>Scale data ingest and preprocessing</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../train/train.html" title="Ray Train"><span class="navbar-link-title">Ray Train</span>Scale machine learning training</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../tune/index.html" title="Ray Tune"><span class="navbar-link-title">Ray Tune</span>Scale hyperparameter tuning</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../serve/index.html" title="Ray Serve"><span class="navbar-link-title">Ray Serve</span>Scale model serving</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../rllib/index.html" title="Ray RLlib"><span class="navbar-link-title">Ray RLlib</span>Scale reinforcement learning</a></p>
</div>
</li>
</ul>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../index.html" title="Docs"><span class="navbar-link-title">Docs</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://discuss.ray.io" title="Resources"><span class="navbar-link-title">Resources</span></a></p>
<i class="fa-solid fa-chevron-down"></i></div>
<div class="navbar-dropdown docutils container">
<ul class="navbar-sublevel">
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://discuss.ray.io" title="Discussion Forum"><span class="navbar-link-title">Discussion Forum</span>Get your Ray questions answered</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://github.com/ray-project/ray-educational-materials" title="Training"><span class="navbar-link-title">Training</span>Hands-on learning</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/blog" title="Blog"><span class="navbar-link-title">Blog</span>Updates, best practices, user-stories</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/events" title="Events"><span class="navbar-link-title">Events</span>Webinars, meetups, office hours</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/blog/how-ray-and-anyscale-make-it-easy-to-do-massive-scale-machine-learning-on" title="Success Stories"><span class="navbar-link-title">Success Stories</span>Real-world workload examples</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/ray-libraries.html" title="Ecosystem"><span class="navbar-link-title">Ecosystem</span>Libraries integrated with Ray</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.ray.io/community" title="Community"><span class="navbar-link-title">Community</span>Connect with us</a></p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>

</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
        
          <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="versionswitcherbutton" type="button" role="button" class="version-switcher__button btn btn-sm navbar-btn dropdown-toggle" data-bs-toggle="dropdown" aria-haspopup="listbox" aria-controls="versionswitcherlist" aria-label="Version switcher list">
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="versionswitcherlist" class="version-switcher__menu dropdown-menu list-group-flush py-0" role="listbox" aria-labelledby="versionswitcherbutton">
    <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item"><nav id="main-sidebar" class="bd-docs-nav bd-links" aria-label="Section Navigation">
  <div class="bd-toc-item navbar-nav"><ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../ray-overview/index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ray-overview/getting-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ray-overview/installation.html">Installation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../ray-overview/use-cases.html">Use Cases</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-air/getting-started.html">Ray for ML Infrastructure</a></li>

</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../ray-overview/examples/index.html">Examples</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-overview/examples/entity-recognition-with-llms/README.html">Entity Recognition with LLMs</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-overview/examples/e2e-audio/index.html">End-to-end audio</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-overview/examples/e2e-audio/README.html">Audio data curation with Ray</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-overview/examples/e2e-audio/e2e_audio/curation.html">Audio curation with offline batch inference</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-overview/examples/e2e-xgboost/index.html">End-to-end distributed XGBoost</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-overview/examples/e2e-xgboost/README.html">Overview</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../../../ray-overview/examples/e2e-xgboost/notebooks/01-Distributed_Training.html">Distributed training of an XGBoost model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-overview/examples/e2e-xgboost/notebooks/02-Validation.html">Model validation using offline batch inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-overview/examples/e2e-xgboost/notebooks/03-Serving.html">Scalable online XGBoost inference with Ray Serve</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ray-overview/ray-libraries.html">Ecosystem</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../ray-core/walkthrough.html">Ray Core</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-core/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-core/user-guide.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/tasks.html">Tasks</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/tasks/nested-tasks.html">Nested Remote Functions</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/actors.html">Actors</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/named-actors.html">Named Actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/terminating-actors.html">Terminating Actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/async_api.html">AsyncIO / Concurrency for Actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/concurrency_group_api.html">Limiting Concurrency Per-Method with Concurrency Groups</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/actor-utils.html">Utility Classes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/out-of-band-communication.html">Out-of-band Communication</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/task-orders.html">Actor Task Execution Order</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/objects.html">Objects</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/objects/serialization.html">Serialization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/objects/object-spilling.html">Object Spilling</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/handling-dependencies.html">Environment Dependencies</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/scheduling/index.html">Scheduling</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/scheduling/resources.html">Resources</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/scheduling/accelerators.html">Accelerator Support</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/scheduling/placement-group.html">Placement Groups</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/scheduling/memory-management.html">Memory Management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/scheduling/ray-oom-prevention.html">Out-Of-Memory Prevention</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/fault-tolerance.html">Fault tolerance</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/fault_tolerance/tasks.html">Task Fault Tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/fault_tolerance/actors.html">Actor Fault Tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/fault_tolerance/objects.html">Object Fault Tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/fault_tolerance/nodes.html">Node Fault Tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/fault_tolerance/gcs.html">GCS Fault Tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/fault_tolerance/head-ha.html">Head High-Availability Feature</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/patterns/index.html">Design Patterns &amp; Anti-patterns</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/nested-tasks.html">Pattern: Using nested tasks to achieve nested parallelism</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/generators.html">Pattern: Using generators to reduce heap memory usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/limit-pending-tasks.html">Pattern: Using ray.wait to limit the number of pending tasks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/limit-running-tasks.html">Pattern: Using resources to limit the number of concurrently running tasks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/concurrent-operations-async-actor.html">Pattern: Using asyncio to run actor methods concurrently</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/actor-sync.html">Pattern: Using an actor to synchronize other tasks and actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/tree-of-actors.html">Pattern: Using a supervisor actor to manage a tree of actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/pipelining.html">Pattern: Using pipelining to increase throughput</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/return-ray-put.html">Anti-pattern: Returning ray.put() ObjectRefs from a task harms performance and fault tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/ray-get-loop.html">Anti-pattern: Calling ray.get in a loop harms parallelism</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/unnecessary-ray-get.html">Anti-pattern: Calling ray.get unnecessarily harms performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/ray-get-submission-order.html">Anti-pattern: Processing results in submission order using ray.get increases runtime</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/ray-get-too-many-objects.html">Anti-pattern: Fetching too many objects at once with ray.get causes failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/too-fine-grained-tasks.html">Anti-pattern: Over-parallelizing with too fine-grained tasks harms speedup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/redefine-task-actor-loop.html">Anti-pattern: Redefining the same remote function or class harms performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/pass-large-arg-by-value.html">Anti-pattern: Passing the same large argument by value repeatedly harms performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/closure-capture-large-objects.html">Anti-pattern: Closure capturing large objects harms performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/global-variables.html">Anti-pattern: Using global variables to share state between tasks and actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/out-of-band-object-ref-serialization.html">Anti-pattern: Serialize ray.ObjectRef out of band</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/fork-new-processes.html">Anti-pattern: Forking new processes in application code</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/compiled-graph/ray-compiled-graph.html">Ray Compiled Graph (beta)</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/compiled-graph/quickstart.html">Quickstart</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/compiled-graph/profiling.html">Profiling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/compiled-graph/overlap.html">Experimental: Overlapping communication and computation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/compiled-graph/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/compiled-graph/compiled-graph-api.html">Compiled Graph API</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/advanced-topics.html">Advanced topics</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/tips-for-first-time.html">Tips for first-time users</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/starting-ray.html">Starting Ray</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/ray-generator.html">Ray Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/namespaces.html">Using Namespaces</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/cross-language.html">Cross-language programming</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/using-ray-with-jupyter.html">Working with Jupyter Notebooks &amp; JupyterLab</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/ray-dag.html">Lazy Computation Graphs with the Ray DAG API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/miscellaneous.html">Miscellaneous Topics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/runtime_env_auth.html">Authenticating Remote URIs in runtime_env</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/user-spawn-processes.html">Lifetimes of a User-Spawn Process</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-core/examples/overview.html">Examples</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/automl_for_time_series.html">Simple AutoML for time series with Ray Core</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/batch_prediction.html">Batch Prediction with Ray Core</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/gentle_walkthrough.html">A Gentle Introduction to Ray Core by Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/highly_parallel.html">Using Ray for Highly Parallelizable Tasks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/map_reduce.html">A Simple MapReduce Example with Ray Core</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/monte_carlo_pi.html">Monte Carlo Estimation of Ï€</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/plot_hyperparameter.html">Simple Parallel Model Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/plot_parameter_server.html">Parameter Server</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/plot_pong_example.html">Learning to Play Pong</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/web-crawler.html">Speed up your web crawler by parallelizing it with Ray</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-core/api/index.html">Ray Core API</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/api/core.html">Core API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/api/scheduling.html">Scheduling API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/api/runtime-env.html">Runtime Env API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/api/utility.html">Utility</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/api/exceptions.html">Exceptions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/api/cli.html">Ray Core CLI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/reference/cli.html">State CLI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/reference/api.html">State API</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../data/data.html">Ray Data</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../data/quickstart.html">Ray Data Quickstart</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../data/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../data/user-guide.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/loading-data.html">Loading Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/inspecting-data.html">Inspecting Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/transforming-data.html">Transforming Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/iterating-over-data.html">Iterating over Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/joining-data.html">Joining datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/shuffling-data.html">Shuffling Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/saving-data.html">Saving Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/working-with-images.html">Working with Images</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/working-with-text.html">Working with Text</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/working-with-tensors.html">Working with Tensors / NumPy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/working-with-pytorch.html">Working with PyTorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/working-with-llms.html">Working with LLMs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/monitoring-your-workload.html">Monitoring Your Workload</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/execution-configurations.html">Execution Configurations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/batch_inference.html">End-to-end: Offline Batch Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/performance-tips.html">Advanced: Performance Tips and Tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/custom-datasource-example.html">Advanced: Read and Write Custom File Types</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../data/examples.html">Examples</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../data/api/api.html">Ray Data API</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/input_output.html">Input/Output</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/dataset.html">Dataset API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/data_iterator.html">DataIterator API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/execution_options.html">ExecutionOptions API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/aggregate.html">Aggregation API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/grouped_data.html">GroupedData API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/data_context.html">Global configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/preprocessor.html">Preprocessor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/llm.html">Large Language Model (LLM) API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/from_other_data_libs.html">API Guide for Users from Other Data Libraries</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../data/comparisons.html">Comparing Ray Data to other systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../data/data-internals.html">Ray Data Internals</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../train/train.html">Ray Train</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-20"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/overview.html">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/getting-started-pytorch.html">PyTorch Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/getting-started-pytorch-lightning.html">PyTorch Lightning Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/getting-started-transformers.html">Hugging Face Transformers Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/getting-started-xgboost.html">XGBoost Guide</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../train/more-frameworks.html">More Frameworks</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-21"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/huggingface-accelerate.html">Hugging Face Accelerate Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/deepspeed.html">DeepSpeed Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/distributed-tensorflow-keras.html">TensorFlow and Keras Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/examples/xgboost/distributed-xgboost-lightgbm.html">XGBoost and LightGBM Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/horovod.html">Horovod Guide</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../train/user-guides.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-22"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/data-loading-preprocessing.html">Data Loading and Preprocessing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/using-gpus.html">Configuring Scale and GPUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/persistent-storage.html">Configuring Persistent Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/monitoring-logging.html">Monitoring and Logging Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/checkpoints.html">Saving and Loading Checkpoints</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/experiment-tracking.html">Experiment Tracking</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/results.html">Inspecting Training Results</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/fault-tolerance.html">Handling Failures and Node Preemption</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/reproducibility.html">Reproducibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/hyperparameter-optimization.html">Hyperparameter Optimization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/examples.html">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/benchmarks.html">Benchmarks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/api/api.html">Ray Train API</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../tune/index.html">Ray Tune</a><input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-23"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../tune/getting-started.html">Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../tune/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../tune/tutorials/overview.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-24"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-run.html">Running Basic Experiments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-output.html">Logging and Outputs in Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-resources.html">Setting Trial Resources</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-search-spaces.html">Using Search Spaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-stopping.html">How to Define Stopping Criteria for a Ray Tune Experiment</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-trial-checkpoints.html">How to Save and Load Trial Checkpoints</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-storage.html">How to Configure Persistent Storage in Ray Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-fault-tolerance.html">How to Enable Fault Tolerance in Ray Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-metrics.html">Using Callbacks and Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune_get_data_in_and_out.html">Getting Data in and out of Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/tune_analyze_results.html">Analyzing Tune Experiment Results</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../tune/examples/pbt_guide.html">A Guide to Population Based Training with Tune</a><input class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-25"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/pbt_visualization/pbt_visualization.html">Visualizing and Understanding PBT</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-distributed.html">Deploying Tune in the Cloud</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-lifecycle.html">Tune Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-scalability.html">Scalability Benchmarks</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../tune/examples/index.html">Ray Tune Examples</a><input class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-26"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/tune-pytorch-cifar.html">PyTorch Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/tune-pytorch-lightning.html">PyTorch Lightning Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/tune-xgboost.html">XGBoost Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/lightgbm_example.html">LightGBM Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/pbt_transformers.html">Hugging Face Transformers Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/pbt_ppo_example.html">Ray RLlib Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/tune_mnist_keras.html">Keras Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/horovod_simple.html">Horovod Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/tune-wandb.html">Weights &amp; Biases Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/tune-mlflow.html">MLflow Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/tune-aim.html">Aim Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/tune-comet.html">Comet Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/ax_example.html">Ax Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/hyperopt_example.html">HyperOpt Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/bayesopt_example.html">Bayesopt Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/bohb_example.html">BOHB Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/nevergrad_example.html">Nevergrad Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/optuna_example.html">Optuna Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../tune/faq.html">Ray Tune FAQ</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../tune/api/api.html">Ray Tune API</a><input class="toctree-checkbox" id="toctree-checkbox-27" name="toctree-checkbox-27" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-27"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/execution.html">Tune Execution (tune.Tuner)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/result_grid.html">Tune Experiment Results (tune.ResultGrid)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/trainable.html">Training in Tune (tune.Trainable, tune.report)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/search_space.html">Tune Search Space API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/suggestion.html">Tune Search Algorithms (tune.search)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/schedulers.html">Tune Trial Schedulers (tune.schedulers)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/stoppers.html">Tune Stopping Mechanisms (tune.stopper)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/reporters.html">Tune Console Output (Reporters)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/syncing.html">Syncing in Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/logging.html">Tune Loggers (tune.logger)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/callbacks.html">Tune Callbacks (tune.Callback)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/env.html">Environment variables used by Ray Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/integration.html">External library integrations for Ray Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/internals.html">Tune Internals</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/cli.html">Tune CLI (Experimental)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../serve/index.html">Ray Serve</a><input class="toctree-checkbox" id="toctree-checkbox-28" name="toctree-checkbox-28" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-28"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/getting_started.html">Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/develop-and-deploy.html">Develop and Deploy an ML Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/model_composition.html">Deploy Compositions of Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/multi-app.html">Deploy Multiple Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/model-multiplexing.html">Model Multiplexing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/configure-serve-deployment.html">Configure Ray Serve deployments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/http-guide.html">Set Up FastAPI and HTTP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/llm/serving-llms.html">Serving LLMs</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../serve/production-guide/index.html">Production Guide</a><input class="toctree-checkbox" id="toctree-checkbox-29" name="toctree-checkbox-29" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-29"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/production-guide/config.html">Serve Config Files</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/production-guide/kubernetes.html">Deploy on Kubernetes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/production-guide/docker.html">Custom Docker Images</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/production-guide/fault-tolerance.html">Add End-to-End Fault Tolerance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/production-guide/handling-dependencies.html">Handle Dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/production-guide/best-practices.html">Best practices in production</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/monitoring.html">Monitor Your Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/resource-allocation.html">Resource Allocation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/autoscaling-guide.html">Ray Serve Autoscaling</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../serve/advanced-guides/index.html">Advanced Guides</a><input class="toctree-checkbox" id="toctree-checkbox-30" name="toctree-checkbox-30" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-30"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/app-builder-guide.html">Pass Arguments to Applications</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/advanced-autoscaling.html">Advanced Ray Serve Autoscaling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/performance.html">Performance Tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/dyn-req-batch.html">Dynamic Request Batching</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/inplace-updates.html">Updating Applications In-Place</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/dev-workflow.html">Development Workflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/grpc-guide.html">Set Up a gRPC Service</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/managing-java-deployments.html">Experimental Java API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/deploy-vm.html">Deploy on VM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/multi-app-container.html">Run Multiple Applications in Different Containers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/architecture.html">Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/examples.html">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/api/index.html">Ray Serve API</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../rllib/index.html">Ray RLlib</a><input class="toctree-checkbox" id="toctree-checkbox-31" name="toctree-checkbox-31" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-31"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../rllib/getting-started.html">Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../rllib/key-concepts.html">Key concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../rllib/rllib-env.html">Environments</a><input class="toctree-checkbox" id="toctree-checkbox-32" name="toctree-checkbox-32" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-32"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/multi-agent-envs.html">Multi-Agent Environments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/hierarchical-envs.html">Hierarchical Environments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/external-envs.html">External Environments and Applications</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../rllib/algorithm-config.html">AlgorithmConfig API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../rllib/rllib-algorithms.html">Algorithms</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../rllib/user-guides.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-33" name="toctree-checkbox-33" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-33"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-advanced-api.html">Advanced Python APIs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-callback.html">Callbacks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/checkpoints.html">Checkpointing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/metrics-logger.html">MetricsLogger API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/single-agent-episode.html">Episodes</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-replay-buffers.html">Replay Buffers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-offline.html">Working with offline data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rl-modules.html">RL Modules</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-learner.html">Learner (Alpha)</a></li>



<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-torch2x.html">Using RLlib with torch 2.x compile</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-fault-tolerance.html">Fault Tolerance And Elastic Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-dev.html">Install RLlib for Development</a></li>




<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/scaling-guide.html">RLlib scaling guide</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../rllib/rllib-examples.html">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../rllib/new-api-stack-migration-guide.html">New API stack migration guide</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../rllib/package_ref/index.html">Ray RLlib API</a><input class="toctree-checkbox" id="toctree-checkbox-34" name="toctree-checkbox-34" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-34"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/algorithm-config.html">Algorithm Configuration API</a><input class="toctree-checkbox" id="toctree-checkbox-35" name="toctree-checkbox-35" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-35"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_algo.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_algo</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner_group.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner_group</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.is_multi_agent.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.is_multi_agent</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.is_offline.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.is_offline</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learner_class.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learner_class</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.model_config.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.model_config</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.rl_module_spec.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.rl_module_spec</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.total_train_batch_size.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.total_train_batch_size</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_learner_class.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_learner_class</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_rl_module_spec.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_rl_module_spec</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_evaluation_config_object.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_evaluation_config_object</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_multi_rl_module_spec.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_multi_rl_module_spec</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_multi_agent_setup.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_multi_agent_setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_rollout_fragment_length.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_rollout_fragment_length</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.copy.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.copy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.validate.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.validate</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.freeze.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.freeze</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/algorithm.html">Algorithms</a><input class="toctree-checkbox" id="toctree-checkbox-36" name="toctree-checkbox-36" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-36"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.html">ray.rllib.algorithms.algorithm.Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.setup.html">ray.rllib.algorithms.algorithm.Algorithm.setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_default_config.html">ray.rllib.algorithms.algorithm.Algorithm.get_default_config</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.env_runner.html">ray.rllib.algorithms.algorithm.Algorithm.env_runner</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.eval_env_runner.html">ray.rllib.algorithms.algorithm.Algorithm.eval_env_runner</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.train.html">ray.rllib.algorithms.algorithm.Algorithm.train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.training_step.html">ray.rllib.algorithms.algorithm.Algorithm.training_step</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.save_to_path.html">ray.rllib.algorithms.algorithm.Algorithm.save_to_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.restore_from_path.html">ray.rllib.algorithms.algorithm.Algorithm.restore_from_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.from_checkpoint.html">ray.rllib.algorithms.algorithm.Algorithm.from_checkpoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_state.html">ray.rllib.algorithms.algorithm.Algorithm.get_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.set_state.html">ray.rllib.algorithms.algorithm.Algorithm.set_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.evaluate.html">ray.rllib.algorithms.algorithm.Algorithm.evaluate</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_module.html">ray.rllib.algorithms.algorithm.Algorithm.get_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.add_policy.html">ray.rllib.algorithms.algorithm.Algorithm.add_policy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.remove_policy.html">ray.rllib.algorithms.algorithm.Algorithm.remove_policy</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/callback.html">Callback APIs</a><input class="toctree-checkbox" id="toctree-checkbox-37" name="toctree-checkbox-37" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-37"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.html">ray.rllib.callbacks.callbacks.RLlibCallback</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_algorithm_init.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_algorithm_init</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_sample_end.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_sample_end</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_train_result.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_train_result</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_evaluate_start.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_evaluate_start</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_evaluate_end.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_evaluate_end</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_env_runners_recreated.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_env_runners_recreated</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_checkpoint_loaded.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_checkpoint_loaded</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_environment_created.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_environment_created</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_created.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_created</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_start.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_start</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_step.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_step</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_end.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_end</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/env.html">Environments</a><input class="toctree-checkbox" id="toctree-checkbox-38" name="toctree-checkbox-38" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-38"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/env/env_runner.html">EnvRunner API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/env/single_agent_env_runner.html">SingleAgentEnvRunner API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/env/single_agent_episode.html">SingleAgentEpisode API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/env/multi_agent_env.html">MultiAgentEnv API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/env/multi_agent_env_runner.html">MultiAgentEnvRunner API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/env/multi_agent_episode.html">MultiAgentEpisode API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/env/utils.html">Env Utils</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/rl_modules.html">RLModule APIs</a><input class="toctree-checkbox" id="toctree-checkbox-39" name="toctree-checkbox-39" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-39"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.build.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.build</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.module_class.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.module_class</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.observation_space.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.observation_space</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.action_space.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.action_space</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.inference_only.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.inference_only</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.learner_only.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.learner_only</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.model_config.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.model_config</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec.build.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec.build</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.default_model_config.DefaultModelConfig.html">ray.rllib.core.rl_module.default_model_config.DefaultModelConfig</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.html">ray.rllib.core.rl_module.rl_module.RLModule</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.observation_space.html">ray.rllib.core.rl_module.rl_module.RLModule.observation_space</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.action_space.html">ray.rllib.core.rl_module.rl_module.RLModule.action_space</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.inference_only.html">ray.rllib.core.rl_module.rl_module.RLModule.inference_only</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.model_config.html">ray.rllib.core.rl_module.rl_module.RLModule.model_config</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.setup.html">ray.rllib.core.rl_module.rl_module.RLModule.setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.as_multi_rl_module.html">ray.rllib.core.rl_module.rl_module.RLModule.as_multi_rl_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.forward_exploration.html">ray.rllib.core.rl_module.rl_module.RLModule.forward_exploration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.forward_inference.html">ray.rllib.core.rl_module.rl_module.RLModule.forward_inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.forward_train.html">ray.rllib.core.rl_module.rl_module.RLModule.forward_train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule._forward.html">ray.rllib.core.rl_module.rl_module.RLModule._forward</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule._forward_exploration.html">ray.rllib.core.rl_module.rl_module.RLModule._forward_exploration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule._forward_inference.html">ray.rllib.core.rl_module.rl_module.RLModule._forward_inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule._forward_train.html">ray.rllib.core.rl_module.rl_module.RLModule._forward_train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.save_to_path.html">ray.rllib.core.rl_module.rl_module.RLModule.save_to_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.restore_from_path.html">ray.rllib.core.rl_module.rl_module.RLModule.restore_from_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.from_checkpoint.html">ray.rllib.core.rl_module.rl_module.RLModule.from_checkpoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.get_state.html">ray.rllib.core.rl_module.rl_module.RLModule.get_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.set_state.html">ray.rllib.core.rl_module.rl_module.RLModule.set_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.setup.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.as_multi_rl_module.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.as_multi_rl_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.add_module.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.add_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.remove_module.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.remove_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.save_to_path.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.save_to_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.restore_from_path.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.restore_from_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.from_checkpoint.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.from_checkpoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.get_state.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.get_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.set_state.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.set_state</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/distributions.html">Distribution API</a><input class="toctree-checkbox" id="toctree-checkbox-40" name="toctree-checkbox-40" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-40"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.distributions.Distribution.html">ray.rllib.models.distributions.Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.distributions.Distribution.from_logits.html">ray.rllib.models.distributions.Distribution.from_logits</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.distributions.Distribution.sample.html">ray.rllib.models.distributions.Distribution.sample</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.distributions.Distribution.rsample.html">ray.rllib.models.distributions.Distribution.rsample</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.distributions.Distribution.logp.html">ray.rllib.models.distributions.Distribution.logp</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.distributions.Distribution.kl.html">ray.rllib.models.distributions.Distribution.kl</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/learner.html">LearnerGroup API</a><input class="toctree-checkbox" id="toctree-checkbox-41" name="toctree-checkbox-41" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-41"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learners.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learners</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner_group.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner_group</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner_group.LearnerGroup.html">ray.rllib.core.learner.learner_group.LearnerGroup</a></li>
</ul>
</li>

<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/offline.html">Offline RL API</a><input class="toctree-checkbox" id="toctree-checkbox-42" name="toctree-checkbox-42" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-42"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.offline_data.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.offline_data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learners.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learners</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.env_runners.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.env_runners</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner.html">ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_data.OfflineData.html">ray.rllib.offline.offline_data.OfflineData</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_data.OfflineData.__init__.html">ray.rllib.offline.offline_data.OfflineData.__init__</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_data.OfflineData.sample.html">ray.rllib.offline.offline_data.OfflineData.sample</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_data.OfflineData.default_map_batches_kwargs.html">ray.rllib.offline.offline_data.OfflineData.default_map_batches_kwargs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_data.OfflineData.default_iter_batches_kwargs.html">ray.rllib.offline.offline_data.OfflineData.default_iter_batches_kwargs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.__init__.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner.__init__</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.SCHEMA.html">ray.rllib.offline.offline_prelearner.SCHEMA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.__call__.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner.__call__</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner._map_to_episodes.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner._map_to_episodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner._map_sample_batch_to_episode.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner._map_sample_batch_to_episode</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner._should_module_be_updated.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner._should_module_be_updated</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.default_prelearner_buffer_class.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner.default_prelearner_buffer_class</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.default_prelearner_buffer_kwargs.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner.default_prelearner_buffer_kwargs</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/replay-buffers.html">Replay Buffer API</a><input class="toctree-checkbox" id="toctree-checkbox-43" name="toctree-checkbox-43" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-43"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.StorageUnit.html">ray.rllib.utils.replay_buffers.replay_buffer.StorageUnit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.prioritized_replay_buffer.PrioritizedReplayBuffer.html">ray.rllib.utils.replay_buffers.prioritized_replay_buffer.PrioritizedReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.reservoir_replay_buffer.ReservoirReplayBuffer.html">ray.rllib.utils.replay_buffers.reservoir_replay_buffer.ReservoirReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.sample.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.sample</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.add.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.add</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.get_state.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.get_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.set_state.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.set_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.multi_agent_replay_buffer.MultiAgentReplayBuffer.html">ray.rllib.utils.replay_buffers.multi_agent_replay_buffer.MultiAgentReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.multi_agent_prioritized_replay_buffer.MultiAgentPrioritizedReplayBuffer.html">ray.rllib.utils.replay_buffers.multi_agent_prioritized_replay_buffer.MultiAgentPrioritizedReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.utils.update_priorities_in_replay_buffer.html">ray.rllib.utils.replay_buffers.utils.update_priorities_in_replay_buffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.utils.sample_min_n_steps_from_buffer.html">ray.rllib.utils.replay_buffers.utils.sample_min_n_steps_from_buffer</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/utils.html">RLlib Utilities</a><input class="toctree-checkbox" id="toctree-checkbox-44" name="toctree-checkbox-44" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-44"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.peek.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger.peek</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_value.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_value</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_dict.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_dict</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.aggregate.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger.aggregate</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_time.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_time</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.schedules.scheduler.Scheduler.html">ray.rllib.utils.schedules.scheduler.Scheduler</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.schedules.scheduler.Scheduler.validate.html">ray.rllib.utils.schedules.scheduler.Scheduler.validate</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.schedules.scheduler.Scheduler.get_current_value.html">ray.rllib.utils.schedules.scheduler.Scheduler.get_current_value</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.schedules.scheduler.Scheduler.update.html">ray.rllib.utils.schedules.scheduler.Scheduler.update</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.schedules.scheduler.Scheduler._create_tensor_variable.html">ray.rllib.utils.schedules.scheduler.Scheduler._create_tensor_variable</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.framework.try_import_torch.html">ray.rllib.utils.framework.try_import_torch</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.clip_gradients.html">ray.rllib.utils.torch_utils.clip_gradients</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.compute_global_norm.html">ray.rllib.utils.torch_utils.compute_global_norm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.convert_to_torch_tensor.html">ray.rllib.utils.torch_utils.convert_to_torch_tensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.explained_variance.html">ray.rllib.utils.torch_utils.explained_variance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.flatten_inputs_to_1d_tensor.html">ray.rllib.utils.torch_utils.flatten_inputs_to_1d_tensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.global_norm.html">ray.rllib.utils.torch_utils.global_norm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.one_hot.html">ray.rllib.utils.torch_utils.one_hot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.reduce_mean_ignore_inf.html">ray.rllib.utils.torch_utils.reduce_mean_ignore_inf</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.sequence_mask.html">ray.rllib.utils.torch_utils.sequence_mask</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.set_torch_seed.html">ray.rllib.utils.torch_utils.set_torch_seed</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.softmax_cross_entropy_with_logits.html">ray.rllib.utils.torch_utils.softmax_cross_entropy_with_logits</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.update_target_network.html">ray.rllib.utils.torch_utils.update_target_network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.aligned_array.html">ray.rllib.utils.numpy.aligned_array</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.concat_aligned.html">ray.rllib.utils.numpy.concat_aligned</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.convert_to_numpy.html">ray.rllib.utils.numpy.convert_to_numpy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.fc.html">ray.rllib.utils.numpy.fc</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.flatten_inputs_to_1d_tensor.html">ray.rllib.utils.numpy.flatten_inputs_to_1d_tensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.make_action_immutable.html">ray.rllib.utils.numpy.make_action_immutable</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.huber_loss.html">ray.rllib.utils.numpy.huber_loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.l2_loss.html">ray.rllib.utils.numpy.l2_loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.lstm.html">ray.rllib.utils.numpy.lstm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.one_hot.html">ray.rllib.utils.numpy.one_hot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.relu.html">ray.rllib.utils.numpy.relu</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.sigmoid.html">ray.rllib.utils.numpy.sigmoid</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.softmax.html">ray.rllib.utils.numpy.softmax</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.checkpoints.try_import_msgpack.html">ray.rllib.utils.checkpoints.try_import_msgpack</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.checkpoints.Checkpointable.html">ray.rllib.utils.checkpoints.Checkpointable</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../ray-more-libs/index.html">More Libraries</a><input class="toctree-checkbox" id="toctree-checkbox-45" name="toctree-checkbox-45" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-45"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-more-libs/joblib.html">Distributed Scikit-learn / Joblib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-more-libs/multiprocessing.html">Distributed multiprocessing.Pool</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-more-libs/ray-collective.html">Ray Collective Communication Lib</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-more-libs/dask-on-ray.html">Using Dask on Ray</a><input class="toctree-checkbox" id="toctree-checkbox-46" name="toctree-checkbox-46" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-46"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.RayDaskCallback.html">ray.util.dask.RayDaskCallback</a><input class="toctree-checkbox" id="toctree-checkbox-47" name="toctree-checkbox-47" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-47"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.RayDaskCallback.ray_active.html">ray.util.dask.RayDaskCallback.ray_active</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_presubmit.html">ray.util.dask.callbacks.RayDaskCallback._ray_presubmit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_postsubmit.html">ray.util.dask.callbacks.RayDaskCallback._ray_postsubmit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_pretask.html">ray.util.dask.callbacks.RayDaskCallback._ray_pretask</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_posttask.html">ray.util.dask.callbacks.RayDaskCallback._ray_posttask</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_postsubmit_all.html">ray.util.dask.callbacks.RayDaskCallback._ray_postsubmit_all</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_finish.html">ray.util.dask.callbacks.RayDaskCallback._ray_finish</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-more-libs/raydp.html">Using Spark on Ray (RayDP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-more-libs/mars-on-ray.html">Using Mars on Ray</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-more-libs/modin/index.html">Using Pandas on Ray (Modin)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-more-libs/data_juicer_distributed_data_processing.html">Distributed Data Processing in Data-Juicer</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../workflows/index.html">Ray Workflows (Deprecated)</a><input class="toctree-checkbox" id="toctree-checkbox-48" name="toctree-checkbox-48" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-48"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/basics.html">Getting Started</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/management.html">Workflow Management</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/metadata.html">Workflow Metadata</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/events.html">Events</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/comparison.html">API Comparisons</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/advanced.html">Advanced Topics</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../workflows/api/api.html">Ray Workflows API</a><input class="toctree-checkbox" id="toctree-checkbox-49" name="toctree-checkbox-49" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-49"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../workflows/api/execution.html">Workflow Execution API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../workflows/api/management.html">Workflow Management API</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../cluster/getting-started.html">Ray Clusters</a><input class="toctree-checkbox" id="toctree-checkbox-50" name="toctree-checkbox-50" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-50"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../cluster/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/index.html">Deploying on Kubernetes</a><input class="toctree-checkbox" id="toctree-checkbox-51" name="toctree-checkbox-51" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-51"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/getting-started.html">Getting Started with KubeRay</a><input class="toctree-checkbox" id="toctree-checkbox-52" name="toctree-checkbox-52" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-52"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/getting-started/kuberay-operator-installation.html">KubeRay Operator Installation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/getting-started/raycluster-quick-start.html">RayCluster Quickstart</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/getting-started/rayjob-quick-start.html">RayJob Quickstart</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/getting-started/rayservice-quick-start.html">RayService Quickstart</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-53" name="toctree-checkbox-53" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-53"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/rayservice.html">Deploy Ray Serve Apps</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/rayservice-no-ray-serve-replica.html">RayService worker Pods arenâ€™t ready</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/rayservice-high-availability.html">RayService high availability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/observability.html">KubeRay Observability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/upgrade-guide.html">KubeRay upgrade guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/k8s-cluster-setup.html">Managed Kubernetes services</a></li>




<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/storage.html">Best Practices for Storage and Dependencies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/config.html">RayCluster Configuration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/configuring-autoscaling.html">KubeRay Autoscaling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/kuberay-gcs-ft.html">GCS fault tolerance in KubeRay</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/kuberay-gcs-persistent-ft.html">Tuning Redis for a Persistent Fault Tolerant GCS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/gke-gcs-bucket.html">Configuring KubeRay to use Google Cloud Storage Buckets in GKE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/persist-kuberay-custom-resource-logs.html">Persist KubeRay custom resource logs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/persist-kuberay-operator-logs.html">Persist KubeRay Operator Logs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/gpu.html">Using GPUs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/tpu.html">Use TPUs with KubeRay</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/rayserve-dev-doc.html">Developing Ray Serve Python scripts on a RayCluster</a></li>









<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/pod-command.html">Specify container commands for Ray head/worker Pods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/helm-chart-rbac.html">Helm Chart RBAC</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/tls.html">TLS Authentication</a></li>






<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/k8s-autoscaler.html">(Advanced) Understanding the Ray Autoscaler in the Context of Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.html">(Advanced) Deploying a static Ray cluster without KubeRay</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/kubectl-plugin.html">Use kubectl plugin (beta)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/kuberay-auth.html">Configure Ray clusters with authentication and access control using KubeRay</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/reduce-image-pull-latency.html">Reducing image pull latency on Kubernetes</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/examples.html">Examples</a><input class="toctree-checkbox" id="toctree-checkbox-54" name="toctree-checkbox-54" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-54"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/ml-example.html">Ray Train XGBoostTrainer on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/gpu-training-example.html">Train PyTorch ResNet model with GPUs on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/mnist-training-example.html">Train a PyTorch model on Fashion MNIST with CPUs on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/stable-diffusion-rayservice.html">Serve a StableDiffusion text-to-image model on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/tpu-serve-stable-diffusion.html">Serve a Stable Diffusion model on GKE with TPUs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/mobilenet-rayservice.html">Serve a MobileNet image classifier on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/text-summarizer-rayservice.html">Serve a text summarizer on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/rayjob-batch-inference-example.html">RayJob Batch Inference Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/rayjob-kueue-priority-scheduling.html">Priority Scheduling with RayJob and Kueue</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/rayjob-kueue-gang-scheduling.html">Gang Scheduling with RayJob and Kueue</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/distributed-checkpointing-with-gcsfuse.html">Distributed checkpointing with KubeRay and GCSFuse</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/modin-example.html">Use Modin with Ray on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/vllm-rayservice.html">Serve a Large Language Model with vLLM on Kubernetes</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem.html">KubeRay Ecosystem</a><input class="toctree-checkbox" id="toctree-checkbox-55" name="toctree-checkbox-55" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-55"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/ingress.html">Ingress</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/prometheus-grafana.html">Using Prometheus and Grafana</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/pyspy.html">Profiling with py-spy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/volcano.html">KubeRay integration with Volcano</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/yunikorn.html">KubeRay integration with Apache YuniKorn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/kueue.html">Gang scheduling and priority scheduling for RayJob with Kueue</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/istio.html">mTLS and L7 observability with Istio</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/benchmarks.html">KubeRay Benchmarks</a><input class="toctree-checkbox" id="toctree-checkbox-56" name="toctree-checkbox-56" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-56"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/benchmarks/memory-scalability-benchmark.html">KubeRay memory and scalability benchmark</a></li>

</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/troubleshooting.html">KubeRay Troubleshooting</a><input class="toctree-checkbox" id="toctree-checkbox-57" name="toctree-checkbox-57" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-57"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/troubleshooting/troubleshooting.html">Troubleshooting guide</a></li>

<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/troubleshooting/rayservice-troubleshooting.html">RayService troubleshooting</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/kubernetes/references.html">API Reference</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../cluster/vms/index.html">Deploying on VMs</a><input class="toctree-checkbox" id="toctree-checkbox-58" name="toctree-checkbox-58" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-58"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/vms/getting-started.html">Getting Started</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/vms/user-guides/index.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-59" name="toctree-checkbox-59" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-59"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/user-guides/launching-clusters/index.html">Launching Ray Clusters on AWS, GCP, Azure, vSphere, On-Prem</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/user-guides/large-cluster-best-practices.html">Best practices for deploying large clusters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/user-guides/configuring-autoscaling.html">Configuring Autoscaling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/user-guides/logging.html">Log Persistence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/user-guides/community/index.html">Community Supported Cluster Managers</a></li>

</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/vms/examples/index.html">Examples</a><input class="toctree-checkbox" id="toctree-checkbox-60" name="toctree-checkbox-60" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-60"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/examples/ml-example.html">Ray Train XGBoostTrainer on VMs</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/vms/references/index.html">API References</a><input class="toctree-checkbox" id="toctree-checkbox-61" name="toctree-checkbox-61" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-61"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/references/ray-cluster-cli.html">Cluster Launcher Commands</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/references/ray-cluster-configuration.html">Cluster YAML Configuration Options</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../cluster/metrics.html">Collecting and monitoring metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../cluster/configure-manage-dashboard.html">Configuring and Managing Ray Dashboard</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../cluster/running-applications/index.html">Applications Guide</a><input class="toctree-checkbox" id="toctree-checkbox-62" name="toctree-checkbox-62" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-62"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/index.html">Ray Jobs Overview</a><input class="toctree-checkbox" id="toctree-checkbox-63" name="toctree-checkbox-63" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-63"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/quickstart.html">Quickstart using the Ray Jobs CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/sdk.html">Python SDK Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/jobs-package-ref.html">Python SDK API Reference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/cli.html">Ray Jobs CLI API Reference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/rest.html">Ray Jobs REST API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/ray-client.html">Ray Client</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/running-applications/autoscaling/reference.html">Programmatic Cluster Scaling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../cluster/faq.html">FAQ</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../cluster/package-overview.html">Ray Cluster Management API</a><input class="toctree-checkbox" id="toctree-checkbox-64" name="toctree-checkbox-64" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-64"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/cli.html">Cluster Management CLI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/jobs-package-ref.html">Python SDK API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/cli.html">Ray Jobs CLI API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/running-applications/autoscaling/reference.html">Programmatic Cluster Scaling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../cluster/usage-stats.html">Usage Stats Collection</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../virtual-cluster/getting-started.html">Ray Virtual Clusters</a><input class="toctree-checkbox" id="toctree-checkbox-65" name="toctree-checkbox-65" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-65"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../virtual-cluster/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../virtual-cluster/design-overview.html">Design Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../virtual-cluster/management.html">Virtual Cluster Management API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../virtual-cluster/cli.html">Virtual Cluster CLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../virtual-cluster/examples.html">Examples</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../ray-observability/index.html">Monitoring and Debugging</a><input class="toctree-checkbox" id="toctree-checkbox-66" name="toctree-checkbox-66" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-66"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-observability/getting-started.html">Ray Dashboard</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-observability/ray-distributed-debugger.html">Ray Distributed Debugger</a></li>



<li class="toctree-l2"><a class="reference internal" href="../../../../ray-observability/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-observability/user-guides/index.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-67" name="toctree-checkbox-67" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-67"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/index.html">Debugging Applications</a><input class="toctree-checkbox" id="toctree-checkbox-68" name="toctree-checkbox-68" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-68"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/general-debugging.html">Common Issues</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/debug-memory.html">Debugging Memory Issues</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/debug-hangs.html">Debugging Hangs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/debug-failures.html">Debugging Failures</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/optimize-performance.html">Optimizing Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-observability/ray-distributed-debugger.html">Ray Distributed Debugger</a></li>



<li class="toctree-l4"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/ray-debugging.html">Using the Ray Debugger</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/user-guides/cli-sdk.html">Monitoring with the CLI or SDK</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/user-guides/configure-logging.html">Configuring Logging</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/user-guides/profiling.html">Profiling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/user-guides/add-app-metrics.html">Adding Application-Level Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/user-guides/ray-tracing.html">Tracing</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-observability/reference/index.html">Reference</a><input class="toctree-checkbox" id="toctree-checkbox-69" name="toctree-checkbox-69" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-69"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/reference/api.html">State API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/reference/cli.html">State CLI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/reference/system-metrics.html">System Metrics</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../ray-contribute/index.html">Developer Guides</a><input class="toctree-checkbox" id="toctree-checkbox-70" name="toctree-checkbox-70" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-70"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-contribute/stability.html">API Stability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-contribute/api-policy.html">API Policy</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-contribute/getting-involved.html">Getting Involved / Contributing</a><input class="toctree-checkbox" id="toctree-checkbox-71" name="toctree-checkbox-71" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-71"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/development.html">Building Ray from Source</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/ci.html">CI Testing Workflow on PRs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/docs.html">Contributing to the Ray Documentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/writing-code-snippets.html">How to write code snippets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/fake-autoscaler.html">Testing Autoscaling Locally</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/testing-tips.html">Tips for testing Ray programs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/debugging.html">Debugging for Ray Developers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/profiling.html">Profiling for Ray Developers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-core/configure.html">Configuring Ray</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-contribute/whitepaper.html">Architecture Whitepapers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ray-references/glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ray-security/index.html">Security</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">ray.rllib.al...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <h1>Source code for ray.rllib.algorithms.algorithm</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">concurrent</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">copy</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">datetime</span><span class="w"> </span><span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">functools</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">gymnasium</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">gym</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">importlib</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">importlib.metadata</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">packaging</span><span class="w"> </span><span class="kn">import</span> <span class="n">version</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pathlib</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pyarrow.fs</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tempfile</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Any</span><span class="p">,</span>
    <span class="n">Callable</span><span class="p">,</span>
    <span class="n">Collection</span><span class="p">,</span>
    <span class="n">DefaultDict</span><span class="p">,</span>
    <span class="n">Dict</span><span class="p">,</span>
    <span class="n">List</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">Set</span><span class="p">,</span>
    <span class="n">Tuple</span><span class="p">,</span>
    <span class="n">Type</span><span class="p">,</span>
    <span class="n">TYPE_CHECKING</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">tree</span>  <span class="c1"># pip install dm_tree</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">ray</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.tune.result</span><span class="w"> </span><span class="kn">import</span> <span class="n">TRAINING_ITERATION</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray._private.usage.usage_lib</span><span class="w"> </span><span class="kn">import</span> <span class="n">TagKey</span><span class="p">,</span> <span class="n">record_extra_usage_tag</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.actor</span><span class="w"> </span><span class="kn">import</span> <span class="n">ActorHandle</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.tune</span><span class="w"> </span><span class="kn">import</span> <span class="n">Checkpoint</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">ray.cloudpickle</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pickle</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.algorithms.algorithm_config</span><span class="w"> </span><span class="kn">import</span> <span class="n">AlgorithmConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.algorithms.registry</span><span class="w"> </span><span class="kn">import</span> <span class="n">ALGORITHMS_CLASS_TO_NAME</span> <span class="k">as</span> <span class="n">ALL_ALGORITHMS</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.algorithms.utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">AggregatorActor</span><span class="p">,</span>
    <span class="n">_get_env_runner_bundles</span><span class="p">,</span>
    <span class="n">_get_offline_eval_runner_bundles</span><span class="p">,</span>
    <span class="n">_get_learner_bundles</span><span class="p">,</span>
    <span class="n">_get_main_process_bundle</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.callbacks.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_callback</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.connectors.agent.obs_preproc</span><span class="w"> </span><span class="kn">import</span> <span class="n">ObsPreprocessorConnector</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.connectors.connector_pipeline_v2</span><span class="w"> </span><span class="kn">import</span> <span class="n">ConnectorPipelineV2</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.core</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">COMPONENT_ENV_RUNNER</span><span class="p">,</span>
    <span class="n">COMPONENT_ENV_TO_MODULE_CONNECTOR</span><span class="p">,</span>
    <span class="n">COMPONENT_EVAL_ENV_RUNNER</span><span class="p">,</span>
    <span class="n">COMPONENT_LEARNER</span><span class="p">,</span>
    <span class="n">COMPONENT_LEARNER_GROUP</span><span class="p">,</span>
    <span class="n">COMPONENT_METRICS_LOGGER</span><span class="p">,</span>
    <span class="n">COMPONENT_MODULE_TO_ENV_CONNECTOR</span><span class="p">,</span>
    <span class="n">COMPONENT_RL_MODULE</span><span class="p">,</span>
    <span class="n">DEFAULT_MODULE_ID</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.core.columns</span><span class="w"> </span><span class="kn">import</span> <span class="n">Columns</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.core.rl_module.multi_rl_module</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">MultiRLModule</span><span class="p">,</span>
    <span class="n">MultiRLModuleSpec</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.core.rl_module</span><span class="w"> </span><span class="kn">import</span> <span class="n">validate_module_id</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.core.rl_module.rl_module</span><span class="w"> </span><span class="kn">import</span> <span class="n">RLModule</span><span class="p">,</span> <span class="n">RLModuleSpec</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.env</span><span class="w"> </span><span class="kn">import</span> <span class="n">INPUT_ENV_SPACES</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.env.env_context</span><span class="w"> </span><span class="kn">import</span> <span class="n">EnvContext</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.env.env_runner</span><span class="w"> </span><span class="kn">import</span> <span class="n">EnvRunner</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.env.env_runner_group</span><span class="w"> </span><span class="kn">import</span> <span class="n">EnvRunnerGroup</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.env.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">_gym_env_creator</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.evaluation.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">collect_episodes</span><span class="p">,</span>
    <span class="n">summarize_episodes</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.execution.rollout_ops</span><span class="w"> </span><span class="kn">import</span> <span class="n">synchronous_parallel_sample</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.offline</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_dataset_and_shards</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.offline.estimators</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">OffPolicyEstimator</span><span class="p">,</span>
    <span class="n">ImportanceSampling</span><span class="p">,</span>
    <span class="n">WeightedImportanceSampling</span><span class="p">,</span>
    <span class="n">DirectMethod</span><span class="p">,</span>
    <span class="n">DoublyRobust</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.offline.offline_evaluator</span><span class="w"> </span><span class="kn">import</span> <span class="n">OfflineEvaluator</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.policy.policy</span><span class="w"> </span><span class="kn">import</span> <span class="n">Policy</span><span class="p">,</span> <span class="n">PolicySpec</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.policy.sample_batch</span><span class="w"> </span><span class="kn">import</span> <span class="n">DEFAULT_POLICY_ID</span><span class="p">,</span> <span class="n">SampleBatch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">deep_update</span><span class="p">,</span> <span class="n">FilterManager</span><span class="p">,</span> <span class="n">force_list</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.utils.actor_manager</span><span class="w"> </span><span class="kn">import</span> <span class="n">FaultTolerantActorManager</span><span class="p">,</span> <span class="n">RemoteCallResults</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.utils.annotations</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">DeveloperAPI</span><span class="p">,</span>
    <span class="n">ExperimentalAPI</span><span class="p">,</span>
    <span class="n">OldAPIStack</span><span class="p">,</span>
    <span class="n">override</span><span class="p">,</span>
    <span class="n">OverrideToImplementCustomLogic</span><span class="p">,</span>
    <span class="n">OverrideToImplementCustomLogic_CallToSuperRecommended</span><span class="p">,</span>
    <span class="n">PublicAPI</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.utils.checkpoints</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Checkpointable</span><span class="p">,</span>
    <span class="n">CHECKPOINT_VERSION</span><span class="p">,</span>
    <span class="n">CHECKPOINT_VERSION_LEARNER_AND_ENV_RUNNER</span><span class="p">,</span>
    <span class="n">get_checkpoint_info</span><span class="p">,</span>
    <span class="n">try_import_msgpack</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.utils.debug</span><span class="w"> </span><span class="kn">import</span> <span class="n">update_global_seed_if_necessary</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.utils.deprecation</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">DEPRECATED_VALUE</span><span class="p">,</span>
    <span class="n">Deprecated</span><span class="p">,</span>
    <span class="n">deprecation_warning</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.utils.error</span><span class="w"> </span><span class="kn">import</span> <span class="n">ERR_MSG_INVALID_ENV_DESCRIPTOR</span><span class="p">,</span> <span class="n">EnvError</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.utils.framework</span><span class="w"> </span><span class="kn">import</span> <span class="n">try_import_tf</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.utils.from_config</span><span class="w"> </span><span class="kn">import</span> <span class="n">from_config</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.utils.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">AGGREGATOR_ACTOR_RESULTS</span><span class="p">,</span>
    <span class="n">ALL_MODULES</span><span class="p">,</span>
    <span class="n">DATASET_NUM_ITERS_EVALUATED</span><span class="p">,</span>
    <span class="n">ENV_RUNNER_RESULTS</span><span class="p">,</span>
    <span class="n">ENV_RUNNER_SAMPLING_TIMER</span><span class="p">,</span>
    <span class="n">EPISODE_LEN_MEAN</span><span class="p">,</span>
    <span class="n">EPISODE_RETURN_MEAN</span><span class="p">,</span>
    <span class="n">EVALUATION_ITERATION_TIMER</span><span class="p">,</span>
    <span class="n">EVALUATION_RESULTS</span><span class="p">,</span>
    <span class="n">FAULT_TOLERANCE_STATS</span><span class="p">,</span>
    <span class="n">LEARNER_RESULTS</span><span class="p">,</span>
    <span class="n">LEARNER_UPDATE_TIMER</span><span class="p">,</span>
    <span class="n">NUM_AGENT_STEPS_SAMPLED</span><span class="p">,</span>
    <span class="n">NUM_AGENT_STEPS_SAMPLED_LIFETIME</span><span class="p">,</span>
    <span class="n">NUM_AGENT_STEPS_SAMPLED_THIS_ITER</span><span class="p">,</span>
    <span class="n">NUM_AGENT_STEPS_TRAINED</span><span class="p">,</span>
    <span class="n">NUM_AGENT_STEPS_TRAINED_LIFETIME</span><span class="p">,</span>
    <span class="n">NUM_ENV_STEPS_SAMPLED</span><span class="p">,</span>
    <span class="n">NUM_ENV_STEPS_SAMPLED_LIFETIME</span><span class="p">,</span>
    <span class="n">NUM_ENV_STEPS_SAMPLED_THIS_ITER</span><span class="p">,</span>
    <span class="n">NUM_ENV_STEPS_SAMPLED_FOR_EVALUATION_THIS_ITER</span><span class="p">,</span>
    <span class="n">NUM_ENV_STEPS_TRAINED</span><span class="p">,</span>
    <span class="n">NUM_ENV_STEPS_TRAINED_LIFETIME</span><span class="p">,</span>
    <span class="n">NUM_EPISODES</span><span class="p">,</span>
    <span class="n">NUM_EPISODES_LIFETIME</span><span class="p">,</span>
    <span class="n">NUM_TRAINING_STEP_CALLS_PER_ITERATION</span><span class="p">,</span>
    <span class="n">OFFLINE_EVAL_RUNNER_RESULTS</span><span class="p">,</span>
    <span class="n">OFFLINE_EVALUATION_ITERATION_TIMER</span><span class="p">,</span>
    <span class="n">RESTORE_ENV_RUNNERS_TIMER</span><span class="p">,</span>
    <span class="n">RESTORE_EVAL_ENV_RUNNERS_TIMER</span><span class="p">,</span>
    <span class="n">RESTORE_OFFLINE_EVAL_RUNNERS_TIMER</span><span class="p">,</span>
    <span class="n">SYNCH_ENV_CONNECTOR_STATES_TIMER</span><span class="p">,</span>
    <span class="n">SYNCH_EVAL_ENV_CONNECTOR_STATES_TIMER</span><span class="p">,</span>
    <span class="n">SYNCH_WORKER_WEIGHTS_TIMER</span><span class="p">,</span>
    <span class="n">TIMERS</span><span class="p">,</span>
    <span class="n">TRAINING_ITERATION_TIMER</span><span class="p">,</span>
    <span class="n">TRAINING_STEP_TIMER</span><span class="p">,</span>
    <span class="n">STEPS_TRAINED_THIS_ITER_COUNTER</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.utils.metrics.learner_info</span><span class="w"> </span><span class="kn">import</span> <span class="n">LEARNER_INFO</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.utils.metrics.metrics_logger</span><span class="w"> </span><span class="kn">import</span> <span class="n">MetricsLogger</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.utils.replay_buffers</span><span class="w"> </span><span class="kn">import</span> <span class="n">MultiAgentReplayBuffer</span><span class="p">,</span> <span class="n">ReplayBuffer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.utils.runners.runner_group</span><span class="w"> </span><span class="kn">import</span> <span class="n">RunnerGroup</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.utils.serialization</span><span class="w"> </span><span class="kn">import</span> <span class="n">deserialize_type</span><span class="p">,</span> <span class="n">NOT_SERIALIZABLE</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.utils.spaces</span><span class="w"> </span><span class="kn">import</span> <span class="n">space_utils</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.utils.typing</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">AgentConnectorDataType</span><span class="p">,</span>
    <span class="n">AgentID</span><span class="p">,</span>
    <span class="n">AgentToModuleMappingFn</span><span class="p">,</span>
    <span class="n">AlgorithmConfigDict</span><span class="p">,</span>
    <span class="n">EnvCreator</span><span class="p">,</span>
    <span class="n">EnvInfoDict</span><span class="p">,</span>
    <span class="n">EnvType</span><span class="p">,</span>
    <span class="n">EpisodeID</span><span class="p">,</span>
    <span class="n">ModuleID</span><span class="p">,</span>
    <span class="n">PartialAlgorithmConfigDict</span><span class="p">,</span>
    <span class="n">PolicyID</span><span class="p">,</span>
    <span class="n">PolicyState</span><span class="p">,</span>
    <span class="n">ResultDict</span><span class="p">,</span>
    <span class="n">SampleBatchType</span><span class="p">,</span>
    <span class="n">ShouldModuleBeUpdatedFn</span><span class="p">,</span>
    <span class="n">StateDict</span><span class="p">,</span>
    <span class="n">TensorStructType</span><span class="p">,</span>
    <span class="n">TensorType</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.train.constants</span><span class="w"> </span><span class="kn">import</span> <span class="n">DEFAULT_STORAGE_PATH</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.tune.execution.placement_groups</span><span class="w"> </span><span class="kn">import</span> <span class="n">PlacementGroupFactory</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.tune.experiment.trial</span><span class="w"> </span><span class="kn">import</span> <span class="n">ExportFormat</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.tune.logger</span><span class="w"> </span><span class="kn">import</span> <span class="n">Logger</span><span class="p">,</span> <span class="n">UnifiedLogger</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.tune.registry</span><span class="w"> </span><span class="kn">import</span> <span class="n">ENV_CREATOR</span><span class="p">,</span> <span class="n">_global_registry</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.tune.resources</span><span class="w"> </span><span class="kn">import</span> <span class="n">Resources</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.tune.trainable</span><span class="w"> </span><span class="kn">import</span> <span class="n">Trainable</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.util</span><span class="w"> </span><span class="kn">import</span> <span class="n">log_once</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.util.timer</span><span class="w"> </span><span class="kn">import</span> <span class="n">_Timer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.tune.registry</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_trainable_cls</span>

<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.core.learner.learner_group</span><span class="w"> </span><span class="kn">import</span> <span class="n">LearnerGroup</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.offline.offline_data</span><span class="w"> </span><span class="kn">import</span> <span class="n">OfflineData</span>

<span class="n">tf1</span><span class="p">,</span> <span class="n">tf</span><span class="p">,</span> <span class="n">tfv</span> <span class="o">=</span> <span class="n">try_import_tf</span><span class="p">()</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<div class="viewcode-block" id="Algorithm">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.html#ray.rllib.algorithms.algorithm.Algorithm">[docs]</a>
<span class="nd">@PublicAPI</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Algorithm</span><span class="p">(</span><span class="n">Checkpointable</span><span class="p">,</span> <span class="n">Trainable</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;An RLlib algorithm responsible for training one or more neural network models.</span>

<span class="sd">    You can write your own Algorithm classes by sub-classing from `Algorithm`</span>
<span class="sd">    or any of its built-in subclasses.</span>
<span class="sd">    Override the `training_step` method to implement your own algorithm logic.</span>
<span class="sd">    Find the various built-in `training_step()` methods for different algorithms in</span>
<span class="sd">    their respective [algo name].py files, for example:</span>
<span class="sd">    `ray.rllib.algorithms.dqn.dqn.py` or `ray.rllib.algorithms.impala.impala.py`.</span>

<span class="sd">    The most important API methods an Algorithm exposes are `train()` for running a</span>
<span class="sd">    single training iteration, `evaluate()` for running a single round of evaluation,</span>
<span class="sd">    `save_to_path()` for creating a checkpoint, and `restore_from_path()` for loading a</span>
<span class="sd">    state from an existing checkpoint.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1">#: The AlgorithmConfig instance of the Algorithm.</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AlgorithmConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1">#: The MetricsLogger instance of the Algorithm. RLlib uses this to log</span>
    <span class="c1">#: metrics from within the `training_step()` method. Users can use it to log</span>
    <span class="c1">#: metrics from within their custom Algorithm-based callbacks.</span>
    <span class="n">metrics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MetricsLogger</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1">#: The `EnvRunnerGroup` of the Algorithm. An `EnvRunnerGroup` is</span>
    <span class="c1">#: composed of a single local `EnvRunner` (see: `self.env_runner`), serving as</span>
    <span class="c1">#: the reference copy of the models to be trained and optionally one or more</span>
    <span class="c1">#: remote `EnvRunners` used to generate training samples from the RL</span>
    <span class="c1">#: environment, in parallel. EnvRunnerGroup is fault-tolerant and elastic. It</span>
    <span class="c1">#: tracks health states for all the managed remote EnvRunner actors. As a</span>
    <span class="c1">#: result, Algorithm should never access the underlying actor handles directly.</span>
    <span class="c1">#: Instead, always access them via all the foreach APIs with assigned IDs of</span>
    <span class="c1">#: the underlying EnvRunners.</span>
    <span class="n">env_runner_group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">EnvRunnerGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1">#: A special EnvRunnerGroup only used for evaluation, not to</span>
    <span class="c1">#: collect training samples.</span>
    <span class="n">eval_env_runner_group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">EnvRunnerGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1">#: The `LearnerGroup` instance of the Algorithm, managing either</span>
    <span class="c1">#: one local `Learner` or one or more remote `Learner` actors. Responsible for</span>
    <span class="c1">#: updating the models from RL environment (episode) data.</span>
    <span class="n">learner_group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;LearnerGroup&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1">#: An optional OfflineData instance, used for offline RL.</span>
    <span class="n">offline_data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;OfflineData&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># Whether to allow unknown top-level config keys.</span>
    <span class="n">_allow_unknown_configs</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># List of top-level keys with value=dict, for which new sub-keys are</span>
    <span class="c1"># allowed to be added to the value dict.</span>
    <span class="n">_allow_unknown_subkeys</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;tf_session_args&quot;</span><span class="p">,</span>
        <span class="s2">&quot;local_tf_session_args&quot;</span><span class="p">,</span>
        <span class="s2">&quot;env_config&quot;</span><span class="p">,</span>
        <span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="s2">&quot;optimizer&quot;</span><span class="p">,</span>
        <span class="s2">&quot;custom_resources_per_env_runner&quot;</span><span class="p">,</span>
        <span class="s2">&quot;custom_resources_per_worker&quot;</span><span class="p">,</span>
        <span class="s2">&quot;evaluation_config&quot;</span><span class="p">,</span>
        <span class="s2">&quot;exploration_config&quot;</span><span class="p">,</span>
        <span class="s2">&quot;replay_buffer_config&quot;</span><span class="p">,</span>
        <span class="s2">&quot;extra_python_environs_for_worker&quot;</span><span class="p">,</span>
        <span class="s2">&quot;input_config&quot;</span><span class="p">,</span>
        <span class="s2">&quot;output_config&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="c1"># List of top level keys with value=dict, for which we always override the</span>
    <span class="c1"># entire value (dict), iff the &quot;type&quot; key in that value dict changes.</span>
    <span class="n">_override_all_subkeys_if_type_changes</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;exploration_config&quot;</span><span class="p">,</span>
        <span class="s2">&quot;replay_buffer_config&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="c1"># List of keys that are always fully overridden if present in any dict or sub-dict</span>
    <span class="n">_override_all_key_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;off_policy_estimation_methods&quot;</span><span class="p">,</span> <span class="s2">&quot;policies&quot;</span><span class="p">]</span>

    <span class="n">_progress_metrics</span> <span class="o">=</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">ENV_RUNNER_RESULTS</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">EPISODE_RETURN_MEAN</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">EVALUATION_RESULTS</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">ENV_RUNNER_RESULTS</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">EPISODE_RETURN_MEAN</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">NUM_ENV_STEPS_SAMPLED_LIFETIME</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">NUM_ENV_STEPS_TRAINED_LIFETIME</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">NUM_EPISODES_LIFETIME</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">ENV_RUNNER_RESULTS</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">EPISODE_LEN_MEAN</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Backward compatibility with old checkpoint system (now through the</span>
    <span class="c1"># `Checkpointable` API).</span>
    <span class="n">METADATA_FILE_NAME</span> <span class="o">=</span> <span class="s2">&quot;rllib_checkpoint.json&quot;</span>
    <span class="n">STATE_FILE_NAME</span> <span class="o">=</span> <span class="s2">&quot;algorithm_state&quot;</span>

<div class="viewcode-block" id="Algorithm.from_checkpoint">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.from_checkpoint.html#ray.rllib.algorithms.algorithm.Algorithm.from_checkpoint">[docs]</a>
    <span class="nd">@classmethod</span>
    <span class="nd">@override</span><span class="p">(</span><span class="n">Checkpointable</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_checkpoint</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Checkpoint</span><span class="p">],</span>
        <span class="n">filesystem</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;pyarrow.fs.FileSystem&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="c1"># @OldAPIStack</span>
        <span class="n">policy_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Collection</span><span class="p">[</span><span class="n">PolicyID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policy_mapping_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">AgentID</span><span class="p">,</span> <span class="n">EpisodeID</span><span class="p">],</span> <span class="n">PolicyID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policies_to_train</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">Collection</span><span class="p">[</span><span class="n">PolicyID</span><span class="p">],</span>
                <span class="n">Callable</span><span class="p">[[</span><span class="n">PolicyID</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SampleBatchType</span><span class="p">]],</span> <span class="nb">bool</span><span class="p">],</span>
            <span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="c1"># deprecated args</span>
        <span class="n">checkpoint</span><span class="o">=</span><span class="n">DEPRECATED_VALUE</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Algorithm&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Creates a new algorithm instance from a given checkpoint.</span>

<span class="sd">        Args:</span>
<span class="sd">            path: The path (str) to the checkpoint directory to use or a Ray Train</span>
<span class="sd">                Checkpoint instance to restore from.</span>
<span class="sd">            filesystem: PyArrow FileSystem to use to access data at the `path`. If not</span>
<span class="sd">                specified, this is inferred from the URI scheme of `path`.</span>
<span class="sd">            policy_ids: Optional list of PolicyIDs to recover. This allows users to</span>
<span class="sd">                restore an Algorithm with only a subset of the originally present</span>
<span class="sd">                Policies.</span>
<span class="sd">            policy_mapping_fn: An optional (updated) policy mapping function to use from</span>
<span class="sd">                here on.</span>
<span class="sd">            policies_to_train: An optional list of policy IDs to be trained or a</span>
<span class="sd">                callable taking PolicyID and SampleBatchType and returning a bool</span>
<span class="sd">                (trainable or not?). If None, will keep the existing setup in place.</span>
<span class="sd">                Policies, whose IDs are not in the list (or for which the callable</span>
<span class="sd">                returns False) will not be updated.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The instantiated Algorithm.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">checkpoint</span> <span class="o">!=</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;Algorithm.from_checkpoint(checkpoint=...)&quot;</span><span class="p">,</span>
                <span class="n">new</span><span class="o">=</span><span class="s2">&quot;Algorithm.from_checkpoint(path=...)&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">checkpoint_info</span> <span class="o">=</span> <span class="n">get_checkpoint_info</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>

        <span class="c1"># New API stack -&gt; Use Checkpointable&#39;s default implementation.</span>
        <span class="k">if</span> <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;checkpoint_version&quot;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">version</span><span class="o">.</span><span class="n">Version</span><span class="p">(</span><span class="s2">&quot;2.0&quot;</span><span class="p">):</span>
            <span class="c1"># `path` is a Checkpoint instance: Translate to directory and continue.</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">Checkpoint</span><span class="p">):</span>
                <span class="n">path</span> <span class="o">=</span> <span class="n">path</span><span class="o">.</span><span class="n">to_directory</span><span class="p">()</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">from_checkpoint</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">filesystem</span><span class="o">=</span><span class="n">filesystem</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Not possible for (v0.1) (algo class and config information missing</span>
        <span class="c1"># or very hard to retrieve).</span>
        <span class="k">elif</span> <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;checkpoint_version&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">version</span><span class="o">.</span><span class="n">Version</span><span class="p">(</span><span class="s2">&quot;0.1&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot restore a v0 checkpoint using `Algorithm.from_checkpoint()`!&quot;</span>
                <span class="s2">&quot;In this case, do the following:</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="s2">&quot;1) Create a new Algorithm object using your original config.</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="s2">&quot;2) Call the `restore()` method of this algo object passing it&quot;</span>
                <span class="s2">&quot; your checkpoint dir or AIR Checkpoint object.&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;checkpoint_version&quot;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">version</span><span class="o">.</span><span class="n">Version</span><span class="p">(</span><span class="s2">&quot;1.0&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`checkpoint_info[&#39;checkpoint_version&#39;]` in `Algorithm.from_checkpoint&quot;</span>
                <span class="s2">&quot;()` must be 1.0 or later! You are using a checkpoint with &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;version v</span><span class="si">{</span><span class="n">checkpoint_info</span><span class="p">[</span><span class="s1">&#39;checkpoint_version&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># This is a msgpack checkpoint.</span>
        <span class="k">if</span> <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;format&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;msgpack&quot;</span><span class="p">:</span>
            <span class="c1"># User did not provide unserializable function with this call</span>
            <span class="c1"># (`policy_mapping_fn`). Note that if `policies_to_train` is None, it</span>
            <span class="c1"># defaults to training all policies (so it&#39;s ok to not provide this here).</span>
            <span class="k">if</span> <span class="n">policy_mapping_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Only DEFAULT_POLICY_ID present in this algorithm, provide default</span>
                <span class="c1"># implementations of these two functions.</span>
                <span class="k">if</span> <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;policy_ids&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="p">{</span><span class="n">DEFAULT_POLICY_ID</span><span class="p">}:</span>
                    <span class="n">policy_mapping_fn</span> <span class="o">=</span> <span class="n">AlgorithmConfig</span><span class="o">.</span><span class="n">DEFAULT_POLICY_MAPPING_FN</span>
                <span class="c1"># Provide meaningful error message.</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;You are trying to restore a multi-agent algorithm from a &quot;</span>
                        <span class="s2">&quot;`msgpack` formatted checkpoint, which do NOT store the &quot;</span>
                        <span class="s2">&quot;`policy_mapping_fn` or `policies_to_train` &quot;</span>
                        <span class="s2">&quot;functions! Make sure that when using the &quot;</span>
                        <span class="s2">&quot;`Algorithm.from_checkpoint()` utility, you also pass the &quot;</span>
                        <span class="s2">&quot;args: `policy_mapping_fn` and `policies_to_train` with your &quot;</span>
                        <span class="s2">&quot;call. You might leave `policies_to_train=None` in case &quot;</span>
                        <span class="s2">&quot;you would like to train all policies anyways.&quot;</span>
                    <span class="p">)</span>

        <span class="n">state</span> <span class="o">=</span> <span class="n">Algorithm</span><span class="o">.</span><span class="n">_checkpoint_info_to_algorithm_state</span><span class="p">(</span>
            <span class="n">checkpoint_info</span><span class="o">=</span><span class="n">checkpoint_info</span><span class="p">,</span>
            <span class="n">policy_ids</span><span class="o">=</span><span class="n">policy_ids</span><span class="p">,</span>
            <span class="n">policy_mapping_fn</span><span class="o">=</span><span class="n">policy_mapping_fn</span><span class="p">,</span>
            <span class="n">policies_to_train</span><span class="o">=</span><span class="n">policies_to_train</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">Algorithm</span><span class="o">.</span><span class="n">from_state</span><span class="p">(</span><span class="n">state</span><span class="p">)</span></div>


<div class="viewcode-block" id="Algorithm.__init__">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.__init__.html#ray.rllib.algorithms.algorithm.Algorithm.__init__">[docs]</a>
    <span class="nd">@PublicAPI</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AlgorithmConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">env</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># deprecated arg</span>
        <span class="n">logger_creator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[],</span> <span class="n">Logger</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes an Algorithm instance.</span>

<span class="sd">        Args:</span>
<span class="sd">            config: Algorithm-specific configuration object.</span>
<span class="sd">            logger_creator: Callable that creates a ray.tune.Logger</span>
<span class="sd">                object. If unspecified, a default logger is created.</span>
<span class="sd">            **kwargs: Arguments passed to the Trainable base class.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Translate possible dict into an AlgorithmConfig object, as well as,</span>
        <span class="c1"># resolving generic config objects into specific ones (e.g. passing</span>
        <span class="c1"># an `AlgorithmConfig` super-class instance into a PPO constructor,</span>
        <span class="c1"># which normally would expect a PPOConfig object).</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">default_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_default_config</span><span class="p">()</span>
            <span class="c1"># `self.get_default_config()` also returned a dict -&gt;</span>
            <span class="c1"># Last resort: Create core AlgorithmConfig from merged dicts.</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">default_config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="k">if</span> <span class="s2">&quot;class&quot;</span> <span class="ow">in</span> <span class="n">config</span><span class="p">:</span>
                    <span class="n">AlgorithmConfig</span><span class="o">.</span><span class="n">from_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">config</span> <span class="o">=</span> <span class="n">AlgorithmConfig</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
                        <span class="n">config_dict</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">merge_algorithm_configs</span><span class="p">(</span>
                            <span class="n">default_config</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="kc">True</span>
                        <span class="p">)</span>
                    <span class="p">)</span>

            <span class="c1"># Default config is an AlgorithmConfig -&gt; update its properties</span>
            <span class="c1"># from the given config dict.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="s2">&quot;class&quot;</span> <span class="ow">in</span> <span class="n">config</span><span class="p">:</span>
                    <span class="n">config</span> <span class="o">=</span> <span class="n">default_config</span><span class="o">.</span><span class="n">from_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">config</span> <span class="o">=</span> <span class="n">default_config</span><span class="o">.</span><span class="n">update_from_dict</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">default_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_default_config</span><span class="p">()</span>
            <span class="c1"># Given AlgorithmConfig is not of the same type as the default config:</span>
            <span class="c1"># This could be the case e.g. if the user is building an algo from a</span>
            <span class="c1"># generic AlgorithmConfig() object.</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">default_config</span><span class="p">)):</span>
                <span class="n">config</span> <span class="o">=</span> <span class="n">default_config</span><span class="o">.</span><span class="n">update_from_dict</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">config</span> <span class="o">=</span> <span class="n">default_config</span><span class="o">.</span><span class="n">from_state</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">get_state</span><span class="p">())</span>

        <span class="c1"># In case this algo is using a generic config (with no algo_class set), set it</span>
        <span class="c1"># here.</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">algo_class</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">config</span><span class="o">.</span><span class="n">algo_class</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">env</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;algo = Algorithm(env=&#39;</span><span class="si">{</span><span class="n">env</span><span class="si">}</span><span class="s2">&#39;, ...)&quot;</span><span class="p">,</span>
                <span class="n">new</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;algo = AlgorithmConfig().environment(&#39;</span><span class="si">{</span><span class="n">env</span><span class="si">}</span><span class="s2">&#39;).build()&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>

        <span class="c1"># Validate and freeze our AlgorithmConfig object (no more changes possible).</span>
        <span class="n">config</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span>
        <span class="n">config</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>

        <span class="c1"># Convert `env` provided in config into a concrete env creator callable, which</span>
        <span class="c1"># takes an EnvContext (config dict) as arg and returning an RLlib supported Env</span>
        <span class="c1"># type (e.g. a gym.Env).</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_env_id</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_creator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_env_id_and_creator</span><span class="p">(</span>
            <span class="n">config</span><span class="o">.</span><span class="n">env</span><span class="p">,</span> <span class="n">config</span>
        <span class="p">)</span>
        <span class="n">env_descr</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_env_id</span><span class="o">.</span><span class="vm">__name__</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_env_id</span><span class="p">,</span> <span class="nb">type</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_env_id</span>
        <span class="p">)</span>

        <span class="c1"># Placeholder for a local replay buffer instance.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_replay_buffer</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Placeholder for our LearnerGroup responsible for updating the RLModule(s).</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;LearnerGroup&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># The Algorithm&#39;s `MetricsLogger` object to collect stats from all its</span>
        <span class="c1"># components (including timers, counters and other stats in its own</span>
        <span class="c1"># `training_step()` and other methods) as well as custom callbacks.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span> <span class="o">=</span> <span class="n">MetricsLogger</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Create a default logger creator if no logger_creator is specified</span>
        <span class="k">if</span> <span class="n">logger_creator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Default logdir prefix containing the agent&#39;s name and the</span>
            <span class="c1"># env id.</span>
            <span class="n">timestr</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">today</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">&quot;%Y-%m-</span><span class="si">%d</span><span class="s2">_%H-%M-%S&quot;</span><span class="p">)</span>
            <span class="n">env_descr_for_dir</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s2">&quot;[/</span><span class="se">\\\\</span><span class="s2">]&quot;</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">env_descr</span><span class="p">))</span>
            <span class="n">logdir_prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">env_descr_for_dir</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">timestr</span><span class="si">}</span><span class="s2">&quot;</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">DEFAULT_STORAGE_PATH</span><span class="p">):</span>
                <span class="c1"># Possible race condition if dir is created several times on</span>
                <span class="c1"># rollout workers</span>
                <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">DEFAULT_STORAGE_PATH</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">logdir</span> <span class="o">=</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="n">logdir_prefix</span><span class="p">,</span> <span class="nb">dir</span><span class="o">=</span><span class="n">DEFAULT_STORAGE_PATH</span><span class="p">)</span>

            <span class="c1"># Allow users to more precisely configure the created logger</span>
            <span class="c1"># via &quot;logger_config.type&quot;.</span>
            <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">logger_config</span> <span class="ow">and</span> <span class="s2">&quot;type&quot;</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">logger_config</span><span class="p">:</span>

                <span class="k">def</span><span class="w"> </span><span class="nf">default_logger_creator</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">                    </span><span class="sd">&quot;&quot;&quot;Creates a custom logger with the default prefix.&quot;&quot;&quot;</span>
                    <span class="n">cfg</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;logger_config&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                    <span class="bp">cls</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;type&quot;</span><span class="p">)</span>
                    <span class="c1"># Provide default for logdir, in case the user does</span>
                    <span class="c1"># not specify this in the &quot;logger_config&quot; dict.</span>
                    <span class="n">logdir_</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;logdir&quot;</span><span class="p">,</span> <span class="n">logdir</span><span class="p">)</span>
                    <span class="k">return</span> <span class="n">from_config</span><span class="p">(</span><span class="bp">cls</span><span class="o">=</span><span class="bp">cls</span><span class="p">,</span> <span class="n">_args</span><span class="o">=</span><span class="p">[</span><span class="n">cfg</span><span class="p">],</span> <span class="n">logdir</span><span class="o">=</span><span class="n">logdir_</span><span class="p">)</span>

            <span class="c1"># If no `type` given, use tune&#39;s UnifiedLogger as last resort.</span>
            <span class="k">else</span><span class="p">:</span>

                <span class="k">def</span><span class="w"> </span><span class="nf">default_logger_creator</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">                    </span><span class="sd">&quot;&quot;&quot;Creates a Unified logger with the default prefix.&quot;&quot;&quot;</span>
                    <span class="k">return</span> <span class="n">UnifiedLogger</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">logdir</span><span class="p">,</span> <span class="n">loggers</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

            <span class="n">logger_creator</span> <span class="o">=</span> <span class="n">default_logger_creator</span>

        <span class="c1"># Metrics-related properties.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="n">_Timer</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_episode_history</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_episodes_to_be_collected</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># The fully qualified AlgorithmConfig used for evaluation</span>
        <span class="c1"># (or None if evaluation not setup).</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AlgorithmConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Evaluation EnvRunnerGroup and metrics last returned by `self.evaluate()`.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">EnvRunnerGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
            <span class="n">logger_creator</span><span class="o">=</span><span class="n">logger_creator</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="Algorithm.get_default_config">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_default_config.html#ray.rllib.algorithms.algorithm.Algorithm.get_default_config">[docs]</a>
    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_default_config</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AlgorithmConfig</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">AlgorithmConfig</span><span class="p">()</span></div>


    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_remote_worker_ids_for_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns a list of remote worker IDs to fetch metrics from.</span>

<span class="sd">        Specific Algorithm implementations can override this method to</span>
<span class="sd">        use a subset of the workers for metrics collection.</span>

<span class="sd">        Returns:</span>
<span class="sd">            List of remote worker IDs to fetch metrics from.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">healthy_worker_ids</span><span class="p">()</span>

<div class="viewcode-block" id="Algorithm.setup">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.setup.html#ray.rllib.algorithms.algorithm.Algorithm.setup">[docs]</a>
    <span class="nd">@OverrideToImplementCustomLogic_CallToSuperRecommended</span>
    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">AlgorithmConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Setup our config: Merge the user-supplied config dict (which could</span>
        <span class="c1"># be a partial config dict) with the class&#39; default.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">AlgorithmConfig</span><span class="p">):</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">PartialAlgorithmConfigDict</span><span class="p">)</span>
            <span class="n">config_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_default_config</span><span class="p">()</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config_obj</span><span class="p">,</span> <span class="n">AlgorithmConfig</span><span class="p">):</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">PartialAlgorithmConfigDict</span><span class="p">)</span>
                <span class="n">config_obj</span> <span class="o">=</span> <span class="n">AlgorithmConfig</span><span class="p">()</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">config_obj</span><span class="p">)</span>
            <span class="n">config_obj</span><span class="o">.</span><span class="n">update_from_dict</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
            <span class="n">config_obj</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_env_id</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config_obj</span>

        <span class="c1"># Set Algorithm&#39;s seed after we have - if necessary - enabled</span>
        <span class="c1"># tf eager-execution.</span>
        <span class="n">update_global_seed_if_necessary</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">framework_str</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_record_usage</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>

        <span class="c1"># Create the callbacks object.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="bp">cls</span><span class="p">()</span> <span class="k">for</span> <span class="bp">cls</span> <span class="ow">in</span> <span class="n">force_list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">callbacks_class</span><span class="p">)]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">callbacks_class</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">log_level</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;WARN&quot;</span><span class="p">,</span> <span class="s2">&quot;ERROR&quot;</span><span class="p">]:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Current log_level is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">log_level</span><span class="si">}</span><span class="s2">. For more information, &quot;</span>
                <span class="s2">&quot;set &#39;log_level&#39;: &#39;INFO&#39; / &#39;DEBUG&#39; or use the -v and &quot;</span>
                <span class="s2">&quot;-vv flags.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">log_level</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;ray.rllib&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">log_level</span><span class="p">)</span>

        <span class="c1"># Create local replay buffer if necessary.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_replay_buffer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_local_replay_buffer_if_necessary</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span>
        <span class="p">)</span>

        <span class="c1"># Create a dict, mapping ActorHandles to sets of open remote</span>
        <span class="c1"># requests (object refs). This way, we keep track, of which actors</span>
        <span class="c1"># inside this Algorithm (e.g. a remote EnvRunner) have</span>
        <span class="c1"># already been sent how many (e.g. `sample()`) requests.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">remote_requests_in_flight</span><span class="p">:</span> <span class="n">DefaultDict</span><span class="p">[</span>
            <span class="n">ActorHandle</span><span class="p">,</span> <span class="n">Set</span><span class="p">[</span><span class="n">ray</span><span class="o">.</span><span class="n">ObjectRef</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">set</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">EnvRunnerGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># In case there is no local EnvRunner anymore, we need to handle connector</span>
        <span class="c1"># pipelines directly here.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spaces</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env_to_module_connector</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ConnectorPipelineV2</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module_to_env_connector</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ConnectorPipelineV2</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Offline RL settings.</span>
        <span class="n">input_evaluation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;input_evaluation&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">input_evaluation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">input_evaluation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="n">ope_dict</span> <span class="o">=</span> <span class="p">{</span><span class="nb">str</span><span class="p">(</span><span class="n">ope</span><span class="p">):</span> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="n">ope</span><span class="p">}</span> <span class="k">for</span> <span class="n">ope</span> <span class="ow">in</span> <span class="n">input_evaluation</span><span class="p">}</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;config.input_evaluation=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">input_evaluation</span><span class="p">),</span>
                <span class="n">new</span><span class="o">=</span><span class="s2">&quot;config.evaluation(evaluation_config=config.overrides(&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;off_policy_estimation_methods=</span><span class="si">{</span><span class="n">ope_dict</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="s2">&quot;))&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Running OPE during training is not recommended.&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">off_policy_estimation_methods</span> <span class="o">=</span> <span class="n">ope_dict</span>

        <span class="c1"># If an input path is available and we are on the new API stack generate</span>
        <span class="c1"># an `OfflineData` instance.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_offline</span><span class="p">:</span>
            <span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.offline.offline_data</span><span class="w"> </span><span class="kn">import</span> <span class="n">OfflineData</span>

            <span class="c1"># Use either user-provided `OfflineData` class or RLlib&#39;s default.</span>
            <span class="n">offline_data_class</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">offline_data_class</span> <span class="ow">or</span> <span class="n">OfflineData</span>
            <span class="c1"># Build the `OfflineData` class.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">offline_data</span> <span class="o">=</span> <span class="n">offline_data_class</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
        <span class="c1"># Otherwise set the attribute to `None`.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">offline_data</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_online</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="c1"># Create a set of env runner actors via a EnvRunnerGroup.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span> <span class="o">=</span> <span class="n">EnvRunnerGroup</span><span class="p">(</span>
                <span class="n">env_creator</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_creator</span><span class="p">,</span>
                <span class="n">validate_env</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">validate_env</span><span class="p">,</span>
                <span class="n">default_policy_class</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">get_default_policy_class</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">),</span>
                <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
                <span class="c1"># New API stack: User decides whether to create local env runner.</span>
                <span class="c1"># Old API stack: Always create local EnvRunner.</span>
                <span class="n">local_env_runner</span><span class="o">=</span><span class="p">(</span>
                    <span class="kc">True</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span>
                    <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">create_local_env_runner</span>
                <span class="p">),</span>
                <span class="n">logdir</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">logdir</span><span class="p">,</span>
                <span class="n">tune_trial_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trial_id</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Compile, validate, and freeze an evaluation config.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get_evaluation_config_object</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>

        <span class="c1"># Evaluation EnvRunnerGroup setup.</span>
        <span class="c1"># User would like to setup a separate evaluation worker set.</span>
        <span class="c1"># Note: We skip EnvRunnerGroup creation if we need to do offline evaluation.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_should_create_evaluation_env_runners</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="p">):</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">env_creator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_env_id_and_creator</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">env</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span>
            <span class="p">)</span>

            <span class="c1"># Create a separate evaluation worker set for evaluation.</span>
            <span class="c1"># If evaluation_num_env_runners=0, use the evaluation set&#39;s local</span>
            <span class="c1"># worker for evaluation, otherwise, use its remote workers</span>
            <span class="c1"># (parallelized evaluation).</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="p">:</span> <span class="n">EnvRunnerGroup</span> <span class="o">=</span> <span class="n">EnvRunnerGroup</span><span class="p">(</span>
                <span class="n">env_creator</span><span class="o">=</span><span class="n">env_creator</span><span class="p">,</span>
                <span class="n">validate_env</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">default_policy_class</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">get_default_policy_class</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">),</span>
                <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="p">,</span>
                <span class="n">logdir</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">logdir</span><span class="p">,</span>
                <span class="n">tune_trial_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trial_id</span><span class="p">,</span>
                <span class="c1"># New API stack: User decides whether to create local env runner.</span>
                <span class="c1"># Old API stack: Always create local EnvRunner.</span>
                <span class="n">local_env_runner</span><span class="o">=</span><span class="p">(</span>
                    <span class="kc">True</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span>
                    <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">create_local_env_runner</span>
                <span class="p">),</span>
                <span class="n">pg_offset</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_env_runners</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">spaces</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">get_spaces</span><span class="p">()</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">spaces</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">get_spaces</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">spaces</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_to_module_connector</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">build_env_to_module_connector</span><span class="p">(</span>
                <span class="n">spaces</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">spaces</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">module_to_env_connector</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">build_module_to_env_connector</span><span class="p">(</span>
                <span class="n">spaces</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">spaces</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_dataset</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">off_policy_estimation_methods</span>
            <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">ope_split_batch_by_episode</span>
        <span class="p">):</span>
            <span class="c1"># the num worker is set to 0 to avoid creating shards. The dataset will not</span>
            <span class="c1"># be repartioned to num_workers blocks.</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Creating evaluation dataset ...&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_dataset</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_dataset_and_shards</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span>
            <span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Evaluation dataset created&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reward_estimators</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">OffPolicyEstimator</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">ope_types</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;is&quot;</span><span class="p">:</span> <span class="n">ImportanceSampling</span><span class="p">,</span>
            <span class="s2">&quot;wis&quot;</span><span class="p">:</span> <span class="n">WeightedImportanceSampling</span><span class="p">,</span>
            <span class="s2">&quot;dm&quot;</span><span class="p">:</span> <span class="n">DirectMethod</span><span class="p">,</span>
            <span class="s2">&quot;dr&quot;</span><span class="p">:</span> <span class="n">DoublyRobust</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">method_config</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">off_policy_estimation_methods</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">method_type</span> <span class="o">=</span> <span class="n">method_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;type&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">method_type</span> <span class="ow">in</span> <span class="n">ope_types</span><span class="p">:</span>
                <span class="n">deprecation_warning</span><span class="p">(</span>
                    <span class="n">old</span><span class="o">=</span><span class="n">method_type</span><span class="p">,</span>
                    <span class="n">new</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">ope_types</span><span class="p">[</span><span class="n">method_type</span><span class="p">]),</span>
                    <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">method_type</span> <span class="o">=</span> <span class="n">ope_types</span><span class="p">[</span><span class="n">method_type</span><span class="p">]</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">method_type</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Trying to import from string: &quot;</span> <span class="o">+</span> <span class="n">method_type</span><span class="p">)</span>
                <span class="n">mod</span><span class="p">,</span> <span class="n">obj</span> <span class="o">=</span> <span class="n">method_type</span><span class="o">.</span><span class="n">rsplit</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">mod</span> <span class="o">=</span> <span class="n">importlib</span><span class="o">.</span><span class="n">import_module</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>
                <span class="n">method_type</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">obj</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">method_type</span><span class="p">,</span> <span class="nb">type</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span>
                <span class="n">method_type</span><span class="p">,</span> <span class="n">OfflineEvaluator</span>
            <span class="p">):</span>
                <span class="c1"># TODO(kourosh) : Add an integration test for all these</span>
                <span class="c1"># offline evaluators.</span>
                <span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_policy</span><span class="p">()</span>
                <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">method_type</span><span class="p">,</span> <span class="n">OffPolicyEstimator</span><span class="p">):</span>
                    <span class="n">method_config</span><span class="p">[</span><span class="s2">&quot;gamma&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">gamma</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">reward_estimators</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">method_type</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="o">**</span><span class="n">method_config</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Unknown off_policy_estimation type: </span><span class="si">{</span><span class="n">method_type</span><span class="si">}</span><span class="s2">! Must be &quot;</span>
                    <span class="s2">&quot;either a class path or a sub-class of ray.rllib.&quot;</span>
                    <span class="s2">&quot;offline.offline_evaluator::OfflineEvaluator&quot;</span>
                <span class="p">)</span>
            <span class="c1"># TODO (Rohan138): Refactor this and remove deprecated methods</span>
            <span class="c1"># Need to add back method_type in case Algorithm is restored from checkpoint</span>
            <span class="n">method_config</span><span class="p">[</span><span class="s2">&quot;type&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">method_type</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_rl_module_and_learner</span><span class="p">:</span>
            <span class="n">spaces</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">INPUT_ENV_SPACES</span><span class="p">:</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">}</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="p">:</span>
                <span class="n">spaces</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spaces</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="p">:</span>
                <span class="n">spaces</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">get_spaces</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">spaces</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                    <span class="p">{</span>
                        <span class="n">DEFAULT_MODULE_ID</span><span class="p">:</span> <span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span>
                        <span class="p">),</span>
                    <span class="p">}</span>
                <span class="p">)</span>

            <span class="n">module_spec</span><span class="p">:</span> <span class="n">MultiRLModuleSpec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get_multi_rl_module_spec</span><span class="p">(</span>
                <span class="n">spaces</span><span class="o">=</span><span class="n">spaces</span><span class="p">,</span>
                <span class="n">inference_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">build_learner_group</span><span class="p">(</span>
                <span class="n">rl_module_spec</span><span class="o">=</span><span class="n">module_spec</span>
            <span class="p">)</span>

            <span class="c1"># Check if there are modules to load from the `module_spec`.</span>
            <span class="n">rl_module_ckpt_dirs</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="n">multi_rl_module_ckpt_dir</span> <span class="o">=</span> <span class="n">module_spec</span><span class="o">.</span><span class="n">load_state_path</span>
            <span class="n">modules_to_load</span> <span class="o">=</span> <span class="n">module_spec</span><span class="o">.</span><span class="n">modules_to_load</span>
            <span class="k">for</span> <span class="n">module_id</span><span class="p">,</span> <span class="n">sub_module_spec</span> <span class="ow">in</span> <span class="n">module_spec</span><span class="o">.</span><span class="n">rl_module_specs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">sub_module_spec</span><span class="o">.</span><span class="n">load_state_path</span><span class="p">:</span>
                    <span class="n">rl_module_ckpt_dirs</span><span class="p">[</span><span class="n">module_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">sub_module_spec</span><span class="o">.</span><span class="n">load_state_path</span>
            <span class="k">if</span> <span class="n">multi_rl_module_ckpt_dir</span> <span class="ow">or</span> <span class="n">rl_module_ckpt_dirs</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">load_module_state</span><span class="p">(</span>
                    <span class="n">multi_rl_module_ckpt_dir</span><span class="o">=</span><span class="n">multi_rl_module_ckpt_dir</span><span class="p">,</span>
                    <span class="n">modules_to_load</span><span class="o">=</span><span class="n">modules_to_load</span><span class="p">,</span>
                    <span class="n">rl_module_ckpt_dirs</span><span class="o">=</span><span class="n">rl_module_ckpt_dirs</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="c1"># Sync the weights from the learner group to the EnvRunners.</span>
            <span class="n">rl_module_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span>
                <span class="n">components</span><span class="o">=</span><span class="n">COMPONENT_LEARNER</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span> <span class="n">COMPONENT_RL_MODULE</span><span class="p">,</span>
                <span class="n">inference_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)[</span><span class="n">COMPONENT_LEARNER</span><span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">sync_env_runner_states</span><span class="p">(</span>
                    <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
                    <span class="n">env_steps_sampled</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span>
                        <span class="p">(</span><span class="n">ENV_RUNNER_RESULTS</span><span class="p">,</span> <span class="n">NUM_ENV_STEPS_SAMPLED_LIFETIME</span><span class="p">),</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span>
                    <span class="p">),</span>
                    <span class="n">rl_module_state</span><span class="o">=</span><span class="n">rl_module_state</span><span class="p">,</span>
                    <span class="n">env_to_module</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_to_module_connector</span><span class="p">,</span>
                    <span class="n">module_to_env</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">module_to_env_connector</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">sync_env_runner_states</span><span class="p">(</span>
                    <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="p">,</span>
                    <span class="n">env_steps_sampled</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span>
                        <span class="p">(</span><span class="n">ENV_RUNNER_RESULTS</span><span class="p">,</span> <span class="n">NUM_ENV_STEPS_SAMPLED_LIFETIME</span><span class="p">),</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span>
                    <span class="p">),</span>
                    <span class="n">rl_module_state</span><span class="o">=</span><span class="n">rl_module_state</span><span class="p">,</span>
                    <span class="n">env_to_module</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_to_module_connector</span><span class="p">,</span>
                    <span class="n">module_to_env</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">module_to_env_connector</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="c1"># TODO (simon): Update modules in DataWorkers.</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">offline_data</span><span class="p">:</span>
                <span class="c1"># If the learners are remote we need to provide specific</span>
                <span class="c1"># information and the learner&#39;s actor handles.</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">is_remote</span><span class="p">:</span>
                    <span class="c1"># If learners run on different nodes, locality hints help</span>
                    <span class="c1"># to use the nearest learner in the workers that do the</span>
                    <span class="c1"># data preprocessing.</span>
                    <span class="n">learner_node_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">foreach_learner</span><span class="p">(</span>
                        <span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">ray</span><span class="o">.</span><span class="n">get_runtime_context</span><span class="p">()</span><span class="o">.</span><span class="n">get_node_id</span><span class="p">()</span>
                    <span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">offline_data</span><span class="o">.</span><span class="n">locality_hints</span> <span class="o">=</span> <span class="p">[</span>
                        <span class="n">node_id</span><span class="o">.</span><span class="n">get</span><span class="p">()</span> <span class="k">for</span> <span class="n">node_id</span> <span class="ow">in</span> <span class="n">learner_node_ids</span>
                    <span class="p">]</span>
                    <span class="c1"># Provide the actor handles for the learners for module</span>
                    <span class="c1"># updating during preprocessing.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">offline_data</span><span class="o">.</span><span class="n">learner_handles</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">_workers</span>
                <span class="c1"># Otherwise we can simply pass in the local learner.</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">offline_data</span><span class="o">.</span><span class="n">learner_handles</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">_learner</span><span class="p">]</span>
                <span class="c1"># TODO (simon, sven): Replace these set-some-object&#39;s-attributes-</span>
                <span class="c1"># directly? We should find some solution for this in the future, an API,</span>
                <span class="c1"># or setting these in the OfflineData constructor?</span>
                <span class="c1"># Provide the module_spec. Note, in the remote case this is needed</span>
                <span class="c1"># because the learner module cannot be copied, but must be built.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">offline_data</span><span class="o">.</span><span class="n">module_spec</span> <span class="o">=</span> <span class="n">module_spec</span>
                <span class="c1"># Provide the `OfflineData` instance with space information. It might</span>
                <span class="c1"># need it for reading recorded experiences.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">offline_data</span><span class="o">.</span><span class="n">spaces</span> <span class="o">=</span> <span class="n">spaces</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_should_create_offline_evaluation_runners</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="p">):</span>
                <span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.offline.offline_evaluation_runner_group</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
                    <span class="n">OfflineEvaluationRunnerGroup</span><span class="p">,</span>
                <span class="p">)</span>

                <span class="c1"># If no inference-only `RLModule` should be used in offline evaluation,</span>
                <span class="c1"># get the complete learner module.</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">offline_eval_rl_module_inference_only</span><span class="p">:</span>
                    <span class="n">rl_module_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span>
                        <span class="n">components</span><span class="o">=</span><span class="n">COMPONENT_LEARNER</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span> <span class="n">COMPONENT_RL_MODULE</span><span class="p">,</span>
                        <span class="n">inference_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="p">)[</span><span class="n">COMPONENT_LEARNER</span><span class="p">]</span>
                <span class="c1"># Create the offline evaluation runner group.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">offline_eval_runner_group</span><span class="p">:</span> <span class="n">OfflineEvaluationRunnerGroup</span> <span class="o">=</span> <span class="n">OfflineEvaluationRunnerGroup</span><span class="p">(</span>
                    <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="p">,</span>
                    <span class="c1"># Do not create a local runner such that the dataset can be split.</span>
                    <span class="n">local_runner</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="c1"># Provide the `RLModule`&#39;s state for the `OfflinePreLearner`s.</span>
                    <span class="n">module_state</span><span class="o">=</span><span class="n">rl_module_state</span><span class="p">[</span><span class="n">COMPONENT_RL_MODULE</span><span class="p">],</span>
                    <span class="n">module_spec</span><span class="o">=</span><span class="n">module_spec</span><span class="p">,</span>
                    <span class="c1"># Note, even if no environment is run, the `MultiRLModule` needs</span>
                    <span class="c1"># spaces to construct the policy network.</span>
                    <span class="n">spaces</span><span class="o">=</span><span class="n">spaces</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="c1"># Create an Aggregator actor set, if necessary.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_aggregator_actor_manager</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_rl_module_and_learner</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_aggregator_actors_per_learner</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="p">):</span>
            <span class="n">rl_module_spec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get_multi_rl_module_spec</span><span class="p">(</span>
                <span class="n">spaces</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">spaces</span><span class="p">,</span>
                <span class="n">inference_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">agg_cls</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span>
                <span class="n">num_cpus</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">max_restarts</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)(</span><span class="n">AggregatorActor</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_aggregator_actor_manager</span> <span class="o">=</span> <span class="n">FaultTolerantActorManager</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="n">agg_cls</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">rl_module_spec</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span>
                        <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_learners</span> <span class="ow">or</span> <span class="mi">1</span><span class="p">)</span>
                        <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_aggregator_actors_per_learner</span>
                    <span class="p">)</span>
                <span class="p">],</span>
                <span class="n">max_remote_requests_in_flight_per_actor</span><span class="o">=</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">max_requests_in_flight_per_aggregator_actor</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="c1"># Get the devices of each learner.</span>
            <span class="n">learner_locations</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
                <span class="nb">enumerate</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">foreach_learner</span><span class="p">(</span>
                        <span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">_learner</span><span class="p">:</span> <span class="p">(</span><span class="n">_learner</span><span class="o">.</span><span class="n">node</span><span class="p">,</span> <span class="n">_learner</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="c1"># Get the devices of each AggregatorActor.</span>
            <span class="n">aggregator_locations</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
                <span class="nb">enumerate</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_aggregator_actor_manager</span><span class="o">.</span><span class="n">foreach_actor</span><span class="p">(</span>
                        <span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">actor</span><span class="p">:</span> <span class="p">(</span><span class="n">actor</span><span class="o">.</span><span class="n">_node</span><span class="p">,</span> <span class="n">actor</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_aggregator_actor_to_learner</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">agg_idx</span><span class="p">,</span> <span class="n">aggregator_location</span> <span class="ow">in</span> <span class="n">aggregator_locations</span><span class="p">:</span>
                <span class="n">aggregator_location</span> <span class="o">=</span> <span class="n">aggregator_location</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
                <span class="k">for</span> <span class="n">learner_idx</span><span class="p">,</span> <span class="n">learner_location</span> <span class="ow">in</span> <span class="n">learner_locations</span><span class="p">:</span>
                    <span class="c1"># TODO (sven): Activate full comparison (including device) when Ray</span>
                    <span class="c1">#  has figured out GPU pre-loading.</span>
                    <span class="k">if</span> <span class="n">learner_location</span><span class="o">.</span><span class="n">get</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">aggregator_location</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                        <span class="c1"># Round-robin, in case all Learners are on same device/node.</span>
                        <span class="n">learner_locations</span> <span class="o">=</span> <span class="n">learner_locations</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">[</span>
                            <span class="n">learner_locations</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                        <span class="p">]</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_aggregator_actor_to_learner</span><span class="p">[</span><span class="n">agg_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">learner_idx</span>
                        <span class="k">break</span>
                <span class="k">if</span> <span class="n">agg_idx</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_aggregator_actor_to_learner</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s2">&quot;No Learner worker found that matches aggregation worker &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;#</span><span class="si">{</span><span class="n">agg_idx</span><span class="si">}</span><span class="s2">&#39;s node (</span><span class="si">{</span><span class="n">aggregator_location</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">) and device &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="n">aggregator_location</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">)! The Learner workers&#39; locations &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;are </span><span class="si">{</span><span class="n">learner_locations</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="p">)</span>

            <span class="c1"># Make sure, each Learner index is mapped to from at least one</span>
            <span class="c1"># AggregatorActor.</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span>
                <span class="n">learner_idx</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_aggregator_actor_to_learner</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
                <span class="k">for</span> <span class="n">learner_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_learners</span> <span class="ow">or</span> <span class="mi">1</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;Some Learner indices are not mapped to from any AggregatorActors! &quot;</span>
                    <span class="s2">&quot;Final AggregatorActor idx -&gt; Learner idx mapping is: &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_aggregator_actor_to_learner</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

        <span class="c1"># Run `on_algorithm_init` callback after initialization is done.</span>
        <span class="n">make_callback</span><span class="p">(</span>
            <span class="s2">&quot;on_algorithm_init&quot;</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">callbacks_on_algorithm_init</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
                <span class="n">algorithm</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">metrics_logger</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="Algorithm.get_default_policy_class">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_default_policy_class.html#ray.rllib.algorithms.algorithm.Algorithm.get_default_policy_class">[docs]</a>
    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_default_policy_class</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">AlgorithmConfig</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Type</span><span class="p">[</span><span class="n">Policy</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns a default Policy class to use, given a config.</span>

<span class="sd">        This class will be used by an Algorithm in case</span>
<span class="sd">        the policy class is not provided by the user in any single- or</span>
<span class="sd">        multi-agent PolicySpec.</span>

<span class="sd">        Note: This method is ignored when the RLModule API is enabled.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="Algorithm.step">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.step.html#ray.rllib.algorithms.algorithm.Algorithm.step">[docs]</a>
    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ResultDict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Implements the main `Algorithm.train()` logic.</span>

<span class="sd">        Takes n attempts to perform a single training step. Thereby</span>
<span class="sd">        catches RayErrors resulting from worker failures. After n attempts,</span>
<span class="sd">        fails gracefully.</span>

<span class="sd">        Override this method in your Algorithm sub-classes if you would like to</span>
<span class="sd">        handle worker failures yourself.</span>
<span class="sd">        Otherwise, override only `training_step()` to implement the core</span>
<span class="sd">        algorithm logic.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The results dict with stats/infos on sampling, training,</span>
<span class="sd">            and - if required - evaluation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Do we have to run `self.evaluate()` this iteration?</span>
        <span class="c1"># `self.iteration` gets incremented after this function returns,</span>
        <span class="c1"># meaning that e.g. the first time this function is called,</span>
        <span class="c1"># self.iteration will be 0.</span>
        <span class="n">evaluate_this_iter</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_interval</span>
            <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_interval</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">)</span>

        <span class="n">evaluate_offline_this_iter</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">offline_evaluation_interval</span>
            <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">offline_evaluation_interval</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">)</span>

        <span class="c1"># Results dict for training (and if appolicable: evaluation).</span>
        <span class="n">eval_results</span><span class="p">:</span> <span class="n">ResultDict</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Parallel eval + training: Kick off evaluation-loop and parallel train() call.</span>
        <span class="k">if</span> <span class="n">evaluate_this_iter</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_parallel_to_training</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">offline_evaluation_parallel_to_training</span>
        <span class="p">):</span>
            <span class="p">(</span>
                <span class="n">train_results</span><span class="p">,</span>
                <span class="n">eval_results</span><span class="p">,</span>
                <span class="n">train_iter_ctx</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_one_training_iteration_and_evaluation_in_parallel</span><span class="p">()</span>

        <span class="c1"># - No evaluation necessary, just run the next training iteration.</span>
        <span class="c1"># - We have to evaluate in this training iteration, but no parallelism -&gt;</span>
        <span class="c1">#   evaluate after the training iteration is entirely done.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
                <span class="n">train_results</span><span class="p">,</span> <span class="n">train_iter_ctx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_one_training_iteration</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="p">(</span>
                    <span class="n">train_results</span><span class="p">,</span>
                    <span class="n">train_iter_ctx</span><span class="p">,</span>
                <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_one_training_iteration_old_api_stack</span><span class="p">()</span>

        <span class="c1"># Sequential: Train (already done above), then evaluate.</span>
        <span class="k">if</span> <span class="n">evaluate_this_iter</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_parallel_to_training</span><span class="p">:</span>
            <span class="n">eval_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_one_evaluation</span><span class="p">(</span><span class="n">parallel_train_future</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">evaluate_offline_this_iter</span><span class="p">:</span>
            <span class="n">offline_eval_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_one_offline_evaluation</span><span class="p">()</span>
            <span class="c1"># If we already have online evaluation results merge the offline</span>
            <span class="c1"># evaluation results.</span>
            <span class="k">if</span> <span class="n">eval_results</span><span class="p">:</span>
                <span class="n">eval_results</span><span class="p">[</span><span class="n">EVALUATION_RESULTS</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                    <span class="n">offline_eval_results</span><span class="p">[</span><span class="n">EVALUATION_RESULTS</span><span class="p">]</span>
                <span class="p">)</span>
            <span class="c1"># Otherwise, just assign.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">eval_results</span> <span class="o">=</span> <span class="n">offline_eval_results</span>

        <span class="c1"># Sync EnvRunner workers.</span>
        <span class="c1"># TODO (sven): For the new API stack, the common execution pattern for any algo</span>
        <span class="c1">#  should be: [sample + get_metrics + get_state] -&gt; send all these in one remote</span>
        <span class="c1">#  call down to `training_step` (where episodes are sent as ray object</span>
        <span class="c1">#  references). Then distribute the episode refs to the learners, store metrics</span>
        <span class="c1">#  in special key in result dict and perform the connector merge/broadcast</span>
        <span class="c1">#  inside the `training_step` as well. See the new IMPALA for an example.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_dont_auto_sync_env_runner_states</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span>
            <span class="p">):</span>
                <span class="c1"># Synchronize EnvToModule and ModuleToEnv connector states and broadcast</span>
                <span class="c1"># new states back to all EnvRunners.</span>
                <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">log_time</span><span class="p">((</span><span class="n">TIMERS</span><span class="p">,</span> <span class="n">SYNCH_ENV_CONNECTOR_STATES_TIMER</span><span class="p">)):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">sync_env_runner_states</span><span class="p">(</span>
                        <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
                        <span class="n">env_to_module</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_to_module_connector</span><span class="p">,</span>
                        <span class="n">module_to_env</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">module_to_env_connector</span><span class="p">,</span>
                    <span class="p">)</span>
            <span class="c1"># Compile final ResultDict from `train_results` and `eval_results`. Note</span>
            <span class="c1"># that, as opposed to the old API stack, EnvRunner stats should already be</span>
            <span class="c1"># in `train_results` and `eval_results`.</span>
            <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compile_iteration_results</span><span class="p">(</span>
                <span class="n">train_results</span><span class="o">=</span><span class="n">train_results</span><span class="p">,</span>
                <span class="n">eval_results</span><span class="o">=</span><span class="n">eval_results</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sync_filters_if_needed</span><span class="p">(</span>
                <span class="n">central_worker</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span><span class="p">,</span>
                <span class="n">workers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="p">,</span>
                <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># Get EnvRunner metrics and compile them into results.</span>
            <span class="n">episodes_this_iter</span> <span class="o">=</span> <span class="n">collect_episodes</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_remote_worker_ids_for_metrics</span><span class="p">(),</span>
                <span class="n">timeout_seconds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">metrics_episode_collection_timeout_s</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compile_iteration_results_old_api_stack</span><span class="p">(</span>
                <span class="n">episodes_this_iter</span><span class="o">=</span><span class="n">episodes_this_iter</span><span class="p">,</span>
                <span class="n">step_ctx</span><span class="o">=</span><span class="n">train_iter_ctx</span><span class="p">,</span>
                <span class="n">iteration_results</span><span class="o">=</span><span class="p">{</span><span class="o">**</span><span class="n">train_results</span><span class="p">,</span> <span class="o">**</span><span class="n">eval_results</span><span class="p">},</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">results</span></div>


<div class="viewcode-block" id="Algorithm.evaluate_offline">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.evaluate_offline.html#ray.rllib.algorithms.algorithm.Algorithm.evaluate_offline">[docs]</a>
    <span class="nd">@PublicAPI</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">evaluate_offline</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Evaluates current policy offline under `evaluation_config` settings.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A ResultDict only containing the offline evaluation results from the current</span>
<span class="sd">            iteration.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># First synchronize weights.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">offline_eval_runner_group</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">(</span>
            <span class="n">from_worker_or_learner_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="p">,</span>
            <span class="n">inference_only</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">offline_eval_rl_module_inference_only</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># TODO (simon): Check, how we can sync without a local runner. Also,</span>
        <span class="c1"># connectors are in the data pipeline not directly in the runner applied.</span>
        <span class="c1"># if self.config.broadcast_offline_eval_runner_states:</span>
        <span class="c1">#     # TODO (simon): Create offline equivalent.</span>
        <span class="c1">#     with self.metrics.log_time(TIMERS, SYNCH_EVAL_ENV_CONNECTOR_STATES_TIMER):</span>
        <span class="c1">#         self.offline_eval_runner_group.sync_runner_states(</span>
        <span class="c1">#             from_runner=</span>
        <span class="c1">#         )</span>

        <span class="n">make_callback</span><span class="p">(</span>
            <span class="s2">&quot;on_evaluate_offline_start&quot;</span><span class="p">,</span>
            <span class="n">callbacks_objects</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="p">,</span>
            <span class="n">callbacks_functions</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">callbacks_on_evaluate_offline_start</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">metrics_logger</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="c1"># Evaluate with fixed duration.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_evaluate_offline_with_fixed_duration</span><span class="p">()</span>
        <span class="c1"># Reduce the evaluation results.</span>
        <span class="n">eval_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span>
            <span class="p">(</span><span class="s2">&quot;EVALUATION_RESULTS&quot;</span><span class="p">,</span> <span class="s2">&quot;OFFLINE_EVAL_RUNNER_RESULTS&quot;</span><span class="p">),</span> <span class="n">default</span><span class="o">=</span><span class="p">{}</span>
        <span class="p">)</span>

        <span class="c1"># Trigger `on_evaluate_offline_end` callback.</span>
        <span class="n">make_callback</span><span class="p">(</span>
            <span class="s2">&quot;on_evaluate_offline_end&quot;</span><span class="p">,</span>
            <span class="n">callbacks_objects</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="p">,</span>
            <span class="n">callbacks_functions</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">callbacks_on_evaluate_offline_end</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
                <span class="n">algorithm</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">metrics_logger</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">,</span>
                <span class="n">evaluation_metrics</span><span class="o">=</span><span class="n">eval_results</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>

        <span class="c1"># Also return the results here for convenience.</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">EVALUATION_RESULTS</span><span class="p">:</span> <span class="p">{</span><span class="n">OFFLINE_EVAL_RUNNER_RESULTS</span><span class="p">:</span> <span class="n">eval_results</span><span class="p">}}</span></div>


<div class="viewcode-block" id="Algorithm.evaluate">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.evaluate.html#ray.rllib.algorithms.algorithm.Algorithm.evaluate">[docs]</a>
    <span class="nd">@PublicAPI</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">evaluate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">parallel_train_future</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">concurrent</span><span class="o">.</span><span class="n">futures</span><span class="o">.</span><span class="n">ThreadPoolExecutor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ResultDict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Evaluates current policy under `evaluation_config` settings.</span>

<span class="sd">        Args:</span>
<span class="sd">            parallel_train_future: In case, we are training and avaluating in parallel,</span>
<span class="sd">                this arg carries the currently running ThreadPoolExecutor object that</span>
<span class="sd">                runs the training iteration. Use `parallel_train_future.done()` to</span>
<span class="sd">                check, whether the parallel training job has completed and</span>
<span class="sd">                `parallel_train_future.result()` to get its return values.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A ResultDict only containing the evaluation results from the current</span>
<span class="sd">            iteration.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Call the `_before_evaluate` hook.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_before_evaluate</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_offline_evaluation_old_api_stack</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">healthy_env_runner_ids</span><span class="p">()</span>
            <span class="p">):</span>
                <span class="c1"># TODO (sven): Replace this with a new ActorManager API:</span>
                <span class="c1">#  try_remote_request_till_success(&quot;get_state&quot;) -&gt; tuple(int,</span>
                <span class="c1">#  remoteresult)</span>
                <span class="n">weights_src</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">_worker_manager</span><span class="o">.</span><span class="n">_actors</span><span class="p">[</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">healthy_env_runner_ids</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
                <span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">weights_src</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">weights_src</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner</span>

        <span class="c1"># Sync weights to the evaluation EnvRunners.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">(</span>
                <span class="n">from_worker_or_learner_group</span><span class="o">=</span><span class="n">weights_src</span><span class="p">,</span>
                <span class="n">inference_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># Merge (eval) EnvRunner states and broadcast the merged state back</span>
            <span class="c1"># to the remote (eval) EnvRunner actors.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">broadcast_env_runner_states</span><span class="p">:</span>
                    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">log_time</span><span class="p">(</span>
                        <span class="p">(</span><span class="n">TIMERS</span><span class="p">,</span> <span class="n">SYNCH_EVAL_ENV_CONNECTOR_STATES_TIMER</span><span class="p">)</span>
                    <span class="p">):</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">sync_env_runner_states</span><span class="p">(</span>
                            <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="p">,</span>
                            <span class="n">from_worker</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_runner</span><span class="p">,</span>
                            <span class="n">env_to_module</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_to_module_connector</span><span class="p">,</span>
                            <span class="n">module_to_env</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">module_to_env_connector</span><span class="p">,</span>
                        <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_sync_filters_if_needed</span><span class="p">(</span>
                    <span class="n">central_worker</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span><span class="p">,</span>
                    <span class="n">workers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="p">,</span>
                    <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="c1"># Sync weights to the local EnvRunner (if no eval EnvRunnerGroup).</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">(</span>
                <span class="n">from_worker_or_learner_group</span><span class="o">=</span><span class="n">weights_src</span><span class="p">,</span>
                <span class="n">inference_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">make_callback</span><span class="p">(</span>
            <span class="s2">&quot;on_evaluate_start&quot;</span><span class="p">,</span>
            <span class="n">callbacks_objects</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="p">,</span>
            <span class="n">callbacks_functions</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">callbacks_on_evaluate_start</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">metrics_logger</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="n">env_steps</span> <span class="o">=</span> <span class="n">agent_steps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">batches</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># We will use a user provided evaluation function.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">custom_evaluation_function</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
                <span class="p">(</span>
                    <span class="n">eval_results</span><span class="p">,</span>
                    <span class="n">env_steps</span><span class="p">,</span>
                    <span class="n">agent_steps</span><span class="p">,</span>
                <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_evaluate_with_custom_eval_function</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">eval_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">custom_evaluation_function</span><span class="p">()</span>
        <span class="c1"># There is no eval EnvRunnerGroup -&gt; Run on local EnvRunner.</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner</span><span class="p">:</span>
            <span class="p">(</span>
                <span class="n">eval_results</span><span class="p">,</span>
                <span class="n">env_steps</span><span class="p">,</span>
                <span class="n">agent_steps</span><span class="p">,</span>
                <span class="n">batches</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_evaluate_on_local_env_runner</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env_runner</span><span class="p">)</span>
        <span class="c1"># There is only a local eval EnvRunner -&gt; Run on that.</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">num_healthy_remote_workers</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="p">(</span>
                <span class="n">eval_results</span><span class="p">,</span>
                <span class="n">env_steps</span><span class="p">,</span>
                <span class="n">agent_steps</span><span class="p">,</span>
                <span class="n">batches</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_evaluate_on_local_env_runner</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner</span><span class="p">)</span>
        <span class="c1"># There are healthy remote evaluation workers -&gt; Run on these.</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">num_healthy_remote_workers</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Running in automatic duration mode (parallel with training step).</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_duration</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">parallel_train_future</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="p">(</span>
                    <span class="n">eval_results</span><span class="p">,</span>
                    <span class="n">env_steps</span><span class="p">,</span>
                    <span class="n">agent_steps</span><span class="p">,</span>
                    <span class="n">batches</span><span class="p">,</span>
                <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_evaluate_with_auto_duration</span><span class="p">(</span><span class="n">parallel_train_future</span><span class="p">)</span>
            <span class="c1"># Running with a fixed amount of data to sample.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="p">(</span>
                    <span class="n">eval_results</span><span class="p">,</span>
                    <span class="n">env_steps</span><span class="p">,</span>
                    <span class="n">agent_steps</span><span class="p">,</span>
                    <span class="n">batches</span><span class="p">,</span>
                <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_evaluate_with_fixed_duration</span><span class="p">()</span>
        <span class="c1"># Can&#39;t find a good way to run this evaluation -&gt; Wait for next iteration.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">eval_results</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="n">eval_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="n">EVALUATION_RESULTS</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">{})</span>
            <span class="k">if</span> <span class="n">log_once</span><span class="p">(</span><span class="s2">&quot;no_eval_results&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">eval_results</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;No evaluation results found for this iteration. This can happen if the evaluation worker(s) is/are not healthy.&quot;</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">eval_results</span> <span class="o">=</span> <span class="p">{</span><span class="n">ENV_RUNNER_RESULTS</span><span class="p">:</span> <span class="n">eval_results</span><span class="p">}</span>
            <span class="n">eval_results</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED_THIS_ITER</span><span class="p">]</span> <span class="o">=</span> <span class="n">agent_steps</span>
            <span class="n">eval_results</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED_THIS_ITER</span><span class="p">]</span> <span class="o">=</span> <span class="n">env_steps</span>
            <span class="n">eval_results</span><span class="p">[</span><span class="s2">&quot;timesteps_this_iter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">env_steps</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED_FOR_EVALUATION_THIS_ITER</span><span class="p">]</span> <span class="o">=</span> <span class="n">env_steps</span>

        <span class="c1"># Compute off-policy estimates</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">custom_evaluation_function</span><span class="p">:</span>
            <span class="n">estimates</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
            <span class="c1"># for each batch run the estimator&#39;s fwd pass</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_estimators</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">:</span>
                    <span class="n">estimate_result</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">estimate</span><span class="p">(</span>
                        <span class="n">batch</span><span class="p">,</span>
                        <span class="n">split_batch_by_episode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">ope_split_batch_by_episode</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">estimates</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">estimate_result</span><span class="p">)</span>

            <span class="c1"># collate estimates from all batches</span>
            <span class="k">if</span> <span class="n">estimates</span><span class="p">:</span>
                <span class="n">eval_results</span><span class="p">[</span><span class="s2">&quot;off_policy_estimator&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">estimate_list</span> <span class="ow">in</span> <span class="n">estimates</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="n">avg_estimate</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
                        <span class="k">lambda</span> <span class="o">*</span><span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="o">*</span><span class="n">estimate_list</span>
                    <span class="p">)</span>
                    <span class="n">eval_results</span><span class="p">[</span><span class="s2">&quot;off_policy_estimator&quot;</span><span class="p">][</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">avg_estimate</span>

        <span class="c1"># Trigger `on_evaluate_end` callback.</span>
        <span class="n">make_callback</span><span class="p">(</span>
            <span class="s2">&quot;on_evaluate_end&quot;</span><span class="p">,</span>
            <span class="n">callbacks_objects</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="p">,</span>
            <span class="n">callbacks_functions</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">callbacks_on_evaluate_end</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
                <span class="n">algorithm</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">metrics_logger</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">,</span>
                <span class="n">evaluation_metrics</span><span class="o">=</span><span class="n">eval_results</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>

        <span class="c1"># Also return the results here for convenience.</span>
        <span class="k">return</span> <span class="n">eval_results</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_evaluate_with_custom_eval_function</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">ResultDict</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Evaluating current state of </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2"> using the custom eval function &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">custom_evaluation_function</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="p">(</span>
                <span class="n">eval_results</span><span class="p">,</span>
                <span class="n">env_steps</span><span class="p">,</span>
                <span class="n">agent_steps</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">custom_evaluation_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">env_steps</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">agent_steps</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Custom eval function must return &quot;</span>
                    <span class="s2">&quot;`Tuple[ResultDict, int, int]` with `int, int` being &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;`env_steps` and `agent_steps`! Got </span><span class="si">{</span><span class="n">env_steps</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">agent_steps</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">eval_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">custom_evaluation_function</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">eval_results</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eval_results</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Custom eval function must return &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;dict of metrics! Got </span><span class="si">{</span><span class="n">eval_results</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">eval_results</span><span class="p">,</span> <span class="n">env_steps</span><span class="p">,</span> <span class="n">agent_steps</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_evaluate_on_local_env_runner</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env_runner</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">env_runner</span><span class="p">,</span> <span class="s2">&quot;input_reader&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">env_runner</span><span class="o">.</span><span class="n">input_reader</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Can&#39;t evaluate on a local worker if this local worker does not have &quot;</span>
                <span class="s2">&quot;an environment!</span><span class="se">\n</span><span class="s2">Try one of the following:&quot;</span>
                <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">1) Set `evaluation_interval` &gt; 0 to force creating a separate &quot;</span>
                <span class="s2">&quot;evaluation EnvRunnerGroup.</span><span class="se">\n</span><span class="s2">2) Set `create_local_env_runner=True` to &quot;</span>
                <span class="s2">&quot;force the local (non-eval) EnvRunner to have an environment to &quot;</span>
                <span class="s2">&quot;evaluate on.&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_parallel_to_training</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot run on local evaluation worker parallel to training! Try &quot;</span>
                <span class="s2">&quot;setting `evaluation_parallel_to_training=False`.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># How many episodes/timesteps do we need to run?</span>
        <span class="n">unit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_duration_unit</span>
        <span class="n">duration</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_duration</span>
        <span class="n">eval_cfg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span>

        <span class="n">env_steps</span> <span class="o">=</span> <span class="n">agent_steps</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Evaluating current state of </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2"> for </span><span class="si">{</span><span class="n">duration</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">unit</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="n">all_batches</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="n">episodes</span> <span class="o">=</span> <span class="n">env_runner</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
                <span class="n">num_timesteps</span><span class="o">=</span><span class="n">duration</span> <span class="k">if</span> <span class="n">unit</span> <span class="o">==</span> <span class="s2">&quot;timesteps&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                <span class="n">num_episodes</span><span class="o">=</span><span class="n">duration</span> <span class="k">if</span> <span class="n">unit</span> <span class="o">==</span> <span class="s2">&quot;episodes&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">agent_steps</span> <span class="o">+=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">agent_steps</span><span class="p">()</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">episodes</span><span class="p">)</span>
            <span class="n">env_steps</span> <span class="o">+=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">env_steps</span><span class="p">()</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">episodes</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">unit</span> <span class="o">==</span> <span class="s2">&quot;episodes&quot;</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">duration</span><span class="p">):</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="n">env_runner</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
                <span class="n">agent_steps</span> <span class="o">+=</span> <span class="n">batch</span><span class="o">.</span><span class="n">agent_steps</span><span class="p">()</span>
                <span class="n">env_steps</span> <span class="o">+=</span> <span class="n">batch</span><span class="o">.</span><span class="n">env_steps</span><span class="p">()</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_estimators</span><span class="p">:</span>
                    <span class="n">all_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="n">env_runner</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
            <span class="n">agent_steps</span> <span class="o">+=</span> <span class="n">batch</span><span class="o">.</span><span class="n">agent_steps</span><span class="p">()</span>
            <span class="n">env_steps</span> <span class="o">+=</span> <span class="n">batch</span><span class="o">.</span><span class="n">env_steps</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_estimators</span><span class="p">:</span>
                <span class="n">all_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

        <span class="n">env_runner_results</span> <span class="o">=</span> <span class="n">env_runner</span><span class="o">.</span><span class="n">get_metrics</span><span class="p">()</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="n">env_runner_results</span> <span class="o">=</span> <span class="n">summarize_episodes</span><span class="p">(</span>
                <span class="n">env_runner_results</span><span class="p">,</span>
                <span class="n">env_runner_results</span><span class="p">,</span>
                <span class="n">keep_custom_metrics</span><span class="o">=</span><span class="n">eval_cfg</span><span class="o">.</span><span class="n">keep_per_episode_custom_metrics</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">aggregate</span><span class="p">(</span>
                <span class="p">[</span><span class="n">env_runner_results</span><span class="p">],</span>
                <span class="n">key</span><span class="o">=</span><span class="p">(</span><span class="n">EVALUATION_RESULTS</span><span class="p">,</span> <span class="n">ENV_RUNNER_RESULTS</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="n">env_runner_results</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">return</span> <span class="n">env_runner_results</span><span class="p">,</span> <span class="n">env_steps</span><span class="p">,</span> <span class="n">agent_steps</span><span class="p">,</span> <span class="n">all_batches</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_evaluate_with_auto_duration</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parallel_train_future</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Evaluating current state of </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2"> for as long as the parallelly &quot;</span>
            <span class="s2">&quot;running training step takes.&quot;</span>
        <span class="p">)</span>

        <span class="n">all_metrics</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">all_batches</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># How many episodes have we run (across all eval workers)?</span>
        <span class="n">num_healthy_workers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">num_healthy_remote_workers</span><span class="p">()</span>
        <span class="c1"># Do we have to force-reset the EnvRunners before the first round of `sample()`</span>
        <span class="c1"># calls.?</span>
        <span class="n">force_reset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_force_reset_envs_before_iteration</span>

        <span class="c1"># Remote function used on healthy EnvRunners to sample, get metrics, and</span>
        <span class="c1"># step counts.</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">_env_runner_remote</span><span class="p">(</span><span class="n">worker</span><span class="p">,</span> <span class="n">num</span><span class="p">,</span> <span class="nb">round</span><span class="p">,</span> <span class="nb">iter</span><span class="p">):</span>
            <span class="c1"># Sample AND get_metrics, but only return metrics (and steps actually taken)</span>
            <span class="c1"># to save time.</span>
            <span class="n">episodes</span> <span class="o">=</span> <span class="n">worker</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
                <span class="n">num_timesteps</span><span class="o">=</span><span class="n">num</span><span class="p">,</span> <span class="n">force_reset</span><span class="o">=</span><span class="n">force_reset</span> <span class="ow">and</span> <span class="nb">round</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="p">)</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="n">worker</span><span class="o">.</span><span class="n">get_metrics</span><span class="p">()</span>
            <span class="n">env_steps</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">env_steps</span><span class="p">()</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">episodes</span><span class="p">)</span>
            <span class="n">agent_steps</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">agent_steps</span><span class="p">()</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">episodes</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">env_steps</span><span class="p">,</span> <span class="n">agent_steps</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="nb">iter</span>

        <span class="n">env_steps</span> <span class="o">=</span> <span class="n">agent_steps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="n">train_mean_time</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span>
                <span class="p">(</span><span class="n">TIMERS</span><span class="p">,</span> <span class="n">TRAINING_ITERATION_TIMER</span><span class="p">),</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.0</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">train_mean_time</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span><span class="p">[</span><span class="n">TRAINING_ITERATION_TIMER</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span>
        <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">algo_iteration</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span>

        <span class="n">_round</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">while</span> <span class="p">(</span>
            <span class="c1"># In case all the remote evaluation workers die during a round of</span>
            <span class="c1"># evaluation, we need to stop.</span>
            <span class="n">num_healthy_workers</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="c1"># Run at least for one round AND at least for as long as the parallel</span>
            <span class="c1"># training step takes.</span>
            <span class="ow">and</span> <span class="p">(</span><span class="n">_round</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">parallel_train_future</span><span class="o">.</span><span class="n">done</span><span class="p">())</span>
        <span class="p">):</span>
            <span class="n">_round</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="c1"># New API stack -&gt; EnvRunners return Episodes.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
                <span class="c1"># Compute rough number of timesteps it takes for a single EnvRunner</span>
                <span class="c1"># to occupy the estimated (parallelly running) train step.</span>
                <span class="n">_num</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span>
                    <span class="c1"># Clamp number of steps to take between a max and a min.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_auto_duration_max_env_steps_per_sample</span><span class="p">,</span>
                    <span class="nb">max</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_auto_duration_min_env_steps_per_sample</span><span class="p">,</span>
                        <span class="p">(</span>
                            <span class="c1"># How much time do we have left?</span>
                            <span class="p">(</span><span class="n">train_mean_time</span> <span class="o">-</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">))</span>
                            <span class="c1"># Multiply by our own (eval) throughput to get the timesteps</span>
                            <span class="c1"># to do (per worker).</span>
                            <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span>
                                <span class="p">(</span>
                                    <span class="n">EVALUATION_RESULTS</span><span class="p">,</span>
                                    <span class="n">ENV_RUNNER_RESULTS</span><span class="p">,</span>
                                    <span class="n">NUM_ENV_STEPS_SAMPLED_LIFETIME</span><span class="p">,</span>
                                <span class="p">),</span>
                                <span class="n">throughput</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                <span class="n">default</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                            <span class="p">)</span>
                            <span class="o">/</span> <span class="n">num_healthy_workers</span>
                        <span class="p">),</span>
                    <span class="p">),</span>
                <span class="p">)</span>

                <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">fetch_ready_async_reqs</span><span class="p">(</span>
                    <span class="n">return_obj_refs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">timeout_seconds</span><span class="o">=</span><span class="mf">0.0</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">foreach_env_runner_async</span><span class="p">(</span>
                    <span class="n">func</span><span class="o">=</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
                        <span class="n">_env_runner_remote</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">_num</span><span class="p">,</span> <span class="nb">round</span><span class="o">=</span><span class="n">_round</span><span class="p">,</span> <span class="nb">iter</span><span class="o">=</span><span class="n">algo_iteration</span>
                    <span class="p">),</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">wid</span><span class="p">,</span> <span class="p">(</span><span class="n">env_s</span><span class="p">,</span> <span class="n">ag_s</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="nb">iter</span><span class="p">)</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
                    <span class="c1"># Ignore eval results kicked off in an earlier iteration.</span>
                    <span class="c1"># (those results would be outdated and thus misleading).</span>
                    <span class="k">if</span> <span class="nb">iter</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="n">env_steps</span> <span class="o">+=</span> <span class="n">env_s</span>
                    <span class="n">agent_steps</span> <span class="o">+=</span> <span class="n">ag_s</span>
                    <span class="n">all_metrics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
                <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span>

            <span class="c1"># Old API stack -&gt; RolloutWorkers return batches.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">foreach_env_runner_async</span><span class="p">(</span>
                    <span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">sample</span><span class="p">(),</span> <span class="n">w</span><span class="o">.</span><span class="n">get_metrics</span><span class="p">(),</span> <span class="n">algo_iteration</span><span class="p">),</span>
                <span class="p">)</span>
                <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">fetch_ready_async_reqs</span><span class="p">(</span>
                    <span class="n">return_obj_refs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">timeout_seconds</span><span class="o">=</span><span class="mf">0.01</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">wid</span><span class="p">,</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="nb">iter</span><span class="p">)</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">iter</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="n">env_steps</span> <span class="o">+=</span> <span class="n">batch</span><span class="o">.</span><span class="n">env_steps</span><span class="p">()</span>
                    <span class="n">agent_steps</span> <span class="o">+=</span> <span class="n">batch</span><span class="o">.</span><span class="n">agent_steps</span><span class="p">()</span>
                    <span class="n">all_metrics</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_estimators</span><span class="p">:</span>
                        <span class="c1"># TODO: (kourosh) This approach will cause an OOM issue when</span>
                        <span class="c1">#  the dataset gets huge (should be ok for now).</span>
                        <span class="n">all_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

            <span class="c1"># Update correct number of healthy remote workers.</span>
            <span class="n">num_healthy_workers</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">num_healthy_remote_workers</span><span class="p">()</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">num_healthy_workers</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;Calling `sample()` on your remote evaluation worker(s) &quot;</span>
                <span class="s2">&quot;resulted in all workers crashing! Make sure a) your environment is not&quot;</span>
                <span class="s2">&quot; too unstable, b) you have enough evaluation workers &quot;</span>
                <span class="s2">&quot;(`config.evaluation(evaluation_num_env_runners=...)`) to cover for &quot;</span>
                <span class="s2">&quot;occasional losses, and c) you use the `config.fault_tolerance(&quot;</span>
                <span class="s2">&quot;restart_failed_env_runners=True)` setting.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="n">env_runner_results</span> <span class="o">=</span> <span class="n">summarize_episodes</span><span class="p">(</span>
                <span class="n">all_metrics</span><span class="p">,</span>
                <span class="n">all_metrics</span><span class="p">,</span>
                <span class="n">keep_custom_metrics</span><span class="o">=</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">keep_per_episode_custom_metrics</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="n">num_episodes</span> <span class="o">=</span> <span class="n">env_runner_results</span><span class="p">[</span><span class="n">NUM_EPISODES</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">aggregate</span><span class="p">(</span>
                <span class="n">all_metrics</span><span class="p">,</span>
                <span class="n">key</span><span class="o">=</span><span class="p">(</span><span class="n">EVALUATION_RESULTS</span><span class="p">,</span> <span class="n">ENV_RUNNER_RESULTS</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="n">num_episodes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span>
                <span class="p">(</span><span class="n">EVALUATION_RESULTS</span><span class="p">,</span> <span class="n">ENV_RUNNER_RESULTS</span><span class="p">,</span> <span class="n">NUM_EPISODES</span><span class="p">),</span>
                <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">env_runner_results</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Warn if results are empty, it could be that this is because the auto-time is</span>
        <span class="c1"># not enough to run through one full episode.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_force_reset_envs_before_iteration</span>
            <span class="ow">and</span> <span class="n">num_episodes</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;This evaluation iteration resulted in an empty set of episode summary &quot;</span>
                <span class="s2">&quot;results! It&#39;s possible that the auto-duration time (roughly the mean &quot;</span>
                <span class="s2">&quot;time it takes for the training step to finish) is not enough to finish&quot;</span>
                <span class="s2">&quot; even a single episode. Your current mean training iteration time is &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">train_mean_time</span><span class="si">}</span><span class="s2">sec. Try setting the min iteration time to a higher &quot;</span>
                <span class="s2">&quot;value via the `config.reporting(min_time_s_per_iteration=...)` OR you &quot;</span>
                <span class="s2">&quot;can also set `config.evaluation_force_reset_envs_before_iteration` to &quot;</span>
                <span class="s2">&quot;False. However, keep in mind that then the evaluation results may &quot;</span>
                <span class="s2">&quot;contain some episode stats generated with earlier weights versions.&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">env_runner_results</span><span class="p">,</span> <span class="n">env_steps</span><span class="p">,</span> <span class="n">agent_steps</span><span class="p">,</span> <span class="n">all_batches</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_evaluate_offline_with_fixed_duration</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># How many batches do we need to run?</span>
        <span class="n">num_workers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_offline_eval_runners</span>
        <span class="n">time_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">offline_evaluation_timeout_s</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">_offline_eval_runner_remote</span><span class="p">(</span><span class="n">runner</span><span class="p">,</span> <span class="nb">iter</span><span class="p">):</span>

            <span class="n">metrics</span> <span class="o">=</span> <span class="n">runner</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

            <span class="k">return</span> <span class="n">metrics</span><span class="p">,</span> <span class="nb">iter</span>

        <span class="n">all_metrics</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">num_units_done</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># How many episodes have we run (across all eval workers)?</span>
        <span class="n">num_units_done</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">num_healthy_workers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">offline_eval_runner_group</span><span class="o">.</span><span class="n">num_healthy_remote_runners</span>

        <span class="c1"># TODO (simon): Note, agent steps might not be available, but only</span>
        <span class="c1"># module steps.</span>

        <span class="n">t_last_result</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">_round</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="n">algo_iteration</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span>

        <span class="c1"># In case all the remote evaluation workers die during a round of</span>
        <span class="c1"># evaluation, we need to stop.</span>
        <span class="k">while</span> <span class="n">num_healthy_workers</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">units_left_to_do</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">offline_evaluation_duration</span> <span class="o">*</span> <span class="n">num_workers</span> <span class="o">-</span> <span class="n">num_units_done</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">units_left_to_do</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="n">_round</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">offline_eval_runner_group</span><span class="o">.</span><span class="n">foreach_runner_async</span><span class="p">(</span>
                <span class="n">func</span><span class="o">=</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
                    <span class="n">_offline_eval_runner_remote</span><span class="p">,</span>
                    <span class="nb">iter</span><span class="o">=</span><span class="n">algo_iteration</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">offline_eval_runner_group</span><span class="o">.</span><span class="n">fetch_ready_async_reqs</span><span class="p">(</span>
                <span class="n">return_obj_refs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">timeout_seconds</span><span class="o">=</span><span class="mf">0.01</span>
            <span class="p">)</span>
            <span class="c1"># Make sure we properly time out if we have not received any results</span>
            <span class="c1"># for more than `time_out` seconds.</span>
            <span class="n">time_now</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">results</span> <span class="ow">and</span> <span class="n">time_now</span> <span class="o">-</span> <span class="n">t_last_result</span> <span class="o">&gt;</span> <span class="n">time_out</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="k">elif</span> <span class="n">results</span><span class="p">:</span>
                <span class="n">t_last_result</span> <span class="o">=</span> <span class="n">time_now</span>
            <span class="k">for</span> <span class="n">wid</span><span class="p">,</span> <span class="p">(</span><span class="n">met</span><span class="p">,</span> <span class="nb">iter</span><span class="p">)</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">iter</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">all_metrics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">met</span><span class="p">)</span>
                <span class="n">num_units_done</span> <span class="o">+=</span> <span class="p">(</span>
                    <span class="n">met</span><span class="p">[</span><span class="n">ALL_MODULES</span><span class="p">][</span><span class="n">DATASET_NUM_ITERS_EVALUATED</span><span class="p">]</span><span class="o">.</span><span class="n">peek</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">DATASET_NUM_ITERS_EVALUATED</span> <span class="ow">in</span> <span class="n">met</span><span class="p">[</span><span class="n">ALL_MODULES</span><span class="p">]</span>
                    <span class="k">else</span> <span class="mi">0</span>
                <span class="p">)</span>

            <span class="c1"># Update correct number of healthy remote workers.</span>
            <span class="n">num_healthy_workers</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">offline_eval_runner_group</span><span class="o">.</span><span class="n">num_healthy_remote_runners</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">num_healthy_workers</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;Calling `run()` on your remote offline evaluation runner(s) &quot;</span>
                <span class="s2">&quot;resulted in all runners crashing! Make sure a) your dataset is not&quot;</span>
                <span class="s2">&quot; corrupted, b) you have enough offline evaluation runners &quot;</span>
                <span class="s2">&quot;(`config.evaluation(num_offline_eval_runners=...)`) to cover for &quot;</span>
                <span class="s2">&quot;occasional losses, and c) you use the `config.fault_tolerance(&quot;</span>
                <span class="s2">&quot;restart_failed_offline_eval_runners=True)` setting.&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">aggregate</span><span class="p">(</span>
            <span class="n">all_metrics</span><span class="p">,</span>
            <span class="n">key</span><span class="o">=</span><span class="p">(</span><span class="n">EVALUATION_RESULTS</span><span class="p">,</span> <span class="n">OFFLINE_EVAL_RUNNER_RESULTS</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_evaluate_with_fixed_duration</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># How many episodes/timesteps do we need to run?</span>
        <span class="n">unit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_duration_unit</span>
        <span class="n">eval_cfg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span>
        <span class="n">num_workers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_num_env_runners</span>
        <span class="n">force_reset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_force_reset_envs_before_iteration</span>
        <span class="n">time_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_sample_timeout_s</span>

        <span class="c1"># Remote function used on healthy EnvRunners to sample, get metrics, and</span>
        <span class="c1"># step counts.</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">_env_runner_remote</span><span class="p">(</span><span class="n">worker</span><span class="p">,</span> <span class="n">num</span><span class="p">,</span> <span class="nb">round</span><span class="p">,</span> <span class="nb">iter</span><span class="p">,</span> <span class="n">_force_reset</span><span class="p">):</span>
            <span class="c1"># Sample AND get_metrics, but only return metrics (and steps actually taken)</span>
            <span class="c1"># to save time. Also return the iteration to check, whether we should</span>
            <span class="c1"># discard and outdated result (from a slow worker).</span>
            <span class="n">episodes</span> <span class="o">=</span> <span class="n">worker</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
                <span class="n">num_timesteps</span><span class="o">=</span><span class="p">(</span>
                    <span class="n">num</span><span class="p">[</span><span class="n">worker</span><span class="o">.</span><span class="n">worker_index</span><span class="p">]</span> <span class="k">if</span> <span class="n">unit</span> <span class="o">==</span> <span class="s2">&quot;timesteps&quot;</span> <span class="k">else</span> <span class="kc">None</span>
                <span class="p">),</span>
                <span class="n">num_episodes</span><span class="o">=</span><span class="p">(</span><span class="n">num</span><span class="p">[</span><span class="n">worker</span><span class="o">.</span><span class="n">worker_index</span><span class="p">]</span> <span class="k">if</span> <span class="n">unit</span> <span class="o">==</span> <span class="s2">&quot;episodes&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">),</span>
                <span class="n">force_reset</span><span class="o">=</span><span class="n">_force_reset</span> <span class="ow">and</span> <span class="nb">round</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="n">worker</span><span class="o">.</span><span class="n">get_metrics</span><span class="p">()</span>
            <span class="n">env_steps</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">env_steps</span><span class="p">()</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">episodes</span><span class="p">)</span>
            <span class="n">agent_steps</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">agent_steps</span><span class="p">()</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">episodes</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">env_steps</span><span class="p">,</span> <span class="n">agent_steps</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="nb">iter</span>

        <span class="n">all_metrics</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">all_batches</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># How many episodes have we run (across all eval workers)?</span>
        <span class="n">num_units_done</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">num_healthy_workers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">num_healthy_remote_workers</span><span class="p">()</span>

        <span class="n">env_steps</span> <span class="o">=</span> <span class="n">agent_steps</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">t_last_result</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">_round</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="n">algo_iteration</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span>

        <span class="c1"># In case all the remote evaluation workers die during a round of</span>
        <span class="c1"># evaluation, we need to stop.</span>
        <span class="k">while</span> <span class="n">num_healthy_workers</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">units_left_to_do</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_duration</span> <span class="o">-</span> <span class="n">num_units_done</span>
            <span class="k">if</span> <span class="n">units_left_to_do</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="n">_round</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># New API stack -&gt; EnvRunners return Episodes.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
                <span class="n">_num</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span>  <span class="c1"># [None]: skip idx=0 (local worker)</span>
                    <span class="p">(</span><span class="n">units_left_to_do</span> <span class="o">//</span> <span class="n">num_healthy_workers</span><span class="p">)</span>
                    <span class="o">+</span> <span class="nb">bool</span><span class="p">(</span><span class="n">i</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="n">units_left_to_do</span> <span class="o">%</span> <span class="n">num_healthy_workers</span><span class="p">))</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_workers</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                <span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">foreach_env_runner_async</span><span class="p">(</span>
                    <span class="n">func</span><span class="o">=</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
                        <span class="n">_env_runner_remote</span><span class="p">,</span>
                        <span class="n">num</span><span class="o">=</span><span class="n">_num</span><span class="p">,</span>
                        <span class="nb">round</span><span class="o">=</span><span class="n">_round</span><span class="p">,</span>
                        <span class="nb">iter</span><span class="o">=</span><span class="n">algo_iteration</span><span class="p">,</span>
                        <span class="n">_force_reset</span><span class="o">=</span><span class="n">force_reset</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">)</span>
                <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">fetch_ready_async_reqs</span><span class="p">(</span>
                    <span class="n">return_obj_refs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">timeout_seconds</span><span class="o">=</span><span class="mf">0.01</span>
                <span class="p">)</span>
                <span class="c1"># Make sure we properly time out if we have not received any results</span>
                <span class="c1"># for more than `time_out` seconds.</span>
                <span class="n">time_now</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">results</span> <span class="ow">and</span> <span class="n">time_now</span> <span class="o">-</span> <span class="n">t_last_result</span> <span class="o">&gt;</span> <span class="n">time_out</span><span class="p">:</span>
                    <span class="k">break</span>
                <span class="k">elif</span> <span class="n">results</span><span class="p">:</span>
                    <span class="n">t_last_result</span> <span class="o">=</span> <span class="n">time_now</span>
                <span class="k">for</span> <span class="n">wid</span><span class="p">,</span> <span class="p">(</span><span class="n">env_s</span><span class="p">,</span> <span class="n">ag_s</span><span class="p">,</span> <span class="n">met</span><span class="p">,</span> <span class="nb">iter</span><span class="p">)</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">iter</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="n">env_steps</span> <span class="o">+=</span> <span class="n">env_s</span>
                    <span class="n">agent_steps</span> <span class="o">+=</span> <span class="n">ag_s</span>
                    <span class="n">all_metrics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">met</span><span class="p">)</span>
                    <span class="n">num_units_done</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="p">(</span><span class="n">met</span><span class="p">[</span><span class="n">NUM_EPISODES</span><span class="p">]</span><span class="o">.</span><span class="n">peek</span><span class="p">()</span> <span class="k">if</span> <span class="n">NUM_EPISODES</span> <span class="ow">in</span> <span class="n">met</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
                        <span class="k">if</span> <span class="n">unit</span> <span class="o">==</span> <span class="s2">&quot;episodes&quot;</span>
                        <span class="k">else</span> <span class="p">(</span>
                            <span class="n">env_s</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">count_steps_by</span> <span class="o">==</span> <span class="s2">&quot;env_steps&quot;</span> <span class="k">else</span> <span class="n">ag_s</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
            <span class="c1"># Old API stack -&gt; RolloutWorkers return batches.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">units_per_healthy_remote_worker</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="mi">1</span>
                    <span class="k">if</span> <span class="n">unit</span> <span class="o">==</span> <span class="s2">&quot;episodes&quot;</span>
                    <span class="k">else</span> <span class="n">eval_cfg</span><span class="o">.</span><span class="n">rollout_fragment_length</span>
                    <span class="o">*</span> <span class="n">eval_cfg</span><span class="o">.</span><span class="n">num_envs_per_env_runner</span>
                <span class="p">)</span>
                <span class="c1"># Select proper number of evaluation workers for this round.</span>
                <span class="n">selected_eval_worker_ids</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">worker_id</span>
                    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">worker_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">healthy_worker_ids</span><span class="p">()</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="n">i</span> <span class="o">*</span> <span class="n">units_per_healthy_remote_worker</span> <span class="o">&lt;</span> <span class="n">units_left_to_do</span>
                <span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">foreach_env_runner_async</span><span class="p">(</span>
                    <span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">sample</span><span class="p">(),</span> <span class="n">w</span><span class="o">.</span><span class="n">get_metrics</span><span class="p">(),</span> <span class="n">algo_iteration</span><span class="p">),</span>
                    <span class="n">remote_worker_ids</span><span class="o">=</span><span class="n">selected_eval_worker_ids</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">fetch_ready_async_reqs</span><span class="p">(</span>
                    <span class="n">return_obj_refs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">timeout_seconds</span><span class="o">=</span><span class="mf">0.01</span>
                <span class="p">)</span>
                <span class="c1"># Make sure we properly time out if we have not received any results</span>
                <span class="c1"># for more than `time_out` seconds.</span>
                <span class="n">time_now</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">results</span> <span class="ow">and</span> <span class="n">time_now</span> <span class="o">-</span> <span class="n">t_last_result</span> <span class="o">&gt;</span> <span class="n">time_out</span><span class="p">:</span>
                    <span class="k">break</span>
                <span class="k">elif</span> <span class="n">results</span><span class="p">:</span>
                    <span class="n">t_last_result</span> <span class="o">=</span> <span class="n">time_now</span>
                <span class="k">for</span> <span class="n">wid</span><span class="p">,</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="nb">iter</span><span class="p">)</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">iter</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="n">env_steps</span> <span class="o">+=</span> <span class="n">batch</span><span class="o">.</span><span class="n">env_steps</span><span class="p">()</span>
                    <span class="n">agent_steps</span> <span class="o">+=</span> <span class="n">batch</span><span class="o">.</span><span class="n">agent_steps</span><span class="p">()</span>
                    <span class="n">all_metrics</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_estimators</span><span class="p">:</span>
                        <span class="c1"># TODO: (kourosh) This approach will cause an OOM issue when</span>
                        <span class="c1">#  the dataset gets huge (should be ok for now).</span>
                        <span class="n">all_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

                <span class="c1"># 1 episode per returned batch.</span>
                <span class="k">if</span> <span class="n">unit</span> <span class="o">==</span> <span class="s2">&quot;episodes&quot;</span><span class="p">:</span>
                    <span class="n">num_units_done</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
                <span class="c1"># n timesteps per returned batch.</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">num_units_done</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">env_steps</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">count_steps_by</span> <span class="o">==</span> <span class="s2">&quot;env_steps&quot;</span>
                        <span class="k">else</span> <span class="n">agent_steps</span>
                    <span class="p">)</span>

            <span class="c1"># Update correct number of healthy remote workers.</span>
            <span class="n">num_healthy_workers</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">num_healthy_remote_workers</span><span class="p">()</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">num_healthy_workers</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;Calling `sample()` on your remote evaluation worker(s) &quot;</span>
                <span class="s2">&quot;resulted in all workers crashing! Make sure a) your environment is not&quot;</span>
                <span class="s2">&quot; too unstable, b) you have enough evaluation workers &quot;</span>
                <span class="s2">&quot;(`config.evaluation(evaluation_num_env_runners=...)`) to cover for &quot;</span>
                <span class="s2">&quot;occasional losses, and c) you use the `config.fault_tolerance(&quot;</span>
                <span class="s2">&quot;restart_failed_env_runners=True)` setting.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="n">env_runner_results</span> <span class="o">=</span> <span class="n">summarize_episodes</span><span class="p">(</span>
                <span class="n">all_metrics</span><span class="p">,</span>
                <span class="n">all_metrics</span><span class="p">,</span>
                <span class="n">keep_custom_metrics</span><span class="o">=</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">keep_per_episode_custom_metrics</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="n">num_episodes</span> <span class="o">=</span> <span class="n">env_runner_results</span><span class="p">[</span><span class="n">NUM_EPISODES</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">aggregate</span><span class="p">(</span>
                <span class="n">all_metrics</span><span class="p">,</span>
                <span class="n">key</span><span class="o">=</span><span class="p">(</span><span class="n">EVALUATION_RESULTS</span><span class="p">,</span> <span class="n">ENV_RUNNER_RESULTS</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="n">num_episodes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span>
                <span class="p">(</span><span class="n">EVALUATION_RESULTS</span><span class="p">,</span> <span class="n">ENV_RUNNER_RESULTS</span><span class="p">,</span> <span class="n">NUM_EPISODES</span><span class="p">),</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span>
            <span class="p">)</span>
            <span class="n">env_runner_results</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Warn if results are empty, it could be that this is because the eval timesteps</span>
        <span class="c1"># are not enough to run through one full episode.</span>
        <span class="k">if</span> <span class="n">num_episodes</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;This evaluation iteration resulted in an empty set of episode summary &quot;</span>
                <span class="s2">&quot;results! It&#39;s possible that your configured duration timesteps are not&quot;</span>
                <span class="s2">&quot; enough to finish even a single episode. You have configured &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_duration</span><span class="si">}</span><span class="s2"> &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_duration_unit</span><span class="si">}</span><span class="s2">. For &#39;timesteps&#39;, try &quot;</span>
                <span class="s2">&quot;increasing this value via the `config.evaluation(evaluation_duration=&quot;</span>
                <span class="s2">&quot;...)` OR change the unit to &#39;episodes&#39; via `config.evaluation(&quot;</span>
                <span class="s2">&quot;evaluation_duration_unit=&#39;episodes&#39;)` OR try increasing the timeout &quot;</span>
                <span class="s2">&quot;threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR &quot;</span>
                <span class="s2">&quot;you can also set `config.evaluation_force_reset_envs_before_iteration`&quot;</span>
                <span class="s2">&quot; to False. However, keep in mind that in the latter case, the &quot;</span>
                <span class="s2">&quot;evaluation results may contain some episode stats generated with &quot;</span>
                <span class="s2">&quot;earlier weights versions.&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">env_runner_results</span><span class="p">,</span> <span class="n">env_steps</span><span class="p">,</span> <span class="n">agent_steps</span><span class="p">,</span> <span class="n">all_batches</span>

<div class="viewcode-block" id="Algorithm.restore_env_runners">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.restore_env_runners.html#ray.rllib.algorithms.algorithm.Algorithm.restore_env_runners">[docs]</a>
    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">restore_env_runners</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env_runner_group</span><span class="p">:</span> <span class="n">EnvRunnerGroup</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Try bringing back unhealthy EnvRunners and - if successful - sync with local.</span>

<span class="sd">        Algorithms that use custom EnvRunners may override this method to</span>
<span class="sd">        disable the default, and create custom restoration logics. Note that &quot;restoring&quot;</span>
<span class="sd">        does not include the actual restarting process, but merely what should happen</span>
<span class="sd">        after such a restart of a (previously failed) worker.</span>

<span class="sd">        Args:</span>
<span class="sd">            env_runner_group: The EnvRunnerGroup to restore. This may be the training or</span>
<span class="sd">                the evaluation EnvRunnerGroup.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A list of EnvRunner indices that have been restored during the call of</span>
<span class="sd">            this method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># If `env_runner_group` is None, or</span>
        <span class="c1"># 1. `env_runner_group` (EnvRunnerGroup) does not have a local worker, and</span>
        <span class="c1"># 2. `self.env_runner_group` (EnvRunnerGroup used for training) does not have a</span>
        <span class="c1"># local EnvRunner -&gt; we don&#39;t have an EnvRunner to get state from, so we can&#39;t</span>
        <span class="c1"># recover remote EnvRunner actors in this case.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">env_runner_group</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="n">env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner</span>
        <span class="p">):</span>
            <span class="k">return</span> <span class="p">[]</span>

        <span class="c1"># This is really cheap, since probe_unhealthy_env_runners() is a no-op</span>
        <span class="c1"># if there are no unhealthy workers.</span>
        <span class="n">restored</span> <span class="o">=</span> <span class="n">env_runner_group</span><span class="o">.</span><span class="n">probe_unhealthy_env_runners</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">restored</span><span class="p">:</span>
            <span class="c1"># Count the restored workers.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="s2">&quot;total_num_restored_workers&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">restored</span><span class="p">)</span>

            <span class="n">from_env_runner</span> <span class="o">=</span> <span class="n">env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner</span>
            <span class="c1"># Get the state of the correct (reference) worker. For example the local</span>
            <span class="c1"># worker of an EnvRunnerGroup.</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">from_env_runner</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>
            <span class="n">state_ref</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

            <span class="k">def</span><span class="w"> </span><span class="nf">_sync_env_runner</span><span class="p">(</span><span class="n">er</span><span class="p">):</span>
                <span class="n">er</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">state_ref</span><span class="p">))</span>

            <span class="c1"># Take out (old) connector states from local worker&#39;s state.</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">pol_states</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;policy_states&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                    <span class="n">pol_states</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;connector_configs&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_multi_agent</span><span class="p">:</span>

                <span class="n">multi_rl_module_spec</span> <span class="o">=</span> <span class="n">MultiRLModuleSpec</span><span class="o">.</span><span class="n">from_module</span><span class="p">(</span>
                    <span class="n">from_env_runner</span><span class="o">.</span><span class="n">module</span>
                <span class="p">)</span>

                <span class="k">def</span><span class="w"> </span><span class="nf">_sync_env_runner</span><span class="p">(</span><span class="n">er</span><span class="p">):</span>  <span class="c1"># noqa</span>
                    <span class="c1"># Remove modules, if necessary.</span>
                    <span class="k">for</span> <span class="n">module_id</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">er</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">_rl_modules</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                        <span class="k">if</span> <span class="n">module_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">multi_rl_module_spec</span><span class="o">.</span><span class="n">rl_module_specs</span><span class="p">:</span>
                            <span class="n">er</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">remove_module</span><span class="p">(</span>
                                <span class="n">module_id</span><span class="p">,</span> <span class="n">raise_err_if_not_found</span><span class="o">=</span><span class="kc">True</span>
                            <span class="p">)</span>
                    <span class="c1"># Add modules, if necessary.</span>
                    <span class="k">for</span> <span class="n">mid</span><span class="p">,</span> <span class="n">mod_spec</span> <span class="ow">in</span> <span class="n">multi_rl_module_spec</span><span class="o">.</span><span class="n">rl_module_specs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                        <span class="k">if</span> <span class="n">mid</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">er</span><span class="o">.</span><span class="n">module</span><span class="p">:</span>
                            <span class="n">er</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="n">mid</span><span class="p">,</span> <span class="n">mod_spec</span><span class="o">.</span><span class="n">build</span><span class="p">(),</span> <span class="n">override</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                    <span class="c1"># Now that the MultiRLModule is fixed, update the state.</span>
                    <span class="n">er</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">state_ref</span><span class="p">))</span>

            <span class="c1"># By default, entire local EnvRunner state is synced after restoration</span>
            <span class="c1"># to bring the previously failed EnvRunner up to date.</span>
            <span class="n">env_runner_group</span><span class="o">.</span><span class="n">foreach_env_runner</span><span class="p">(</span>
                <span class="n">func</span><span class="o">=</span><span class="n">_sync_env_runner</span><span class="p">,</span>
                <span class="n">remote_worker_ids</span><span class="o">=</span><span class="n">restored</span><span class="p">,</span>
                <span class="c1"># Don&#39;t update the local EnvRunner, b/c it&#39;s the one we are synching</span>
                <span class="c1"># from.</span>
                <span class="n">local_env_runner</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">timeout_seconds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">env_runner_restore_timeout_s</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">restored</span></div>


    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">restore_offline_eval_runners</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">runner_group</span><span class="p">:</span> <span class="n">RunnerGroup</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">runner_group</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">runner_group</span><span class="o">.</span><span class="n">local_runner</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[]</span>

        <span class="n">restored</span> <span class="o">=</span> <span class="n">runner_group</span><span class="o">.</span><span class="n">probe_unhealthy_runners</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">restored</span><span class="p">:</span>
            <span class="c1"># Count the restored workers.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="s2">&quot;total_num_restored_workers&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">restored</span><span class="p">)</span>

            <span class="c1"># Get the state of the correct (reference) worker.</span>
            <span class="n">from_runner</span> <span class="o">=</span> <span class="n">runner_group</span><span class="o">.</span><span class="n">healthy_runner_ids</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">runner_group</span><span class="o">.</span><span class="n">foreach_runner</span><span class="p">(</span>
                <span class="s2">&quot;get_state&quot;</span><span class="p">,</span>
                <span class="n">local_runner</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">remote_worker_ids</span><span class="o">=</span><span class="n">from_runner</span><span class="p">,</span>
            <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">state_ref</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

            <span class="k">def</span><span class="w"> </span><span class="nf">_sync_runner</span><span class="p">(</span><span class="n">r</span><span class="p">):</span>
                <span class="n">r</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">state_ref</span><span class="p">))</span>

            <span class="c1"># By default, entire `Runner`` state is synced after restoration</span>
            <span class="c1"># to bring the previously failed `Runner` up to date.</span>
            <span class="n">runner_group</span><span class="o">.</span><span class="n">foreach_runner</span><span class="p">(</span>
                <span class="n">func</span><span class="o">=</span><span class="n">_sync_runner</span><span class="p">,</span>
                <span class="n">remote_worker_ids</span><span class="o">=</span><span class="n">restored</span><span class="p">,</span>
                <span class="c1"># Don&#39;t update the local `Runner`.</span>
                <span class="n">local_runner</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">timeout_seconds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">offline_eval_runner_restore_timeout_s</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># Restore the correct data iterator split stream.</span>
            <span class="c1"># TODO (simon): Define a `restore` method in the `RunnerGroup`</span>
            <span class="c1"># such that we do not have to check here for the group.</span>
            <span class="c1"># Also get a different streaming split if a runner fails and is not</span>
            <span class="c1"># recreated.</span>
            <span class="n">runner_group</span><span class="o">.</span><span class="n">foreach_runner</span><span class="p">(</span>
                <span class="n">func</span><span class="o">=</span><span class="s2">&quot;set_dataset_iterator&quot;</span><span class="p">,</span>
                <span class="n">remote_worker_ids</span><span class="o">=</span><span class="n">restored</span><span class="p">,</span>
                <span class="n">local_runner</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">timeout_seconds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">offline_eval_runner_restore_timeout_s</span><span class="p">,</span>
                <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;iterator&quot;</span><span class="p">:</span> <span class="n">runner_group</span><span class="o">.</span><span class="n">_offline_data_iterators</span><span class="p">[</span><span class="n">restored</span><span class="p">]},</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">restored</span>

<div class="viewcode-block" id="Algorithm.training_step">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.training_step.html#ray.rllib.algorithms.algorithm.Algorithm.training_step">[docs]</a>
    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Default single iteration logic of an algorithm.</span>

<span class="sd">        - Collect on-policy samples (SampleBatches) in parallel using the</span>
<span class="sd">          Algorithm&#39;s EnvRunners (@ray.remote).</span>
<span class="sd">        - Concatenate collected SampleBatches into one train batch.</span>
<span class="sd">        - Note that we may have more than one policy in the multi-agent case:</span>
<span class="sd">          Call the different policies&#39; `learn_on_batch` (simple optimizer) OR</span>
<span class="sd">          `load_batch_into_buffer` + `learn_on_loaded_batch` (multi-GPU</span>
<span class="sd">          optimizer) methods to calculate loss and update the model(s).</span>
<span class="sd">        - Return all collected metrics for the iteration.</span>

<span class="sd">        Returns:</span>
<span class="sd">            For the new API stack, returns None. Results are compiled and extracted</span>
<span class="sd">            automatically through a single `self.metrics.reduce()` call at the very end</span>
<span class="sd">            of an iteration (which might contain more than one call to</span>
<span class="sd">            `training_step()`). This way, we make sure that we account for all</span>
<span class="sd">            results generated by each individual `training_step()` call.</span>
<span class="sd">            For the old API stack, returns the results dict from executing the training</span>
<span class="sd">            step.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;The `Algorithm.training_step()` default implementation no longer &quot;</span>
                <span class="s2">&quot;supports the old API stack! If you would like to continue &quot;</span>
                <span class="s2">&quot;using these &quot;</span>
                <span class="s2">&quot;old APIs with this default `training_step`, simply subclass &quot;</span>
                <span class="s2">&quot;`Algorithm` and override its `training_step` method (copy/paste the &quot;</span>
                <span class="s2">&quot;code and delete this error message).&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Collect a list of Episodes from EnvRunners until we reach the train batch</span>
        <span class="c1"># size.</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">log_time</span><span class="p">((</span><span class="n">TIMERS</span><span class="p">,</span> <span class="n">ENV_RUNNER_SAMPLING_TIMER</span><span class="p">)):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">count_steps_by</span> <span class="o">==</span> <span class="s2">&quot;agent_steps&quot;</span><span class="p">:</span>
                <span class="n">episodes</span><span class="p">,</span> <span class="n">env_runner_results</span> <span class="o">=</span> <span class="n">synchronous_parallel_sample</span><span class="p">(</span>
                    <span class="n">worker_set</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="p">,</span>
                    <span class="n">max_agent_steps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">total_train_batch_size</span><span class="p">,</span>
                    <span class="n">sample_timeout_s</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">sample_timeout_s</span><span class="p">,</span>
                    <span class="n">_uses_new_env_runners</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">_return_metrics</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">episodes</span><span class="p">,</span> <span class="n">env_runner_results</span> <span class="o">=</span> <span class="n">synchronous_parallel_sample</span><span class="p">(</span>
                    <span class="n">worker_set</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="p">,</span>
                    <span class="n">max_env_steps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">total_train_batch_size</span><span class="p">,</span>
                    <span class="n">sample_timeout_s</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">sample_timeout_s</span><span class="p">,</span>
                    <span class="n">_uses_new_env_runners</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">_return_metrics</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="c1"># Reduce EnvRunner metrics over the n EnvRunners.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">aggregate</span><span class="p">(</span><span class="n">env_runner_results</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">ENV_RUNNER_RESULTS</span><span class="p">)</span>

        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">log_time</span><span class="p">((</span><span class="n">TIMERS</span><span class="p">,</span> <span class="n">LEARNER_UPDATE_TIMER</span><span class="p">)):</span>
            <span class="n">learner_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="n">episodes</span><span class="o">=</span><span class="n">episodes</span><span class="p">,</span>
                <span class="n">timesteps</span><span class="o">=</span><span class="p">{</span>
                    <span class="n">NUM_ENV_STEPS_SAMPLED_LIFETIME</span><span class="p">:</span> <span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span><span class="n">NUM_ENV_STEPS_SAMPLED_LIFETIME</span><span class="p">)</span>
                    <span class="p">),</span>
                <span class="p">},</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">learner_results</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">LEARNER_RESULTS</span><span class="p">)</span>

        <span class="c1"># Update weights - after learning on the local worker - on all</span>
        <span class="c1"># remote workers (only those RLModules that were actually trained).</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">log_time</span><span class="p">((</span><span class="n">TIMERS</span><span class="p">,</span> <span class="n">SYNCH_WORKER_WEIGHTS_TIMER</span><span class="p">)):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">(</span>
                <span class="n">from_worker_or_learner_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="p">,</span>
                <span class="n">policies</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">learner_results</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="o">-</span> <span class="p">{</span><span class="n">ALL_MODULES</span><span class="p">}),</span>
                <span class="n">inference_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span></div>


<div class="viewcode-block" id="Algorithm.get_module">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_module.html#ray.rllib.algorithms.algorithm.Algorithm.get_module">[docs]</a>
    <span class="nd">@PublicAPI</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module_id</span><span class="p">:</span> <span class="n">ModuleID</span> <span class="o">=</span> <span class="n">DEFAULT_MODULE_ID</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">RLModule</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the (single-agent) RLModule with `model_id` (None if ID not found).</span>

<span class="sd">        Args:</span>
<span class="sd">            module_id: ID of the (single-agent) RLModule to return from the MARLModule</span>
<span class="sd">                used by the local EnvRunner.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The RLModule found under the ModuleID key inside the local EnvRunner&#39;s</span>
<span class="sd">            MultiRLModule. None if `module_id` doesn&#39;t exist.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner</span><span class="o">.</span><span class="n">module</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">foreach_env_runner</span><span class="p">(</span>
                <span class="k">lambda</span> <span class="n">er</span><span class="p">:</span> <span class="n">er</span><span class="o">.</span><span class="n">module</span><span class="p">,</span>
                <span class="n">remote_worker_ids</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">local_env_runner</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">MultiRLModule</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">module</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">module_id</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">module</span></div>


<div class="viewcode-block" id="Algorithm.add_module">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.add_module.html#ray.rllib.algorithms.algorithm.Algorithm.add_module">[docs]</a>
    <span class="nd">@PublicAPI</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">add_module</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">module_id</span><span class="p">:</span> <span class="n">ModuleID</span><span class="p">,</span>
        <span class="n">module_spec</span><span class="p">:</span> <span class="n">RLModuleSpec</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">config_overrides</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">new_agent_to_module_mapping_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AgentToModuleMappingFn</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">new_should_module_be_updated</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ShouldModuleBeUpdatedFn</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_to_learners</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">add_to_env_runners</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">add_to_eval_env_runners</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MultiRLModuleSpec</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Adds a new (single-agent) RLModule to this Algorithm&#39;s MARLModule.</span>

<span class="sd">        Note that an Algorithm has up to 3 different components to which to add</span>
<span class="sd">        the new module to: The LearnerGroup (with n Learners), the EnvRunnerGroup</span>
<span class="sd">        (with m EnvRunners plus a local one) and - if applicable - the eval</span>
<span class="sd">        EnvRunnerGroup (with o EnvRunners plus a local one).</span>

<span class="sd">        Args:</span>
<span class="sd">            module_id: ID of the RLModule to add to the MARLModule.</span>
<span class="sd">                IMPORTANT: Must not contain characters that</span>
<span class="sd">                are also not allowed in Unix/Win filesystems, such as: `&lt;&gt;:&quot;/|?*`,</span>
<span class="sd">                or a dot, space or backslash at the end of the ID.</span>
<span class="sd">            module_spec: The SingleAgentRLModuleSpec to use for constructing the new</span>
<span class="sd">                RLModule.</span>
<span class="sd">            config_overrides: The `AlgorithmConfig` overrides that should apply to</span>
<span class="sd">                the new Module, if any.</span>
<span class="sd">            new_agent_to_module_mapping_fn: An optional (updated) AgentID to ModuleID</span>
<span class="sd">                mapping function to use from here on. Note that already ongoing</span>
<span class="sd">                episodes will not change their mapping but will use the old mapping till</span>
<span class="sd">                the end of the episode.</span>
<span class="sd">            new_should_module_be_updated: An optional sequence of ModuleIDs or a</span>
<span class="sd">                callable taking ModuleID and SampleBatchType and returning whether the</span>
<span class="sd">                ModuleID should be updated (trained).</span>
<span class="sd">                If None, will keep the existing setup in place. RLModules,</span>
<span class="sd">                whose IDs are not in the list (or for which the callable</span>
<span class="sd">                returns False) will not be updated.</span>
<span class="sd">            add_to_learners: Whether to add the new RLModule to the LearnerGroup</span>
<span class="sd">                (with its n Learners).</span>
<span class="sd">            add_to_env_runners: Whether to add the new RLModule to the EnvRunnerGroup</span>
<span class="sd">                (with its m EnvRunners plus the local one).</span>
<span class="sd">            add_to_eval_env_runners: Whether to add the new RLModule to the eval</span>
<span class="sd">                EnvRunnerGroup (with its o EnvRunners plus the local one).</span>

<span class="sd">        Returns:</span>
<span class="sd">            The new MultiAgentRLModuleSpec (after the RLModule has been added).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">validate_module_id</span><span class="p">(</span><span class="n">module_id</span><span class="p">,</span> <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># The to-be-returned new MultiAgentRLModuleSpec.</span>
        <span class="n">multi_rl_module_spec</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_multi_agent</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Can&#39;t add a new RLModule to a single-agent setup! Make sure that your &quot;</span>
                <span class="s2">&quot;setup is already initially multi-agent by either defining &gt;1 &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;RLModules in your `rl_module_spec` or assigning a ModuleID other &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;than </span><span class="si">{</span><span class="n">DEFAULT_MODULE_ID</span><span class="si">}</span><span class="s2"> to your (only) RLModule.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">([</span><span class="n">add_to_learners</span><span class="p">,</span> <span class="n">add_to_env_runners</span><span class="p">,</span> <span class="n">add_to_eval_env_runners</span><span class="p">]):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;At least one of `add_to_learners`, `add_to_env_runners`, or &quot;</span>
                <span class="s2">&quot;`add_to_eval_env_runners` must be set to True!&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Add to Learners and sync weights.</span>
        <span class="k">if</span> <span class="n">add_to_learners</span><span class="p">:</span>
            <span class="n">multi_rl_module_spec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
                <span class="n">module_id</span><span class="o">=</span><span class="n">module_id</span><span class="p">,</span>
                <span class="n">module_spec</span><span class="o">=</span><span class="n">module_spec</span><span class="p">,</span>
                <span class="n">config_overrides</span><span class="o">=</span><span class="n">config_overrides</span><span class="p">,</span>
                <span class="n">new_should_module_be_updated</span><span class="o">=</span><span class="n">new_should_module_be_updated</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Change our config (AlgorithmConfig) to contain the new Module.</span>
        <span class="c1"># TODO (sven): This is a hack to manipulate the AlgorithmConfig directly,</span>
        <span class="c1">#  but we&#39;ll deprecate config.policies soon anyway.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_is_frozen</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">policies</span><span class="p">[</span><span class="n">module_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">PolicySpec</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">config_overrides</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">multi_agent</span><span class="p">(</span>
                <span class="n">algorithm_config_overrides_per_module</span><span class="o">=</span><span class="p">{</span><span class="n">module_id</span><span class="p">:</span> <span class="n">config_overrides</span><span class="p">}</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">new_agent_to_module_mapping_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">multi_agent</span><span class="p">(</span><span class="n">policy_mapping_fn</span><span class="o">=</span><span class="n">new_agent_to_module_mapping_fn</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">rl_module</span><span class="p">(</span><span class="n">rl_module_spec</span><span class="o">=</span><span class="n">multi_rl_module_spec</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">new_should_module_be_updated</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">multi_agent</span><span class="p">(</span><span class="n">policies_to_train</span><span class="o">=</span><span class="n">new_should_module_be_updated</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">_add</span><span class="p">(</span><span class="n">_env_runner</span><span class="p">,</span> <span class="n">_module_spec</span><span class="o">=</span><span class="n">module_spec</span><span class="p">):</span>
            <span class="c1"># Add the RLModule to the existing one on the EnvRunner.</span>
            <span class="n">_env_runner</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
                <span class="n">module_id</span><span class="o">=</span><span class="n">module_id</span><span class="p">,</span> <span class="n">module</span><span class="o">=</span><span class="n">_module_spec</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="c1"># Update the `agent_to_module_mapping_fn` on the EnvRunner.</span>
            <span class="k">if</span> <span class="n">new_agent_to_module_mapping_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">_env_runner</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">multi_agent</span><span class="p">(</span>
                    <span class="n">policy_mapping_fn</span><span class="o">=</span><span class="n">new_agent_to_module_mapping_fn</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="c1"># Update the `should_module_be_updated` on the EnvRunner. Note that</span>
            <span class="c1"># even though this information is typically not needed by the EnvRunner,</span>
            <span class="c1"># it&#39;s good practice to keep this setting updated everywhere either way.</span>
            <span class="k">if</span> <span class="n">new_should_module_be_updated</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">_env_runner</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">multi_agent</span><span class="p">(</span>
                    <span class="n">policies_to_train</span><span class="o">=</span><span class="n">new_should_module_be_updated</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">MultiRLModuleSpec</span><span class="o">.</span><span class="n">from_module</span><span class="p">(</span><span class="n">_env_runner</span><span class="o">.</span><span class="n">module</span><span class="p">)</span>

        <span class="c1"># Add to (training) EnvRunners and sync weights.</span>
        <span class="k">if</span> <span class="n">add_to_env_runners</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">multi_rl_module_spec</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">multi_rl_module_spec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">foreach_env_runner</span><span class="p">(</span><span class="n">_add</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">foreach_env_runner</span><span class="p">(</span><span class="n">_add</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">(</span>
                <span class="n">from_worker_or_learner_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="p">,</span>
                <span class="n">inference_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="c1"># Add to eval EnvRunners and sync weights.</span>
        <span class="k">if</span> <span class="n">add_to_eval_env_runners</span> <span class="ow">is</span> <span class="kc">True</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">multi_rl_module_spec</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">multi_rl_module_spec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">foreach_env_runner</span><span class="p">(</span>
                    <span class="n">_add</span>
                <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">foreach_env_runner</span><span class="p">(</span><span class="n">_add</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">(</span>
                <span class="n">from_worker_or_learner_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="p">,</span>
                <span class="n">inference_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">multi_rl_module_spec</span></div>


<div class="viewcode-block" id="Algorithm.remove_module">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.remove_module.html#ray.rllib.algorithms.algorithm.Algorithm.remove_module">[docs]</a>
    <span class="nd">@PublicAPI</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">remove_module</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">module_id</span><span class="p">:</span> <span class="n">ModuleID</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">new_agent_to_module_mapping_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AgentToModuleMappingFn</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">new_should_module_be_updated</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ShouldModuleBeUpdatedFn</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">remove_from_learners</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">remove_from_env_runners</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">remove_from_eval_env_runners</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Policy</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Removes a new (single-agent) RLModule from this Algorithm&#39;s MARLModule.</span>

<span class="sd">        Args:</span>
<span class="sd">            module_id: ID of the RLModule to remove from the MARLModule.</span>
<span class="sd">                IMPORTANT: Must not contain characters that</span>
<span class="sd">                are also not allowed in Unix/Win filesystems, such as: `&lt;&gt;:&quot;/|?*`,</span>
<span class="sd">                or a dot, space or backslash at the end of the ID.</span>
<span class="sd">            new_agent_to_module_mapping_fn: An optional (updated) AgentID to ModuleID</span>
<span class="sd">                mapping function to use from here on. Note that already ongoing</span>
<span class="sd">                episodes will not change their mapping but will use the old mapping till</span>
<span class="sd">                the end of the episode.</span>
<span class="sd">            new_should_module_be_updated: An optional sequence of ModuleIDs or a</span>
<span class="sd">                callable taking ModuleID and SampleBatchType and returning whether the</span>
<span class="sd">                ModuleID should be updated (trained).</span>
<span class="sd">                If None, will keep the existing setup in place. RLModules,</span>
<span class="sd">                whose IDs are not in the list (or for which the callable</span>
<span class="sd">                returns False) will not be updated.</span>
<span class="sd">            remove_from_learners: Whether to remove the RLModule from the LearnerGroup</span>
<span class="sd">                (with its n Learners).</span>
<span class="sd">            remove_from_env_runners: Whether to remove the RLModule from the</span>
<span class="sd">                EnvRunnerGroup (with its m EnvRunners plus the local one).</span>
<span class="sd">            remove_from_eval_env_runners: Whether to remove the RLModule from the eval</span>
<span class="sd">                EnvRunnerGroup (with its o EnvRunners plus the local one).</span>

<span class="sd">        Returns:</span>
<span class="sd">            The new MultiAgentRLModuleSpec (after the RLModule has been removed).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># The to-be-returned new MultiAgentRLModuleSpec.</span>
        <span class="n">multi_rl_module_spec</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Remove RLModule from the LearnerGroup.</span>
        <span class="k">if</span> <span class="n">remove_from_learners</span><span class="p">:</span>
            <span class="n">multi_rl_module_spec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">remove_module</span><span class="p">(</span>
                <span class="n">module_id</span><span class="o">=</span><span class="n">module_id</span><span class="p">,</span>
                <span class="n">new_should_module_be_updated</span><span class="o">=</span><span class="n">new_should_module_be_updated</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Change our config (AlgorithmConfig) with the Module removed.</span>
        <span class="c1"># TODO (sven): This is a hack to manipulate the AlgorithmConfig directly,</span>
        <span class="c1">#  but we&#39;ll deprecate config.policies soon anyway.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_is_frozen</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">policies</span><span class="p">[</span><span class="n">module_id</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">algorithm_config_overrides_per_module</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">module_id</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">new_agent_to_module_mapping_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">multi_agent</span><span class="p">(</span><span class="n">policy_mapping_fn</span><span class="o">=</span><span class="n">new_agent_to_module_mapping_fn</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">rl_module</span><span class="p">(</span><span class="n">rl_module_spec</span><span class="o">=</span><span class="n">multi_rl_module_spec</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">new_should_module_be_updated</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">multi_agent</span><span class="p">(</span><span class="n">policies_to_train</span><span class="o">=</span><span class="n">new_should_module_be_updated</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">_remove</span><span class="p">(</span><span class="n">_env_runner</span><span class="p">):</span>
            <span class="c1"># Remove the RLModule from the existing one on the EnvRunner.</span>
            <span class="n">_env_runner</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">remove_module</span><span class="p">(</span><span class="n">module_id</span><span class="o">=</span><span class="n">module_id</span><span class="p">)</span>
            <span class="c1"># Update the `agent_to_module_mapping_fn` on the EnvRunner.</span>
            <span class="k">if</span> <span class="n">new_agent_to_module_mapping_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">_env_runner</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">multi_agent</span><span class="p">(</span>
                    <span class="n">policy_mapping_fn</span><span class="o">=</span><span class="n">new_agent_to_module_mapping_fn</span>
                <span class="p">)</span>
            <span class="c1"># Force reset all ongoing episodes on the EnvRunner to avoid having</span>
            <span class="c1"># different ModuleIDs compute actions for the same AgentID in the same</span>
            <span class="c1"># episode.</span>
            <span class="c1"># TODO (sven): Create an API for this.</span>
            <span class="n">_env_runner</span><span class="o">.</span><span class="n">_needs_initial_reset</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="k">return</span> <span class="n">MultiRLModuleSpec</span><span class="o">.</span><span class="n">from_module</span><span class="p">(</span><span class="n">_env_runner</span><span class="o">.</span><span class="n">module</span><span class="p">)</span>

        <span class="c1"># Remove from (training) EnvRunners and sync weights.</span>
        <span class="k">if</span> <span class="n">remove_from_env_runners</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">multi_rl_module_spec</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">multi_rl_module_spec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">foreach_env_runner</span><span class="p">(</span>
                    <span class="n">_remove</span>
                <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">foreach_env_runner</span><span class="p">(</span><span class="n">_remove</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">(</span>
                <span class="n">from_worker_or_learner_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="p">,</span>
                <span class="n">inference_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Remove from (eval) EnvRunners and sync weights.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">remove_from_eval_env_runners</span> <span class="ow">is</span> <span class="kc">True</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="n">multi_rl_module_spec</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">multi_rl_module_spec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">foreach_env_runner</span><span class="p">(</span>
                    <span class="n">_remove</span>
                <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">foreach_env_runner</span><span class="p">(</span><span class="n">_remove</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">(</span>
                <span class="n">from_worker_or_learner_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="p">,</span>
                <span class="n">inference_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">multi_rl_module_spec</span></div>


<div class="viewcode-block" id="Algorithm.get_policy">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_policy.html#ray.rllib.algorithms.algorithm.Algorithm.get_policy">[docs]</a>
    <span class="nd">@OldAPIStack</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">policy_id</span><span class="p">:</span> <span class="n">PolicyID</span> <span class="o">=</span> <span class="n">DEFAULT_POLICY_ID</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Policy</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return policy for the specified id, or None.</span>

<span class="sd">        Args:</span>
<span class="sd">            policy_id: ID of the policy to return.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner</span><span class="o">.</span><span class="n">get_policy</span><span class="p">(</span><span class="n">policy_id</span><span class="p">)</span></div>


<div class="viewcode-block" id="Algorithm.get_weights">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_weights.html#ray.rllib.algorithms.algorithm.Algorithm.get_weights">[docs]</a>
    <span class="nd">@PublicAPI</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">policies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">PolicyID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return a dict mapping Module/Policy IDs to weights.</span>

<span class="sd">        Args:</span>
<span class="sd">            policies: Optional list of policies to return weights for,</span>
<span class="sd">                or None for all policies.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># New API stack (get weights from LearnerGroup).</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">get_weights</span><span class="p">(</span><span class="n">module_ids</span><span class="o">=</span><span class="n">policies</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner</span><span class="o">.</span><span class="n">get_weights</span><span class="p">(</span><span class="n">policies</span><span class="p">)</span></div>


<div class="viewcode-block" id="Algorithm.set_weights">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.set_weights.html#ray.rllib.algorithms.algorithm.Algorithm.set_weights">[docs]</a>
    <span class="nd">@PublicAPI</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">PolicyID</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set RLModule/Policy weights by Module/Policy ID.</span>

<span class="sd">        Args:</span>
<span class="sd">            weights: Dict mapping ModuleID/PolicyID to weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># New API stack -&gt; Use `set_state` API and specify the LearnerGroup state in the</span>
        <span class="c1"># call, which will automatically take care of weight synching to all EnvRunners.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="n">COMPONENT_LEARNER_GROUP</span><span class="p">:</span> <span class="p">{</span>
                        <span class="n">COMPONENT_LEARNER</span><span class="p">:</span> <span class="p">{</span>
                            <span class="n">COMPONENT_RL_MODULE</span><span class="p">:</span> <span class="n">weights</span><span class="p">,</span>
                        <span class="p">},</span>
                    <span class="p">},</span>
                <span class="p">},</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span></div>


<div class="viewcode-block" id="Algorithm.add_policy">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.add_policy.html#ray.rllib.algorithms.algorithm.Algorithm.add_policy">[docs]</a>
    <span class="nd">@OldAPIStack</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">add_policy</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">policy_id</span><span class="p">:</span> <span class="n">PolicyID</span><span class="p">,</span>
        <span class="n">policy_cls</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Type</span><span class="p">[</span><span class="n">Policy</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Policy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">observation_space</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Space</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">action_space</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Space</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">AlgorithmConfig</span><span class="p">,</span> <span class="n">PartialAlgorithmConfigDict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policy_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">PolicyState</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policy_mapping_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">AgentID</span><span class="p">,</span> <span class="n">EpisodeID</span><span class="p">],</span> <span class="n">PolicyID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policies_to_train</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">Collection</span><span class="p">[</span><span class="n">PolicyID</span><span class="p">],</span>
                <span class="n">Callable</span><span class="p">[[</span><span class="n">PolicyID</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SampleBatchType</span><span class="p">]],</span> <span class="nb">bool</span><span class="p">],</span>
            <span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_to_env_runners</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">add_to_eval_env_runners</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">module_spec</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">RLModuleSpec</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="c1"># Deprecated arg.</span>
        <span class="n">evaluation_workers</span><span class="o">=</span><span class="n">DEPRECATED_VALUE</span><span class="p">,</span>
        <span class="n">add_to_learners</span><span class="o">=</span><span class="n">DEPRECATED_VALUE</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Policy</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Adds a new policy to this Algorithm.</span>

<span class="sd">        Args:</span>
<span class="sd">            policy_id: ID of the policy to add.</span>
<span class="sd">                IMPORTANT: Must not contain characters that</span>
<span class="sd">                are also not allowed in Unix/Win filesystems, such as: `&lt;&gt;:&quot;/|?*`,</span>
<span class="sd">                or a dot, space or backslash at the end of the ID.</span>
<span class="sd">            policy_cls: The Policy class to use for constructing the new Policy.</span>
<span class="sd">                Note: Only one of `policy_cls` or `policy` must be provided.</span>
<span class="sd">            policy: The Policy instance to add to this algorithm. If not None, the</span>
<span class="sd">                given Policy object will be directly inserted into the Algorithm&#39;s</span>
<span class="sd">                local worker and clones of that Policy will be created on all remote</span>
<span class="sd">                workers as well as all evaluation workers.</span>
<span class="sd">                Note: Only one of `policy_cls` or `policy` must be provided.</span>
<span class="sd">            observation_space: The observation space of the policy to add.</span>
<span class="sd">                If None, try to infer this space from the environment.</span>
<span class="sd">            action_space: The action space of the policy to add.</span>
<span class="sd">                If None, try to infer this space from the environment.</span>
<span class="sd">            config: The config object or overrides for the policy to add.</span>
<span class="sd">            policy_state: Optional state dict to apply to the new</span>
<span class="sd">                policy instance, right after its construction.</span>
<span class="sd">            policy_mapping_fn: An optional (updated) policy mapping function</span>
<span class="sd">                to use from here on. Note that already ongoing episodes will</span>
<span class="sd">                not change their mapping but will use the old mapping till</span>
<span class="sd">                the end of the episode.</span>
<span class="sd">            policies_to_train: An optional list of policy IDs to be trained</span>
<span class="sd">                or a callable taking PolicyID and SampleBatchType and</span>
<span class="sd">                returning a bool (trainable or not?).</span>
<span class="sd">                If None, will keep the existing setup in place. Policies,</span>
<span class="sd">                whose IDs are not in the list (or for which the callable</span>
<span class="sd">                returns False) will not be updated.</span>
<span class="sd">            add_to_env_runners: Whether to add the new RLModule to the EnvRunnerGroup</span>
<span class="sd">                (with its m EnvRunners plus the local one).</span>
<span class="sd">            add_to_eval_env_runners: Whether to add the new RLModule to the eval</span>
<span class="sd">                EnvRunnerGroup (with its o EnvRunners plus the local one).</span>
<span class="sd">            module_spec: In the new RLModule API we need to pass in the module_spec for</span>
<span class="sd">                the new module that is supposed to be added. Knowing the policy spec is</span>
<span class="sd">                not sufficient.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The newly added policy (the copy that got added to the local</span>
<span class="sd">            worker). If `workers` was provided, None is returned.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`Algorithm.add_policy()` is not supported on the new API stack w/ &quot;</span>
                <span class="s2">&quot;EnvRunners! Use `Algorithm.add_module()` instead. Also see &quot;</span>
                <span class="s2">&quot;`rllib/examples/self_play_league_based_with_open_spiel.py` for an &quot;</span>
                <span class="s2">&quot;example.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">evaluation_workers</span> <span class="o">!=</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;Algorithm.add_policy(evaluation_workers=...)&quot;</span><span class="p">,</span>
                <span class="n">new</span><span class="o">=</span><span class="s2">&quot;Algorithm.add_policy(add_to_eval_env_runners=...)&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">add_to_learners</span> <span class="o">!=</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;Algorithm.add_policy(add_to_learners=..)&quot;</span><span class="p">,</span>
                <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Hybrid API stack no longer supported by RLlib!&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">validate_module_id</span><span class="p">(</span><span class="n">policy_id</span><span class="p">,</span> <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">add_to_env_runners</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">add_policy</span><span class="p">(</span>
                <span class="n">policy_id</span><span class="p">,</span>
                <span class="n">policy_cls</span><span class="p">,</span>
                <span class="n">policy</span><span class="p">,</span>
                <span class="n">observation_space</span><span class="o">=</span><span class="n">observation_space</span><span class="p">,</span>
                <span class="n">action_space</span><span class="o">=</span><span class="n">action_space</span><span class="p">,</span>
                <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
                <span class="n">policy_state</span><span class="o">=</span><span class="n">policy_state</span><span class="p">,</span>
                <span class="n">policy_mapping_fn</span><span class="o">=</span><span class="n">policy_mapping_fn</span><span class="p">,</span>
                <span class="n">policies_to_train</span><span class="o">=</span><span class="n">policies_to_train</span><span class="p">,</span>
                <span class="n">module_spec</span><span class="o">=</span><span class="n">module_spec</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Add to evaluation workers, if necessary.</span>
        <span class="k">if</span> <span class="n">add_to_eval_env_runners</span> <span class="ow">is</span> <span class="kc">True</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">add_policy</span><span class="p">(</span>
                <span class="n">policy_id</span><span class="p">,</span>
                <span class="n">policy_cls</span><span class="p">,</span>
                <span class="n">policy</span><span class="p">,</span>
                <span class="n">observation_space</span><span class="o">=</span><span class="n">observation_space</span><span class="p">,</span>
                <span class="n">action_space</span><span class="o">=</span><span class="n">action_space</span><span class="p">,</span>
                <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
                <span class="n">policy_state</span><span class="o">=</span><span class="n">policy_state</span><span class="p">,</span>
                <span class="n">policy_mapping_fn</span><span class="o">=</span><span class="n">policy_mapping_fn</span><span class="p">,</span>
                <span class="n">policies_to_train</span><span class="o">=</span><span class="n">policies_to_train</span><span class="p">,</span>
                <span class="n">module_spec</span><span class="o">=</span><span class="n">module_spec</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Return newly added policy (from the local EnvRunner).</span>
        <span class="k">if</span> <span class="n">add_to_env_runners</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_policy</span><span class="p">(</span><span class="n">policy_id</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">add_to_eval_env_runners</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner</span><span class="o">.</span><span class="n">policy_map</span><span class="p">[</span><span class="n">policy_id</span><span class="p">]</span></div>


<div class="viewcode-block" id="Algorithm.remove_policy">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.remove_policy.html#ray.rllib.algorithms.algorithm.Algorithm.remove_policy">[docs]</a>
    <span class="nd">@OldAPIStack</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">remove_policy</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">policy_id</span><span class="p">:</span> <span class="n">PolicyID</span> <span class="o">=</span> <span class="n">DEFAULT_POLICY_ID</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">policy_mapping_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">AgentID</span><span class="p">],</span> <span class="n">PolicyID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policies_to_train</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">Collection</span><span class="p">[</span><span class="n">PolicyID</span><span class="p">],</span>
                <span class="n">Callable</span><span class="p">[[</span><span class="n">PolicyID</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SampleBatchType</span><span class="p">]],</span> <span class="nb">bool</span><span class="p">],</span>
            <span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">remove_from_env_runners</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">remove_from_eval_env_runners</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="c1"># Deprecated args.</span>
        <span class="n">evaluation_workers</span><span class="o">=</span><span class="n">DEPRECATED_VALUE</span><span class="p">,</span>
        <span class="n">remove_from_learners</span><span class="o">=</span><span class="n">DEPRECATED_VALUE</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Removes a policy from this Algorithm.</span>

<span class="sd">        Args:</span>
<span class="sd">            policy_id: ID of the policy to be removed.</span>
<span class="sd">            policy_mapping_fn: An optional (updated) policy mapping function</span>
<span class="sd">                to use from here on. Note that already ongoing episodes will</span>
<span class="sd">                not change their mapping but will use the old mapping till</span>
<span class="sd">                the end of the episode.</span>
<span class="sd">            policies_to_train: An optional list of policy IDs to be trained</span>
<span class="sd">                or a callable taking PolicyID and SampleBatchType and</span>
<span class="sd">                returning a bool (trainable or not?).</span>
<span class="sd">                If None, will keep the existing setup in place. Policies,</span>
<span class="sd">                whose IDs are not in the list (or for which the callable</span>
<span class="sd">                returns False) will not be updated.</span>
<span class="sd">            remove_from_env_runners: Whether to remove the Policy from the</span>
<span class="sd">                EnvRunnerGroup (with its m EnvRunners plus the local one).</span>
<span class="sd">            remove_from_eval_env_runners: Whether to remove the RLModule from the eval</span>
<span class="sd">                EnvRunnerGroup (with its o EnvRunners plus the local one).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">evaluation_workers</span> <span class="o">!=</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;Algorithm.remove_policy(evaluation_workers=...)&quot;</span><span class="p">,</span>
                <span class="n">new</span><span class="o">=</span><span class="s2">&quot;Algorithm.remove_policy(remove_from_eval_env_runners=...)&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">remove_from_eval_env_runners</span> <span class="o">=</span> <span class="n">evaluation_workers</span>
        <span class="k">if</span> <span class="n">remove_from_learners</span> <span class="o">!=</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;Algorithm.remove_policy(remove_from_learners=..)&quot;</span><span class="p">,</span>
                <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Hybrid API stack no longer supported by RLlib!&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">fn</span><span class="p">(</span><span class="n">worker</span><span class="p">):</span>
            <span class="n">worker</span><span class="o">.</span><span class="n">remove_policy</span><span class="p">(</span>
                <span class="n">policy_id</span><span class="o">=</span><span class="n">policy_id</span><span class="p">,</span>
                <span class="n">policy_mapping_fn</span><span class="o">=</span><span class="n">policy_mapping_fn</span><span class="p">,</span>
                <span class="n">policies_to_train</span><span class="o">=</span><span class="n">policies_to_train</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Update all EnvRunner workers.</span>
        <span class="k">if</span> <span class="n">remove_from_env_runners</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">foreach_env_runner</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">local_env_runner</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Update the evaluation worker set&#39;s workers, if required.</span>
        <span class="k">if</span> <span class="n">remove_from_eval_env_runners</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">foreach_env_runner</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">local_env_runner</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="Algorithm.from_state">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.from_state.html#ray.rllib.algorithms.algorithm.Algorithm.from_state">[docs]</a>
    <span class="nd">@OldAPIStack</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_state</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">Dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Algorithm&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Recovers an Algorithm from a state object.</span>

<span class="sd">        The `state` of an instantiated Algorithm can be retrieved by calling its</span>
<span class="sd">        `get_state` method. It contains all information necessary</span>
<span class="sd">        to create the Algorithm from scratch. No access to the original code (e.g.</span>
<span class="sd">        configs, knowledge of the Algorithm&#39;s class, etc..) is needed.</span>

<span class="sd">        Args:</span>
<span class="sd">            state: The state to recover a new Algorithm instance from.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A new Algorithm instance.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">algorithm_class</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">Algorithm</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;algorithm_class&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">algorithm_class</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;No `algorithm_class` key was found in given `state`! &quot;</span>
                <span class="s2">&quot;Cannot create new Algorithm.&quot;</span>
            <span class="p">)</span>
        <span class="c1"># algo_class = get_trainable_cls(algo_class_name)</span>
        <span class="c1"># Create the new algo.</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;config&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">config</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No `config` found in given Algorithm state!&quot;</span><span class="p">)</span>
        <span class="n">new_algo</span> <span class="o">=</span> <span class="n">algorithm_class</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
        <span class="c1"># Set the new algo&#39;s state.</span>
        <span class="n">new_algo</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

        <span class="c1"># Return the new algo.</span>
        <span class="k">return</span> <span class="n">new_algo</span></div>


<div class="viewcode-block" id="Algorithm.export_policy_model">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.export_policy_model.html#ray.rllib.algorithms.algorithm.Algorithm.export_policy_model">[docs]</a>
    <span class="nd">@OldAPIStack</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">export_policy_model</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">export_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">policy_id</span><span class="p">:</span> <span class="n">PolicyID</span> <span class="o">=</span> <span class="n">DEFAULT_POLICY_ID</span><span class="p">,</span>
        <span class="n">onnx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Exports policy model with given policy_id to a local directory.</span>

<span class="sd">        Args:</span>
<span class="sd">            export_dir: Writable local directory.</span>
<span class="sd">            policy_id: Optional policy id to export.</span>
<span class="sd">            onnx: If given, will export model in ONNX format. The</span>
<span class="sd">                value of this parameter set the ONNX OpSet version to use.</span>
<span class="sd">                If None, the output format will be DL framework specific.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">get_policy</span><span class="p">(</span><span class="n">policy_id</span><span class="p">)</span><span class="o">.</span><span class="n">export_model</span><span class="p">(</span><span class="n">export_dir</span><span class="p">,</span> <span class="n">onnx</span><span class="p">)</span></div>


<div class="viewcode-block" id="Algorithm.export_policy_checkpoint">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.export_policy_checkpoint.html#ray.rllib.algorithms.algorithm.Algorithm.export_policy_checkpoint">[docs]</a>
    <span class="nd">@OldAPIStack</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">export_policy_checkpoint</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">export_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">policy_id</span><span class="p">:</span> <span class="n">PolicyID</span> <span class="o">=</span> <span class="n">DEFAULT_POLICY_ID</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Exports Policy checkpoint to a local directory and returns an AIR Checkpoint.</span>

<span class="sd">        Args:</span>
<span class="sd">            export_dir: Writable local directory to store the AIR Checkpoint</span>
<span class="sd">                information into.</span>
<span class="sd">            policy_id: Optional policy ID to export. If not provided, will export</span>
<span class="sd">                &quot;default_policy&quot;. If `policy_id` does not exist in this Algorithm,</span>
<span class="sd">                will raise a KeyError.</span>

<span class="sd">        Raises:</span>
<span class="sd">            KeyError: if `policy_id` cannot be found in this Algorithm.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_policy</span><span class="p">(</span><span class="n">policy_id</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">policy</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Policy with ID </span><span class="si">{</span><span class="n">policy_id</span><span class="si">}</span><span class="s2"> not found in Algorithm!&quot;</span><span class="p">)</span>
        <span class="n">policy</span><span class="o">.</span><span class="n">export_checkpoint</span><span class="p">(</span><span class="n">export_dir</span><span class="p">)</span></div>


<div class="viewcode-block" id="Algorithm.save_checkpoint">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.save_checkpoint.html#ray.rllib.algorithms.algorithm.Algorithm.save_checkpoint">[docs]</a>
    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Exports checkpoint to a local directory.</span>

<span class="sd">        The structure of an Algorithm checkpoint dir will be as follows::</span>

<span class="sd">            policies/</span>
<span class="sd">                pol_1/</span>
<span class="sd">                    policy_state.pkl</span>
<span class="sd">                pol_2/</span>
<span class="sd">                    policy_state.pkl</span>
<span class="sd">            learner/</span>
<span class="sd">                learner_state.json</span>
<span class="sd">                module_state/</span>
<span class="sd">                    module_1/</span>
<span class="sd">                        ...</span>
<span class="sd">                optimizer_state/</span>
<span class="sd">                    optimizers_module_1/</span>
<span class="sd">                        ...</span>
<span class="sd">            rllib_checkpoint.json</span>
<span class="sd">            algorithm_state.pkl</span>

<span class="sd">        Note: `rllib_checkpoint.json` contains a &quot;version&quot; key (e.g. with value 0.1)</span>
<span class="sd">        helping RLlib to remain backward compatible wrt. restoring from checkpoints from</span>
<span class="sd">        Ray 2.0 onwards.</span>

<span class="sd">        Args:</span>
<span class="sd">            checkpoint_dir: The directory where the checkpoint files will be stored.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># New API stack: Delegate to the `Checkpointable` implementation of</span>
        <span class="c1"># `save_to_path()` and return.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_rl_module_and_learner</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">save_to_path</span><span class="p">(</span>
                <span class="n">checkpoint_dir</span><span class="p">,</span>
                <span class="n">use_msgpack</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_use_msgpack_checkpoints</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span>

        <span class="n">checkpoint_dir</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span>

        <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__getstate__</span><span class="p">()</span>

        <span class="c1"># Extract policy states from worker state (Policies get their own</span>
        <span class="c1"># checkpoint sub-dirs).</span>
        <span class="n">policy_states</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="s2">&quot;worker&quot;</span> <span class="ow">in</span> <span class="n">state</span> <span class="ow">and</span> <span class="s2">&quot;policy_states&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;worker&quot;</span><span class="p">]:</span>
            <span class="n">policy_states</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;worker&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;policy_states&quot;</span><span class="p">,</span> <span class="p">{})</span>

        <span class="c1"># Add RLlib checkpoint version.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_rl_module_and_learner</span><span class="p">:</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;checkpoint_version&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">CHECKPOINT_VERSION_LEARNER_AND_ENV_RUNNER</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;checkpoint_version&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">CHECKPOINT_VERSION</span>

        <span class="c1"># Write state (w/o policies) to disk.</span>
        <span class="n">state_file</span> <span class="o">=</span> <span class="n">checkpoint_dir</span> <span class="o">/</span> <span class="s2">&quot;algorithm_state.pkl&quot;</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">state_file</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

        <span class="c1"># Write rllib_checkpoint.json.</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">checkpoint_dir</span> <span class="o">/</span> <span class="s2">&quot;rllib_checkpoint.json&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;Algorithm&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;checkpoint_version&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;checkpoint_version&quot;</span><span class="p">]),</span>
                    <span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;cloudpickle&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;state_file&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">state_file</span><span class="p">),</span>
                    <span class="s2">&quot;policy_ids&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(</span><span class="n">policy_states</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                    <span class="s2">&quot;ray_version&quot;</span><span class="p">:</span> <span class="n">ray</span><span class="o">.</span><span class="n">__version__</span><span class="p">,</span>
                    <span class="s2">&quot;ray_commit&quot;</span><span class="p">:</span> <span class="n">ray</span><span class="o">.</span><span class="n">__commit__</span><span class="p">,</span>
                <span class="p">},</span>
                <span class="n">f</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Old API stack: Write individual policies to disk, each in their own</span>
        <span class="c1"># sub-directory.</span>
        <span class="k">for</span> <span class="n">pid</span><span class="p">,</span> <span class="n">policy_state</span> <span class="ow">in</span> <span class="n">policy_states</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># From here on, disallow policyIDs that would not work as directory names.</span>
            <span class="n">validate_module_id</span><span class="p">(</span><span class="n">pid</span><span class="p">,</span> <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">policy_dir</span> <span class="o">=</span> <span class="n">checkpoint_dir</span> <span class="o">/</span> <span class="s2">&quot;policies&quot;</span> <span class="o">/</span> <span class="n">pid</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">policy_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_policy</span><span class="p">(</span><span class="n">pid</span><span class="p">)</span>
            <span class="n">policy</span><span class="o">.</span><span class="n">export_checkpoint</span><span class="p">(</span><span class="n">policy_dir</span><span class="p">,</span> <span class="n">policy_state</span><span class="o">=</span><span class="n">policy_state</span><span class="p">)</span>

        <span class="c1"># If we are using the learner API (hybrid API stack) -&gt; Save the learner group&#39;s</span>
        <span class="c1"># state inside a &quot;learner&quot; subdir. Note that this is not in line with the</span>
        <span class="c1"># new Checkpointable API, but makes this case backward compatible.</span>
        <span class="c1"># The new Checkpointable API is only strictly applied anyways to the</span>
        <span class="c1"># new API stack.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_rl_module_and_learner</span><span class="p">:</span>
            <span class="n">learner_state_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="s2">&quot;learner&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">save_to_path</span><span class="p">(</span><span class="n">learner_state_dir</span><span class="p">)</span></div>


    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># New API stack: Delegate to the `Checkpointable` implementation of</span>
        <span class="c1"># `restore_from_path()`.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_rl_module_and_learner</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">restore_from_path</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Checkpoint is provided as a local directory.</span>
            <span class="c1"># Restore from the checkpoint file or dir.</span>
            <span class="n">checkpoint_info</span> <span class="o">=</span> <span class="n">get_checkpoint_info</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span>
            <span class="n">checkpoint_data</span> <span class="o">=</span> <span class="n">Algorithm</span><span class="o">.</span><span class="n">_checkpoint_info_to_algorithm_state</span><span class="p">(</span>
                <span class="n">checkpoint_info</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">checkpoint_data</span><span class="p">)</span>

        <span class="c1"># Call the `on_checkpoint_loaded` callback.</span>
        <span class="n">make_callback</span><span class="p">(</span>
            <span class="s2">&quot;on_checkpoint_loaded&quot;</span><span class="p">,</span>
            <span class="n">callbacks_objects</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="p">,</span>
            <span class="n">callbacks_functions</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">callbacks_on_checkpoint_loaded</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="bp">self</span><span class="p">),</span>
        <span class="p">)</span>

<div class="viewcode-block" id="Algorithm.get_state">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_state.html#ray.rllib.algorithms.algorithm.Algorithm.get_state">[docs]</a>
    <span class="nd">@override</span><span class="p">(</span><span class="n">Checkpointable</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_state</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">components</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Collection</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">not_components</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Collection</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">StateDict</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Algorithm.get_state() not supported on the old API stack! &quot;</span>
                <span class="s2">&quot;Use Algorithm.__getstate__() instead.&quot;</span>
            <span class="p">)</span>

        <span class="n">state</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Get (local) EnvRunner state (w/o RLModule).</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_online</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_component</span><span class="p">(</span><span class="n">COMPONENT_ENV_RUNNER</span><span class="p">,</span> <span class="n">components</span><span class="p">,</span> <span class="n">not_components</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner</span><span class="p">:</span>
                    <span class="n">state</span><span class="p">[</span><span class="n">COMPONENT_ENV_RUNNER</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span>
                        <span class="n">components</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_subcomponents</span><span class="p">(</span>
                            <span class="n">COMPONENT_RL_MODULE</span><span class="p">,</span> <span class="n">components</span>
                        <span class="p">),</span>
                        <span class="n">not_components</span><span class="o">=</span><span class="n">force_list</span><span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">_get_subcomponents</span><span class="p">(</span><span class="n">COMPONENT_RL_MODULE</span><span class="p">,</span> <span class="n">not_components</span><span class="p">)</span>
                        <span class="p">)</span>
                        <span class="c1"># We don&#39;t want the RLModule state from the EnvRunners (it&#39;s</span>
                        <span class="c1"># `inference_only` anyway and already provided in full by the</span>
                        <span class="c1"># Learners).</span>
                        <span class="o">+</span> <span class="p">[</span><span class="n">COMPONENT_RL_MODULE</span><span class="p">],</span>
                        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">state</span><span class="p">[</span><span class="n">COMPONENT_ENV_RUNNER</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                        <span class="n">COMPONENT_ENV_TO_MODULE_CONNECTOR</span><span class="p">:</span> <span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">env_to_module_connector</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>
                        <span class="p">),</span>
                        <span class="n">COMPONENT_MODULE_TO_ENV_CONNECTOR</span><span class="p">:</span> <span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">module_to_env_connector</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>
                        <span class="p">),</span>
                    <span class="p">}</span>

                <span class="c1"># Get (local) evaluation EnvRunner state (w/o RLModule).</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_component</span><span class="p">(</span>
            <span class="n">COMPONENT_EVAL_ENV_RUNNER</span><span class="p">,</span> <span class="n">components</span><span class="p">,</span> <span class="n">not_components</span>
        <span class="p">):</span>
            <span class="n">state</span><span class="p">[</span><span class="n">COMPONENT_EVAL_ENV_RUNNER</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span>
                <span class="n">components</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_subcomponents</span><span class="p">(</span><span class="n">COMPONENT_RL_MODULE</span><span class="p">,</span> <span class="n">components</span><span class="p">),</span>
                <span class="n">not_components</span><span class="o">=</span><span class="n">force_list</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_get_subcomponents</span><span class="p">(</span><span class="n">COMPONENT_RL_MODULE</span><span class="p">,</span> <span class="n">not_components</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="c1"># We don&#39;t want the RLModule state from the EnvRunners (it&#39;s</span>
                <span class="c1"># `inference_only` anyway and already provided in full by the Learners).</span>
                <span class="o">+</span> <span class="p">[</span><span class="n">COMPONENT_RL_MODULE</span><span class="p">],</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Get LearnerGroup state (w/ RLModule).</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_component</span><span class="p">(</span><span class="n">COMPONENT_LEARNER_GROUP</span><span class="p">,</span> <span class="n">components</span><span class="p">,</span> <span class="n">not_components</span><span class="p">):</span>
            <span class="n">state</span><span class="p">[</span><span class="n">COMPONENT_LEARNER_GROUP</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span>
                <span class="n">components</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_subcomponents</span><span class="p">(</span><span class="n">COMPONENT_LEARNER_GROUP</span><span class="p">,</span> <span class="n">components</span><span class="p">),</span>
                <span class="n">not_components</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_subcomponents</span><span class="p">(</span>
                    <span class="n">COMPONENT_LEARNER_GROUP</span><span class="p">,</span> <span class="n">not_components</span>
                <span class="p">),</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Get entire MetricsLogger state.</span>
        <span class="c1"># TODO (sven): Make `MetricsLogger` a Checkpointable.</span>
        <span class="n">state</span><span class="p">[</span><span class="n">COMPONENT_METRICS_LOGGER</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>

        <span class="c1"># Save current `training_iteration`.</span>
        <span class="n">state</span><span class="p">[</span><span class="n">TRAINING_ITERATION</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_iteration</span>

        <span class="k">return</span> <span class="n">state</span></div>


<div class="viewcode-block" id="Algorithm.set_state">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.set_state.html#ray.rllib.algorithms.algorithm.Algorithm.set_state">[docs]</a>
    <span class="nd">@override</span><span class="p">(</span><span class="n">Checkpointable</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">set_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">StateDict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Set the (training) EnvRunners&#39; states.</span>
        <span class="k">if</span> <span class="n">COMPONENT_ENV_RUNNER</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">env_runner</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="n">COMPONENT_ENV_RUNNER</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">env_to_module_connector</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span>
                    <span class="n">state</span><span class="p">[</span><span class="n">COMPONENT_ENV_RUNNER</span><span class="p">][</span><span class="n">COMPONENT_ENV_TO_MODULE_CONNECTOR</span><span class="p">]</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">module_to_env_connector</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span>
                    <span class="n">state</span><span class="p">[</span><span class="n">COMPONENT_ENV_RUNNER</span><span class="p">][</span><span class="n">COMPONENT_MODULE_TO_ENV_CONNECTOR</span><span class="p">]</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">sync_env_runner_states</span><span class="p">(</span>
                <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
                <span class="n">from_worker</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_runner</span><span class="p">,</span>
                <span class="n">env_to_module</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_to_module_connector</span><span class="p">,</span>
                <span class="n">module_to_env</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">module_to_env_connector</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Set the (eval) EnvRunners&#39; states.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span> <span class="ow">and</span> <span class="n">COMPONENT_EVAL_ENV_RUNNER</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="n">COMPONENT_ENV_RUNNER</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">sync_env_runner_states</span><span class="p">(</span>
                <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="p">,</span>
                <span class="n">from_worker</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_runner</span><span class="p">,</span>
                <span class="n">env_to_module</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_to_module_connector</span><span class="p">,</span>
                <span class="n">module_to_env</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">module_to_env_connector</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Set the LearnerGroup&#39;s state.</span>
        <span class="k">if</span> <span class="n">COMPONENT_LEARNER_GROUP</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="n">COMPONENT_LEARNER_GROUP</span><span class="p">])</span>
            <span class="c1"># Sync new weights to all EnvRunners.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">(</span>
                <span class="n">from_worker_or_learner_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="p">,</span>
                <span class="n">inference_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">(</span>
                    <span class="n">from_worker_or_learner_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="p">,</span>
                    <span class="n">inference_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="c1"># TODO (sven): Make `MetricsLogger` a Checkpointable.</span>
        <span class="k">if</span> <span class="n">COMPONENT_METRICS_LOGGER</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="n">COMPONENT_METRICS_LOGGER</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">TRAINING_ITERATION</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_iteration</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="n">TRAINING_ITERATION</span><span class="p">]</span></div>


    <span class="nd">@override</span><span class="p">(</span><span class="n">Checkpointable</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_checkpointable_components</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;Checkpointable&quot;</span><span class="p">]]:</span>
        <span class="n">components</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">(</span><span class="n">COMPONENT_LEARNER_GROUP</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="p">),</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_online</span><span class="p">:</span>
            <span class="n">components</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="p">(</span><span class="n">COMPONENT_ENV_RUNNER</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner</span><span class="p">:</span>
            <span class="n">components</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="p">(</span>
                    <span class="n">COMPONENT_EVAL_ENV_RUNNER</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">components</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Checkpointable</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_ctor_args_and_kwargs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get_state</span><span class="p">(),),</span>  <span class="c1"># *args,</span>
            <span class="p">{},</span>  <span class="c1"># **kwargs</span>
        <span class="p">)</span>

<div class="viewcode-block" id="Algorithm.restore_from_path">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.restore_from_path.html#ray.rllib.algorithms.algorithm.Algorithm.restore_from_path">[docs]</a>
    <span class="nd">@override</span><span class="p">(</span><span class="n">Checkpointable</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">restore_from_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># Override from parent method, b/c we might have to sync the EnvRunner weights</span>
        <span class="c1"># after having restored/loaded the LearnerGroup state.</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">restore_from_path</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Sync EnvRunners, if LearnerGroup&#39;s checkpoint can be found in path</span>
        <span class="c1"># or user loaded a subcomponent within the LearnerGroup (for example a module).</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="n">COMPONENT_LEARNER_GROUP</span><span class="p">)</span><span class="o">.</span><span class="n">is_dir</span><span class="p">()</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="s2">&quot;component&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="ow">and</span> <span class="n">COMPONENT_LEARNER_GROUP</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;component&quot;</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="c1"># Make also sure, all (training) EnvRunners get the just loaded weights, but</span>
            <span class="c1"># only the inference-only ones.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">(</span>
                <span class="n">from_worker_or_learner_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="p">,</span>
                <span class="n">inference_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span></div>


    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">log_result</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">result</span><span class="p">:</span> <span class="n">ResultDict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Log after the callback is invoked, so that the user has a chance</span>
        <span class="c1"># to mutate the result.</span>
        <span class="c1"># TODO (sven): It might not make sense to pass in the MetricsLogger at this late</span>
        <span class="c1">#  point in time. In here, the result dict has already been &quot;compiled&quot; (reduced)</span>
        <span class="c1">#  by the MetricsLogger and there is probably no point in adding more Stats</span>
        <span class="c1">#  here.</span>
        <span class="n">make_callback</span><span class="p">(</span>
            <span class="s2">&quot;on_train_result&quot;</span><span class="p">,</span>
            <span class="n">callbacks_objects</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="p">,</span>
            <span class="n">callbacks_functions</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">callbacks_on_train_result</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
                <span class="n">algorithm</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">metrics_logger</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">,</span>
                <span class="n">result</span><span class="o">=</span><span class="n">result</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="c1"># Then log according to Trainable&#39;s logging logic.</span>
        <span class="n">Trainable</span><span class="o">.</span><span class="n">log_result</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">cleanup</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Stop all Learners.</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;learner_group&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>

        <span class="c1"># Stop all aggregation actors.</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_aggregator_actor_manager&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_aggregator_actor_manager</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_aggregator_actor_manager</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>

        <span class="c1"># Stop all EnvRunners.</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;env_runner_group&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;eval_env_runner_group&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>

    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="nd">@classmethod</span>
    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">default_resource_request</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">AlgorithmConfig</span><span class="p">,</span> <span class="n">PartialAlgorithmConfigDict</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Resources</span><span class="p">,</span> <span class="n">PlacementGroupFactory</span><span class="p">]:</span>
        <span class="n">config</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">get_default_config</span><span class="p">()</span><span class="o">.</span><span class="n">update_from_dict</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="n">config</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span>
        <span class="n">config</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
        <span class="n">eval_config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get_evaluation_config_object</span><span class="p">()</span>
        <span class="n">eval_config</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span>
        <span class="n">eval_config</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">enable_rl_module_and_learner</span><span class="p">:</span>
            <span class="n">main_process</span> <span class="o">=</span> <span class="n">_get_main_process_bundle</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">main_process</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="n">config</span><span class="o">.</span><span class="n">num_cpus_for_main_process</span><span class="p">,</span>
                <span class="s2">&quot;GPU&quot;</span><span class="p">:</span> <span class="p">(</span>
                    <span class="mi">0</span>
                    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">_fake_gpus</span>
                    <span class="k">else</span> <span class="n">config</span><span class="o">.</span><span class="n">num_gpus</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">config</span><span class="o">.</span><span class="n">enable_rl_module_and_learner</span>
                    <span class="k">else</span> <span class="mi">0</span>
                <span class="p">),</span>
            <span class="p">}</span>

        <span class="n">env_runner_bundles</span> <span class="o">=</span> <span class="n">_get_env_runner_bundles</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_should_create_evaluation_env_runners</span><span class="p">(</span><span class="n">eval_config</span><span class="p">):</span>
            <span class="n">eval_env_runner_bundles</span> <span class="o">=</span> <span class="n">_get_env_runner_bundles</span><span class="p">(</span><span class="n">eval_config</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">eval_env_runner_bundles</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_should_create_offline_evaluation_runners</span><span class="p">(</span><span class="n">eval_config</span><span class="p">):</span>
            <span class="n">offline_eval_runner_bundles</span> <span class="o">=</span> <span class="n">_get_offline_eval_runner_bundles</span><span class="p">(</span><span class="n">eval_config</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">offline_eval_runner_bundles</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">learner_bundles</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">enable_rl_module_and_learner</span><span class="p">:</span>
            <span class="n">learner_bundles</span> <span class="o">=</span> <span class="n">_get_learner_bundles</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="n">bundles</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">[</span><span class="n">main_process</span><span class="p">]</span>
            <span class="o">+</span> <span class="n">env_runner_bundles</span>
            <span class="o">+</span> <span class="n">eval_env_runner_bundles</span>
            <span class="o">+</span> <span class="n">offline_eval_runner_bundles</span>
            <span class="o">+</span> <span class="n">learner_bundles</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">PlacementGroupFactory</span><span class="p">(</span>
            <span class="n">bundles</span><span class="o">=</span><span class="n">bundles</span><span class="p">,</span>
            <span class="n">strategy</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">placement_strategy</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_before_evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Pre-evaluation callback.&quot;&quot;&quot;</span>
        <span class="k">pass</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_get_env_id_and_creator</span><span class="p">(</span>
        <span class="n">env_specifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">EnvType</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">config</span><span class="p">:</span> <span class="n">AlgorithmConfig</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">EnvCreator</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns env_id and creator callable given original env id from config.</span>

<span class="sd">        Args:</span>
<span class="sd">            env_specifier: An env class, an already tune registered env ID, a known</span>
<span class="sd">                gym env name, or None (if no env is used).</span>
<span class="sd">            config: The AlgorithmConfig object.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple consisting of a) env ID string and b) env creator callable.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Environment is specified via a string.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">env_specifier</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="c1"># An already registered env.</span>
            <span class="k">if</span> <span class="n">_global_registry</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="n">ENV_CREATOR</span><span class="p">,</span> <span class="n">env_specifier</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">env_specifier</span><span class="p">,</span> <span class="n">_global_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">ENV_CREATOR</span><span class="p">,</span> <span class="n">env_specifier</span><span class="p">)</span>

            <span class="c1"># A class path specifier.</span>
            <span class="k">elif</span> <span class="s2">&quot;.&quot;</span> <span class="ow">in</span> <span class="n">env_specifier</span><span class="p">:</span>

                <span class="k">def</span><span class="w"> </span><span class="nf">env_creator_from_classpath</span><span class="p">(</span><span class="n">env_context</span><span class="p">):</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="n">env_obj</span> <span class="o">=</span> <span class="n">from_config</span><span class="p">(</span><span class="n">env_specifier</span><span class="p">,</span> <span class="n">env_context</span><span class="p">)</span>
                    <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="n">EnvError</span><span class="p">(</span>
                            <span class="n">ERR_MSG_INVALID_ENV_DESCRIPTOR</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">env_specifier</span><span class="p">)</span>
                        <span class="p">)</span>
                    <span class="k">return</span> <span class="n">env_obj</span>

                <span class="k">return</span> <span class="n">env_specifier</span><span class="p">,</span> <span class="n">env_creator_from_classpath</span>
            <span class="c1"># Try gym/PyBullet.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">env_specifier</span><span class="p">,</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
                    <span class="n">_gym_env_creator</span><span class="p">,</span> <span class="n">env_descriptor</span><span class="o">=</span><span class="n">env_specifier</span>
                <span class="p">)</span>

        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">env_specifier</span><span class="p">,</span> <span class="nb">type</span><span class="p">):</span>
            <span class="n">env_id</span> <span class="o">=</span> <span class="n">env_specifier</span>  <span class="c1"># .__name__</span>

            <span class="k">if</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;remote_worker_envs&quot;</span><span class="p">]:</span>
                <span class="c1"># Check gym version (0.22 or higher?).</span>
                <span class="c1"># If &gt; 0.21, can&#39;t perform auto-wrapping of the given class as this</span>
                <span class="c1"># would lead to a pickle error.</span>
                <span class="n">gym_version</span> <span class="o">=</span> <span class="n">importlib</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">version</span><span class="p">(</span><span class="s2">&quot;gym&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">gym_version</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="s2">&quot;0.22&quot;</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;Cannot specify a gym.Env class via `config.env` while setting &quot;</span>
                        <span class="s2">&quot;`config.remote_worker_env=True` AND your gym version is &gt;= &quot;</span>
                        <span class="s2">&quot;0.22! Try installing an older version of gym or set `config.&quot;</span>
                        <span class="s2">&quot;remote_worker_env=False`.&quot;</span>
                    <span class="p">)</span>

                <span class="nd">@ray</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="n">num_cpus</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">class</span><span class="w"> </span><span class="nc">_wrapper</span><span class="p">(</span><span class="n">env_specifier</span><span class="p">):</span>
                    <span class="c1"># Add convenience `_get_spaces` and `_is_multi_agent`</span>
                    <span class="c1"># methods:</span>
                    <span class="k">def</span><span class="w"> </span><span class="nf">_get_spaces</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span>

                    <span class="k">def</span><span class="w"> </span><span class="nf">_is_multi_agent</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                        <span class="kn">from</span><span class="w"> </span><span class="nn">ray.rllib.env.multi_agent_env</span><span class="w"> </span><span class="kn">import</span> <span class="n">MultiAgentEnv</span>

                        <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">MultiAgentEnv</span><span class="p">)</span>

                <span class="k">return</span> <span class="n">env_id</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">_wrapper</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
            <span class="c1"># gym.Env-subclass: Also go through our RLlib gym-creator.</span>
            <span class="k">elif</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">env_specifier</span><span class="p">,</span> <span class="n">gym</span><span class="o">.</span><span class="n">Env</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">env_id</span><span class="p">,</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
                    <span class="n">_gym_env_creator</span><span class="p">,</span>
                    <span class="n">env_descriptor</span><span class="o">=</span><span class="n">env_specifier</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="c1"># All other env classes: Call c&#39;tor directly.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">env_id</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">env_specifier</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>

        <span class="c1"># No env -&gt; Env creator always returns None.</span>
        <span class="k">elif</span> <span class="n">env_specifier</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">env_config</span><span class="p">:</span> <span class="kc">None</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> is an invalid env specifier. &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">env_specifier</span><span class="p">)</span>
                <span class="o">+</span> <span class="s2">&quot;You can specify a custom env as either a class &quot;</span>
                <span class="s1">&#39;(e.g., YourEnvCls) or a registered env id (e.g., &quot;your_env&quot;).&#39;</span>
            <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_sync_filters_if_needed</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">central_worker</span><span class="p">:</span> <span class="n">EnvRunner</span><span class="p">,</span>
        <span class="n">workers</span><span class="p">:</span> <span class="n">EnvRunnerGroup</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">AlgorithmConfig</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Synchronizes the filter stats from `workers` to `central_worker`.</span>

<span class="sd">        .. and broadcasts the central_worker&#39;s filter stats back to all `workers`</span>
<span class="sd">        (if configured).</span>

<span class="sd">        Args:</span>
<span class="sd">            central_worker: The worker to sync/aggregate all `workers`&#39; filter stats to</span>
<span class="sd">                and from which to (possibly) broadcast the updated filter stats back to</span>
<span class="sd">                `workers`.</span>
<span class="sd">            workers: The EnvRunnerGroup, whose EnvRunners&#39; filter stats should be used</span>
<span class="sd">                for aggregation on `central_worker` and which (possibly) get updated</span>
<span class="sd">                from `central_worker` after the sync.</span>
<span class="sd">            config: The algorithm config instance. This is used to determine, whether</span>
<span class="sd">                syncing from `workers` should happen at all and whether broadcasting</span>
<span class="sd">                back to `workers` (after possible syncing) should happen.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">central_worker</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">observation_filter</span> <span class="o">!=</span> <span class="s2">&quot;NoFilter&quot;</span><span class="p">:</span>
            <span class="n">FilterManager</span><span class="o">.</span><span class="n">synchronize</span><span class="p">(</span>
                <span class="n">central_worker</span><span class="o">.</span><span class="n">filters</span><span class="p">,</span>
                <span class="n">workers</span><span class="p">,</span>
                <span class="n">update_remote</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">update_worker_filter_stats</span><span class="p">,</span>
                <span class="n">timeout_seconds</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">sync_filters_on_rollout_workers_timeout_s</span><span class="p">,</span>
                <span class="n">use_remote_data_for_update</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">use_worker_filter_stats</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">resource_help</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">AlgorithmConfig</span><span class="p">,</span> <span class="n">AlgorithmConfigDict</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">You can adjust the resource requests of RLlib Algorithms by calling &quot;</span>
            <span class="s2">&quot;`AlgorithmConfig.env_runners(&quot;</span>
            <span class="s2">&quot;num_env_runners=.., num_cpus_per_env_runner=.., &quot;</span>
            <span class="s2">&quot;num_gpus_per_env_runner=.., ..)` and &quot;</span>
            <span class="s2">&quot;`AgorithmConfig.learners(num_learners=.., num_gpus_per_learner=..)`. See &quot;</span>
            <span class="s2">&quot;the `ray.rllib.algorithms.algorithm_config.AlgorithmConfig` classes &quot;</span>
            <span class="s2">&quot;(each Algorithm has its own subclass of this class) for more info.</span><span class="se">\n\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;The config of this Algorithm is: </span><span class="si">{</span><span class="n">config</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_auto_filled_metrics</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">now</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">datetime</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">time_this_iter</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">timestamp</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">debug_metrics_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="c1"># Override this method to make sure, the `config` key of the returned results</span>
        <span class="c1"># contains the proper Tune config dict (instead of an AlgorithmConfig object).</span>
        <span class="n">auto_filled</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_auto_filled_metrics</span><span class="p">(</span>
            <span class="n">now</span><span class="p">,</span> <span class="n">time_this_iter</span><span class="p">,</span> <span class="n">timestamp</span><span class="p">,</span> <span class="n">debug_metrics_only</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;config&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">auto_filled</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;`config` key not found in auto-filled results dict!&quot;</span><span class="p">)</span>

        <span class="c1"># If `config` key is no dict (but AlgorithmConfig object) -&gt;</span>
        <span class="c1"># make sure, it&#39;s a dict to not break Tune APIs.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">auto_filled</span><span class="p">[</span><span class="s2">&quot;config&quot;</span><span class="p">],</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">auto_filled</span><span class="p">[</span><span class="s2">&quot;config&quot;</span><span class="p">],</span> <span class="n">AlgorithmConfig</span><span class="p">)</span>
            <span class="n">auto_filled</span><span class="p">[</span><span class="s2">&quot;config&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">auto_filled</span><span class="p">[</span><span class="s2">&quot;config&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">auto_filled</span>

<div class="viewcode-block" id="Algorithm.merge_algorithm_configs">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.merge_algorithm_configs.html#ray.rllib.algorithms.algorithm.Algorithm.merge_algorithm_configs">[docs]</a>
    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">merge_algorithm_configs</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">config1</span><span class="p">:</span> <span class="n">AlgorithmConfigDict</span><span class="p">,</span>
        <span class="n">config2</span><span class="p">:</span> <span class="n">PartialAlgorithmConfigDict</span><span class="p">,</span>
        <span class="n">_allow_unknown_configs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AlgorithmConfigDict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Merges a complete Algorithm config dict with a partial override dict.</span>

<span class="sd">        Respects nested structures within the config dicts. The values in the</span>
<span class="sd">        partial override dict take priority.</span>

<span class="sd">        Args:</span>
<span class="sd">            config1: The complete Algorithm&#39;s dict to be merged (overridden)</span>
<span class="sd">                with `config2`.</span>
<span class="sd">            config2: The partial override config dict to merge on top of</span>
<span class="sd">                `config1`.</span>
<span class="sd">            _allow_unknown_configs: If True, keys in `config2` that don&#39;t exist</span>
<span class="sd">                in `config1` are allowed and will be added to the final config.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The merged full algorithm config dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">config1</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">config1</span><span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;callbacks&quot;</span> <span class="ow">in</span> <span class="n">config2</span> <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="n">config2</span><span class="p">[</span><span class="s2">&quot;callbacks&quot;</span><span class="p">])</span> <span class="ow">is</span> <span class="nb">dict</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="s2">&quot;callbacks dict interface&quot;</span><span class="p">,</span>
                <span class="s2">&quot;a class extending rllib.callbacks.callbacks.RLlibCallback; &quot;</span>
                <span class="s2">&quot;see `rllib/examples/metrics/custom_metrics_and_callbacks.py` for an &quot;</span>
                <span class="s2">&quot;example.&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">_allow_unknown_configs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_allow_unknown_configs</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_allow_unknown_configs</span>
        <span class="k">return</span> <span class="n">deep_update</span><span class="p">(</span>
            <span class="n">config1</span><span class="p">,</span>
            <span class="n">config2</span><span class="p">,</span>
            <span class="n">_allow_unknown_configs</span><span class="p">,</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_allow_unknown_subkeys</span><span class="p">,</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_override_all_subkeys_if_type_changes</span><span class="p">,</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_override_all_key_list</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="Algorithm.validate_env">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.validate_env.html#ray.rllib.algorithms.algorithm.Algorithm.validate_env">[docs]</a>
    <span class="nd">@staticmethod</span>
    <span class="nd">@ExperimentalAPI</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">validate_env</span><span class="p">(</span><span class="n">env</span><span class="p">:</span> <span class="n">EnvType</span><span class="p">,</span> <span class="n">env_context</span><span class="p">:</span> <span class="n">EnvContext</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Env validator function for this Algorithm class.</span>

<span class="sd">        Override this in child classes to define custom validation</span>
<span class="sd">        behavior.</span>

<span class="sd">        Args:</span>
<span class="sd">            env: The (sub-)environment to validate. This is normally a</span>
<span class="sd">                single sub-environment (e.g. a gym.Env) within a vectorized</span>
<span class="sd">                setup.</span>
<span class="sd">            env_context: The EnvContext to configure the environment.</span>

<span class="sd">        Raises:</span>
<span class="sd">            Exception: in case something is wrong with the given environment.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_run_one_training_iteration</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">ResultDict</span><span class="p">,</span> <span class="s2">&quot;TrainIterCtx&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Runs one training iteration (`self.iteration` will be +1 after this).</span>

<span class="sd">        Calls `self.training_step()` repeatedly until the configured minimum time (sec),</span>
<span class="sd">        minimum sample- or minimum training steps have been reached.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The ResultDict from the last call to `training_step()`. Note that even</span>
<span class="sd">            though we only return the last ResultDict, the user still has full control</span>
<span class="sd">            over the history and reduce behavior of individual metrics at the time these</span>
<span class="sd">            metrics are logged with `self.metrics.log_...()`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">log_time</span><span class="p">((</span><span class="n">TIMERS</span><span class="p">,</span> <span class="n">TRAINING_ITERATION_TIMER</span><span class="p">)):</span>
            <span class="c1"># In case we are training (in a thread) parallel to evaluation,</span>
            <span class="c1"># we may have to re-enable eager mode here (gets disabled in the</span>
            <span class="c1"># thread).</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;framework&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;tf2&quot;</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">tf</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
                <span class="n">tf1</span><span class="o">.</span><span class="n">enable_eager_execution</span><span class="p">()</span>

            <span class="n">has_run_once</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="c1"># Create a step context ...</span>
            <span class="k">with</span> <span class="n">TrainIterCtx</span><span class="p">(</span><span class="n">algo</span><span class="o">=</span><span class="bp">self</span><span class="p">)</span> <span class="k">as</span> <span class="n">train_iter_ctx</span><span class="p">:</span>
                <span class="c1"># .. so we can query it whether we should stop the iteration loop (e.g.</span>
                <span class="c1"># when we have reached `min_time_s_per_iteration`).</span>
                <span class="k">while</span> <span class="ow">not</span> <span class="n">train_iter_ctx</span><span class="o">.</span><span class="n">should_stop</span><span class="p">(</span><span class="n">has_run_once</span><span class="p">):</span>
                    <span class="c1"># Before training step, try to bring failed workers back.</span>
                    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">log_time</span><span class="p">((</span><span class="n">TIMERS</span><span class="p">,</span> <span class="n">RESTORE_ENV_RUNNERS_TIMER</span><span class="p">)):</span>
                        <span class="n">restored</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">restore_env_runners</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="p">)</span>
                        <span class="c1"># Fire the callback for re-created EnvRunners.</span>
                        <span class="k">if</span> <span class="n">restored</span><span class="p">:</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">_make_on_env_runners_recreated_callbacks</span><span class="p">(</span>
                                <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
                                <span class="n">env_runner_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="p">,</span>
                                <span class="n">restored_env_runner_indices</span><span class="o">=</span><span class="n">restored</span><span class="p">,</span>
                            <span class="p">)</span>

                    <span class="c1"># Try to train one step.</span>
                    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">log_time</span><span class="p">((</span><span class="n">TIMERS</span><span class="p">,</span> <span class="n">TRAINING_STEP_TIMER</span><span class="p">)):</span>
                        <span class="n">training_step_return_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_step</span><span class="p">()</span>
                        <span class="n">has_run_once</span> <span class="o">=</span> <span class="kc">True</span>

                    <span class="c1"># On the new API stack, results should NOT be returned anymore as</span>
                    <span class="c1"># a dict, but purely logged through the `MetricsLogger` API. This</span>
                    <span class="c1"># way, we make sure to never miss a single stats/counter/timer</span>
                    <span class="c1"># when calling `self.training_step()` more than once within the same</span>
                    <span class="c1"># iteration.</span>
                    <span class="k">if</span> <span class="n">training_step_return_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="s2">&quot;`Algorithm.training_step()` should NOT return a result &quot;</span>
                            <span class="s2">&quot;dict anymore on the new API stack! Instead, log all &quot;</span>
                            <span class="s2">&quot;results, timers, counters through the `self.metrics` &quot;</span>
                            <span class="s2">&quot;(MetricsLogger) instance of the Algorithm and return &quot;</span>
                            <span class="s2">&quot;None. The logged results are compiled automatically into &quot;</span>
                            <span class="s2">&quot;one single result dict per training iteration.&quot;</span>
                        <span class="p">)</span>

                    <span class="c1"># TODO (sven): Resolve this metric through log_time&#39;s future</span>
                    <span class="c1">#  ability to compute throughput.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">log_value</span><span class="p">(</span>
                        <span class="n">NUM_TRAINING_STEP_CALLS_PER_ITERATION</span><span class="p">,</span>
                        <span class="mi">1</span><span class="p">,</span>
                        <span class="n">reduce</span><span class="o">=</span><span class="s2">&quot;sum&quot;</span><span class="p">,</span>
                        <span class="n">clear_on_reduce</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_aggregator_actors_per_learner</span><span class="p">:</span>
            <span class="n">remote_aggregator_metrics</span><span class="p">:</span> <span class="n">RemoteCallResults</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_aggregator_actor_manager</span><span class="o">.</span><span class="n">fetch_ready_async_reqs</span><span class="p">(</span>
                    <span class="n">timeout_seconds</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                    <span class="n">return_obj_refs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">tags</span><span class="o">=</span><span class="s2">&quot;metrics&quot;</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_aggregator_actor_manager</span><span class="o">.</span><span class="n">foreach_actor_async</span><span class="p">(</span>
                <span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">actor</span><span class="p">:</span> <span class="n">actor</span><span class="o">.</span><span class="n">get_metrics</span><span class="p">(),</span>
                <span class="n">tag</span><span class="o">=</span><span class="s2">&quot;metrics&quot;</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">FaultTolerantActorManager</span><span class="o">.</span><span class="n">handle_remote_call_result_errors</span><span class="p">(</span>
                <span class="n">remote_aggregator_metrics</span><span class="p">,</span>
                <span class="n">ignore_ray_errors</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">aggregate</span><span class="p">(</span>
                <span class="p">[</span><span class="n">res</span><span class="o">.</span><span class="n">get</span><span class="p">()</span> <span class="k">for</span> <span class="n">res</span> <span class="ow">in</span> <span class="n">remote_aggregator_metrics</span><span class="o">.</span><span class="n">result_or_errors</span><span class="p">],</span>
                <span class="n">key</span><span class="o">=</span><span class="n">AGGREGATOR_ACTOR_RESULTS</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Only here (at the end of the iteration), compile the results into a single result dict.</span>
        <span class="c1"># Calling compile here reduces the metrics into single values and adds throughputs to the results where applicable.</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">compile</span><span class="p">(),</span> <span class="n">train_iter_ctx</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_run_one_offline_evaluation</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Runs offline evaluation step via `self.offline_evaluate()` and handling runner</span>
<span class="sd">        failures.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The results dict from the offline evaluation call.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Restore crashed offline evaluation runners.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">offline_eval_runner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">log_time</span><span class="p">((</span><span class="n">TIMERS</span><span class="p">,</span> <span class="n">RESTORE_OFFLINE_EVAL_RUNNERS_TIMER</span><span class="p">)):</span>
                <span class="n">restored</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">restore_offline_eval_runners</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">offline_eval_runner_group</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">restored</span><span class="p">:</span>
                    <span class="c1"># Fire the callback for re-created workers.</span>
                    <span class="n">make_callback</span><span class="p">(</span>
                        <span class="s2">&quot;on_offline_eval_runners_recreated&quot;</span><span class="p">,</span>
                        <span class="n">callbacks_objects</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="p">,</span>
                        <span class="n">callbacks_functions</span><span class="o">=</span><span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">callbacks_on_offline_eval_runners_recreated</span>
                        <span class="p">),</span>
                        <span class="n">kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
                            <span class="n">algorithm</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
                            <span class="n">env_runner_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">offline_eval_runner_group</span><span class="p">,</span>
                            <span class="n">env_runner_indices</span><span class="o">=</span><span class="n">restored</span><span class="p">,</span>
                        <span class="p">),</span>
                    <span class="p">)</span>

        <span class="c1"># Run one offline evaluation and time it.</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">log_time</span><span class="p">((</span><span class="n">TIMERS</span><span class="p">,</span> <span class="n">OFFLINE_EVALUATION_ITERATION_TIMER</span><span class="p">)):</span>
            <span class="n">eval_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_offline</span><span class="p">()</span>

        <span class="c1"># After evaluation, do a round of health check on remote eval runners to see if</span>
        <span class="c1"># any of the failed runners are back.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">offline_eval_runner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Add number of healthy evaluation runners after this iteration.</span>
            <span class="n">eval_results</span><span class="p">[</span>
                <span class="s2">&quot;num_healthy_offline_eval_runners&quot;</span>
            <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">offline_eval_runner_group</span><span class="o">.</span><span class="n">num_healthy_remote_runners</span>
            <span class="n">eval_results</span><span class="p">[</span>
                <span class="s2">&quot;offline_runners_actor_manager_num_outstanding_async_reqs&quot;</span>
            <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">offline_eval_runner_group</span><span class="o">.</span><span class="n">num_in_flight_async_reqs</span>
            <span class="n">eval_results</span><span class="p">[</span>
                <span class="s2">&quot;num_remote_offline_eval_runners_restarts&quot;</span>
            <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">offline_eval_runner_group</span><span class="o">.</span><span class="n">num_remote_runner_restarts</span>

        <span class="k">return</span> <span class="p">{</span><span class="n">EVALUATION_RESULTS</span><span class="p">:</span> <span class="n">eval_results</span><span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_run_one_evaluation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">parallel_train_future</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">concurrent</span><span class="o">.</span><span class="n">futures</span><span class="o">.</span><span class="n">ThreadPoolExecutor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ResultDict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Runs evaluation step via `self.evaluate()` and handling worker failures.</span>

<span class="sd">        Args:</span>
<span class="sd">            parallel_train_future: In case, we are training and avaluating in parallel,</span>
<span class="sd">                this arg carries the currently running ThreadPoolExecutor object that</span>
<span class="sd">                runs the training iteration. Use `parallel_train_future.done()` to</span>
<span class="sd">                check, whether the parallel training job has completed and</span>
<span class="sd">                `parallel_train_future.result()` to get its return values.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The results dict from the evaluation call.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
                <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">log_time</span><span class="p">((</span><span class="n">TIMERS</span><span class="p">,</span> <span class="n">RESTORE_EVAL_ENV_RUNNERS_TIMER</span><span class="p">)):</span>
                    <span class="n">restored</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">restore_env_runners</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span><span class="p">[</span><span class="s2">&quot;restore_eval_workers&quot;</span><span class="p">]:</span>
                    <span class="n">restored</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">restore_env_runners</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="p">)</span>
            <span class="c1"># Fire the callback for re-created EnvRunners.</span>
            <span class="k">if</span> <span class="n">restored</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_make_on_env_runners_recreated_callbacks</span><span class="p">(</span>
                    <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="p">,</span>
                    <span class="n">env_runner_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="p">,</span>
                    <span class="n">restored_env_runner_indices</span><span class="o">=</span><span class="n">restored</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="c1"># Run `self.evaluate()` only once per training iteration.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">log_time</span><span class="p">((</span><span class="n">TIMERS</span><span class="p">,</span> <span class="n">EVALUATION_ITERATION_TIMER</span><span class="p">)):</span>
                <span class="n">eval_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
                    <span class="n">parallel_train_future</span><span class="o">=</span><span class="n">parallel_train_future</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span><span class="p">[</span><span class="n">EVALUATION_ITERATION_TIMER</span><span class="p">]:</span>
                <span class="n">eval_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
                    <span class="n">parallel_train_future</span><span class="o">=</span><span class="n">parallel_train_future</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span><span class="p">[</span><span class="n">EVALUATION_ITERATION_TIMER</span><span class="p">]</span><span class="o">.</span><span class="n">push_units_processed</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED_FOR_EVALUATION_THIS_ITER</span><span class="p">]</span>
            <span class="p">)</span>

        <span class="c1"># After evaluation, do a round of health check on remote eval workers to see if</span>
        <span class="c1"># any of the failed workers are back.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Add number of healthy evaluation workers after this iteration.</span>
            <span class="n">eval_results</span><span class="p">[</span>
                <span class="s2">&quot;num_healthy_workers&quot;</span>
            <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">num_healthy_remote_workers</span><span class="p">()</span>
            <span class="n">eval_results</span><span class="p">[</span>
                <span class="s2">&quot;actor_manager_num_outstanding_async_reqs&quot;</span>
            <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">num_in_flight_async_reqs</span><span class="p">()</span>
            <span class="n">eval_results</span><span class="p">[</span>
                <span class="s2">&quot;num_remote_worker_restarts&quot;</span>
            <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">num_remote_worker_restarts</span><span class="p">()</span>

        <span class="k">return</span> <span class="p">{</span><span class="n">EVALUATION_RESULTS</span><span class="p">:</span> <span class="n">eval_results</span><span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_run_one_training_iteration_and_evaluation_in_parallel</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">ResultDict</span><span class="p">,</span> <span class="n">ResultDict</span><span class="p">,</span> <span class="s2">&quot;TrainIterCtx&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Runs one training iteration and one evaluation step in parallel.</span>

<span class="sd">        First starts the training iteration (via `self._run_one_training_iteration()`)</span>
<span class="sd">        within a ThreadPoolExecutor, then runs the evaluation step in parallel.</span>
<span class="sd">        In auto-duration mode (config.evaluation_duration=auto), makes sure the</span>
<span class="sd">        evaluation step takes roughly the same time as the training iteration.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tuple containing the training results, the evaluation results, and</span>
<span class="sd">            the `TrainIterCtx` object returned by the training call.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">concurrent</span><span class="o">.</span><span class="n">futures</span><span class="o">.</span><span class="n">ThreadPoolExecutor</span><span class="p">()</span> <span class="k">as</span> <span class="n">executor</span><span class="p">:</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
                <span class="n">parallel_train_future</span> <span class="o">=</span> <span class="n">executor</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span>
                    <span class="k">lambda</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_one_training_iteration</span><span class="p">()</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">parallel_train_future</span> <span class="o">=</span> <span class="n">executor</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span>
                    <span class="k">lambda</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_one_training_iteration_old_api_stack</span><span class="p">()</span>
                <span class="p">)</span>

            <span class="c1"># Pass the train_future into `self._run_one_evaluation()` to allow it</span>
            <span class="c1"># to run exactly as long as the training iteration takes in case</span>
            <span class="c1"># evaluation_duration=auto.</span>
            <span class="n">evaluation_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_one_evaluation</span><span class="p">(</span>
                <span class="n">parallel_train_future</span><span class="o">=</span><span class="n">parallel_train_future</span>
            <span class="p">)</span>
            <span class="c1"># Collect the training results from the future.</span>
            <span class="n">train_results</span><span class="p">,</span> <span class="n">train_iter_ctx</span> <span class="o">=</span> <span class="n">parallel_train_future</span><span class="o">.</span><span class="n">result</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">train_results</span><span class="p">,</span> <span class="n">evaluation_results</span><span class="p">,</span> <span class="n">train_iter_ctx</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_run_offline_evaluation_old_api_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Runs offline evaluation via `OfflineEvaluator.estimate_on_dataset()` API.</span>

<span class="sd">        This method will be used when `evaluation_dataset` is provided.</span>
<span class="sd">        Note: This will only work if the policy is a single agent policy.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The results dict from the offline evaluation call.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span><span class="o">.</span><span class="n">policy_map</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>

        <span class="n">parallelism</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">evaluation_num_env_runners</span> <span class="ow">or</span> <span class="mi">1</span>
        <span class="n">offline_eval_results</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;off_policy_estimator&quot;</span><span class="p">:</span> <span class="p">{}}</span>
        <span class="k">for</span> <span class="n">evaluator_name</span><span class="p">,</span> <span class="n">offline_evaluator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_estimators</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">offline_eval_results</span><span class="p">[</span><span class="s2">&quot;off_policy_estimator&quot;</span><span class="p">][</span>
                <span class="n">evaluator_name</span>
            <span class="p">]</span> <span class="o">=</span> <span class="n">offline_evaluator</span><span class="o">.</span><span class="n">estimate_on_dataset</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_dataset</span><span class="p">,</span>
                <span class="n">n_parallelism</span><span class="o">=</span><span class="n">parallelism</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">offline_eval_results</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_should_create_evaluation_env_runners</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">eval_config</span><span class="p">:</span> <span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Determines whether we need to create evaluation workers.</span>

<span class="sd">        Returns False if we need to run offline evaluation</span>
<span class="sd">        (with ope.estimate_on_dastaset API) or when local worker is to be used for</span>
<span class="sd">        evaluation. Note: We only use estimate_on_dataset API with bandits for now.</span>
<span class="sd">        That is when ope_split_batch_by_episode is False.</span>
<span class="sd">        TODO: In future we will do the same for episodic RL OPE.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">run_offline_evaluation</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">eval_config</span><span class="o">.</span><span class="n">off_policy_estimation_methods</span>
            <span class="ow">and</span> <span class="ow">not</span> <span class="n">eval_config</span><span class="o">.</span><span class="n">ope_split_batch_by_episode</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="ow">not</span> <span class="n">run_offline_evaluation</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="n">eval_config</span><span class="o">.</span><span class="n">evaluation_num_env_runners</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="ow">or</span> <span class="n">eval_config</span><span class="o">.</span><span class="n">evaluation_interval</span>
        <span class="p">)</span>

    <span class="c1"># TODO (simon, sven): Flexibilize the different env/offline components and move</span>
    <span class="c1"># away from the currently hard-coded: (1) eval `EnvRunnerGroup`, (2) OfflineData</span>
    <span class="c1"># and (3) `OfflineEvaluationRunnerGroup`.</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_should_create_offline_evaluation_runners</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">eval_config</span><span class="p">:</span> <span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Determines whether we need to create offline evaluation workers.&quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="p">(</span>
            <span class="n">eval_config</span><span class="o">.</span><span class="n">offline_evaluation_interval</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">or</span> <span class="n">eval_config</span><span class="o">.</span><span class="n">num_offline_eval_runners</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_compile_iteration_results</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">train_results</span><span class="p">,</span> <span class="n">eval_results</span><span class="p">):</span>
        <span class="c1"># Error if users still use `self._timers`.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`Algorithm._timers` is no longer supported on the new API stack! &quot;</span>
                <span class="s2">&quot;Instead, use `Algorithm.metrics.log_time(&quot;</span>
                <span class="s2">&quot;[some key (str) or nested key sequence (tuple)])`, e.g. inside your &quot;</span>
                <span class="s2">&quot;custom `training_step()` method, do: &quot;</span>
                <span class="s2">&quot;`with self.metrics.log_time((&#39;timers&#39;, &#39;my_block_to_be_timed&#39;)): ...`&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Return dict (shallow copy of `train_results`).</span>
        <span class="n">results</span><span class="p">:</span> <span class="n">ResultDict</span> <span class="o">=</span> <span class="n">train_results</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="c1"># Backward compatibility `NUM_ENV_STEPS_SAMPLED_LIFETIME` is now:</span>
        <span class="c1"># `ENV_RUNNER_RESULTS/NUM_ENV_STEPS_SAMPLED_LIFETIME`.</span>
        <span class="n">results</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED_LIFETIME</span><span class="p">]</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="n">ENV_RUNNER_RESULTS</span><span class="p">,</span> <span class="p">{}</span>
        <span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">NUM_ENV_STEPS_SAMPLED_LIFETIME</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Evaluation results.</span>
        <span class="k">if</span> <span class="n">eval_results</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">eval_results</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
                <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">eval_results</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
                <span class="ow">and</span> <span class="n">EVALUATION_RESULTS</span> <span class="ow">in</span> <span class="n">eval_results</span>
            <span class="p">)</span>
            <span class="n">results</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">eval_results</span><span class="p">)</span>

        <span class="c1"># EnvRunner actors fault tolerance stats.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="p">:</span>
            <span class="n">results</span><span class="p">[</span><span class="n">FAULT_TOLERANCE_STATS</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;num_healthy_workers&quot;</span><span class="p">:</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">num_healthy_remote_workers</span><span class="p">()</span>
                <span class="p">),</span>
                <span class="s2">&quot;num_remote_worker_restarts&quot;</span><span class="p">:</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">num_remote_worker_restarts</span><span class="p">()</span>
                <span class="p">),</span>
            <span class="p">}</span>
            <span class="n">results</span><span class="p">[</span><span class="s2">&quot;env_runner_group&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;actor_manager_num_outstanding_async_reqs&quot;</span><span class="p">:</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">num_in_flight_async_reqs</span><span class="p">()</span>
                <span class="p">),</span>
            <span class="p">}</span>

        <span class="k">return</span> <span class="n">results</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_make_on_env_runners_recreated_callbacks</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">config</span><span class="p">,</span>
        <span class="n">env_runner_group</span><span class="p">,</span>
        <span class="n">restored_env_runner_indices</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">make_callback</span><span class="p">(</span>
            <span class="s2">&quot;on_env_runners_recreated&quot;</span><span class="p">,</span>
            <span class="n">callbacks_objects</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="p">,</span>
            <span class="n">callbacks_functions</span><span class="o">=</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">callbacks_on_env_runners_recreated</span><span class="p">),</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
                <span class="n">algorithm</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">env_runner_group</span><span class="o">=</span><span class="n">env_runner_group</span><span class="p">,</span>
                <span class="n">env_runner_indices</span><span class="o">=</span><span class="n">restored_env_runner_indices</span><span class="p">,</span>
                <span class="n">is_evaluation</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">in_evaluation</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="c1"># TODO (sven): Deprecate this call.</span>
        <span class="n">make_callback</span><span class="p">(</span>
            <span class="s2">&quot;on_workers_recreated&quot;</span><span class="p">,</span>
            <span class="n">callbacks_objects</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
                <span class="n">algorithm</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">worker_set</span><span class="o">=</span><span class="n">env_runner_group</span><span class="p">,</span>
                <span class="n">worker_ids</span><span class="o">=</span><span class="n">restored_env_runner_indices</span><span class="p">,</span>
                <span class="n">is_evaluation</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">in_evaluation</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_rl_module_and_learner</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;env=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">env</span><span class="si">}</span><span class="s2">; env-runners=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_env_runners</span><span class="si">}</span><span class="s2">; &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;learners=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_learners</span><span class="si">}</span><span class="s2">; &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;multi-agent=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_multi_agent</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;)&quot;</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">env_runner</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The local EnvRunner instance within the algo&#39;s EnvRunnerGroup.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">eval_env_runner</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The local EnvRunner instance within the algo&#39;s evaluation EnvRunnerGroup.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_record_usage</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Record the framework and algorithm used.</span>

<span class="sd">        Args:</span>
<span class="sd">            config: Algorithm config dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">record_extra_usage_tag</span><span class="p">(</span><span class="n">TagKey</span><span class="o">.</span><span class="n">RLLIB_FRAMEWORK</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;framework&quot;</span><span class="p">])</span>
        <span class="n">record_extra_usage_tag</span><span class="p">(</span><span class="n">TagKey</span><span class="o">.</span><span class="n">RLLIB_NUM_WORKERS</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_env_runners&quot;</span><span class="p">]))</span>
        <span class="n">alg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="c1"># We do not want to collect user defined algorithm names.</span>
        <span class="k">if</span> <span class="n">alg</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ALL_ALGORITHMS</span><span class="p">:</span>
            <span class="n">alg</span> <span class="o">=</span> <span class="s2">&quot;USER_DEFINED&quot;</span>
        <span class="n">record_extra_usage_tag</span><span class="p">(</span><span class="n">TagKey</span><span class="o">.</span><span class="n">RLLIB_ALGORITHM</span><span class="p">,</span> <span class="n">alg</span><span class="p">)</span>

    <span class="nd">@OldAPIStack</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_export_model</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">export_formats</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">export_dir</span><span class="p">:</span> <span class="nb">str</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
        <span class="n">ExportFormat</span><span class="o">.</span><span class="n">validate</span><span class="p">(</span><span class="n">export_formats</span><span class="p">)</span>
        <span class="n">exported</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">ExportFormat</span><span class="o">.</span><span class="n">CHECKPOINT</span> <span class="ow">in</span> <span class="n">export_formats</span><span class="p">:</span>
            <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">export_dir</span><span class="p">,</span> <span class="n">ExportFormat</span><span class="o">.</span><span class="n">CHECKPOINT</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">export_policy_checkpoint</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
            <span class="n">exported</span><span class="p">[</span><span class="n">ExportFormat</span><span class="o">.</span><span class="n">CHECKPOINT</span><span class="p">]</span> <span class="o">=</span> <span class="n">path</span>
        <span class="k">if</span> <span class="n">ExportFormat</span><span class="o">.</span><span class="n">MODEL</span> <span class="ow">in</span> <span class="n">export_formats</span><span class="p">:</span>
            <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">export_dir</span><span class="p">,</span> <span class="n">ExportFormat</span><span class="o">.</span><span class="n">MODEL</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">export_policy_model</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
            <span class="n">exported</span><span class="p">[</span><span class="n">ExportFormat</span><span class="o">.</span><span class="n">MODEL</span><span class="p">]</span> <span class="o">=</span> <span class="n">path</span>
        <span class="k">if</span> <span class="n">ExportFormat</span><span class="o">.</span><span class="n">ONNX</span> <span class="ow">in</span> <span class="n">export_formats</span><span class="p">:</span>
            <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">export_dir</span><span class="p">,</span> <span class="n">ExportFormat</span><span class="o">.</span><span class="n">ONNX</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">export_policy_model</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">onnx</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;ONNX_OPSET&quot;</span><span class="p">,</span> <span class="s2">&quot;11&quot;</span><span class="p">)))</span>
            <span class="n">exported</span><span class="p">[</span><span class="n">ExportFormat</span><span class="o">.</span><span class="n">ONNX</span><span class="p">]</span> <span class="o">=</span> <span class="n">path</span>
        <span class="k">return</span> <span class="n">exported</span>

    <span class="nd">@OldAPIStack</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns current state of Algorithm, sufficient to restore it from scratch.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The current state dict of this Algorithm, which can be used to sufficiently</span>
<span class="sd">            restore the algorithm from scratch without any other information.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Algorithm.__getstate__() not supported anymore on the new API stack! &quot;</span>
                <span class="s2">&quot;Use Algorithm.get_state() instead.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Add config to state so complete Algorithm can be reproduced w/o it.</span>
        <span class="n">state</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;algorithm_class&quot;</span><span class="p">:</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span>
            <span class="s2">&quot;config&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get_state</span><span class="p">(),</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;env_runner_group&quot;</span><span class="p">):</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;worker&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>

        <span class="c1"># Also store eval `policy_mapping_fn` (in case it&#39;s different from main</span>
        <span class="c1"># one). Note, the new `EnvRunner API` has no policy mapping function.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;eval_env_runner_group&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;eval_policy_mapping_fn&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner</span><span class="o">.</span><span class="n">policy_mapping_fn</span>

        <span class="c1"># Save counters.</span>
        <span class="n">state</span><span class="p">[</span><span class="s2">&quot;counters&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span>

        <span class="c1"># TODO: Experimental functionality: Store contents of replay buffer</span>
        <span class="c1">#  to checkpoint, only if user has configured this.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_replay_buffer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;store_buffer_in_checkpoints&quot;</span>
        <span class="p">):</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;local_replay_buffer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_replay_buffer</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>

        <span class="c1"># Save current `training_iteration`.</span>
        <span class="n">state</span><span class="p">[</span><span class="n">TRAINING_ITERATION</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_iteration</span>

        <span class="k">return</span> <span class="n">state</span>

    <span class="nd">@OldAPIStack</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sets the algorithm to the provided state.</span>

<span class="sd">        Args:</span>
<span class="sd">            state: The state dict to restore this Algorithm instance to. `state` may</span>
<span class="sd">                have been returned by a call to an Algorithm&#39;s `__getstate__()` method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Algorithm.__setstate__() not supported anymore on the new API stack! &quot;</span>
                <span class="s2">&quot;Use Algorithm.set_state() instead.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Old API stack: The local worker stores its state (together with all the</span>
        <span class="c1"># Module information) in state[&#39;worker&#39;].</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;env_runner_group&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="s2">&quot;worker&quot;</span> <span class="ow">in</span> <span class="n">state</span> <span class="ow">and</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;worker&quot;</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;worker&quot;</span><span class="p">])</span>
            <span class="n">remote_state_ref</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;worker&quot;</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">foreach_env_runner</span><span class="p">(</span>
                <span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">w</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">remote_state_ref</span><span class="p">)),</span>
                <span class="n">local_env_runner</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="p">:</span>
                <span class="c1"># Avoid `state` being pickled into the remote function below.</span>
                <span class="n">_eval_policy_mapping_fn</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;eval_policy_mapping_fn&quot;</span><span class="p">)</span>

                <span class="k">def</span><span class="w"> </span><span class="nf">_setup_eval_worker</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
                    <span class="n">w</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">remote_state_ref</span><span class="p">))</span>
                    <span class="c1"># Override `policy_mapping_fn` as it might be different for eval</span>
                    <span class="c1"># workers.</span>
                    <span class="n">w</span><span class="o">.</span><span class="n">set_policy_mapping_fn</span><span class="p">(</span><span class="n">_eval_policy_mapping_fn</span><span class="p">)</span>

                <span class="c1"># If evaluation workers are used, also restore the policies</span>
                <span class="c1"># there in case they are used for evaluation purpose.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">foreach_env_runner</span><span class="p">(</span><span class="n">_setup_eval_worker</span><span class="p">)</span>

        <span class="c1"># Restore replay buffer data.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_replay_buffer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># TODO: Experimental functionality: Restore contents of replay</span>
            <span class="c1">#  buffer from checkpoint, only if user has configured this.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">store_buffer_in_checkpoints</span><span class="p">:</span>
                <span class="k">if</span> <span class="s2">&quot;local_replay_buffer&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">local_replay_buffer</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;local_replay_buffer&quot;</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="s2">&quot;`store_buffer_in_checkpoints` is True, but no replay &quot;</span>
                        <span class="s2">&quot;data found in state!&quot;</span>
                    <span class="p">)</span>
            <span class="k">elif</span> <span class="s2">&quot;local_replay_buffer&quot;</span> <span class="ow">in</span> <span class="n">state</span> <span class="ow">and</span> <span class="n">log_once</span><span class="p">(</span>
                <span class="s2">&quot;no_store_buffer_in_checkpoints_but_data_found&quot;</span>
            <span class="p">):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;`store_buffer_in_checkpoints` is False, but some replay &quot;</span>
                    <span class="s2">&quot;data found in state!&quot;</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;counters&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;counters&quot;</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">TRAINING_ITERATION</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_iteration</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="n">TRAINING_ITERATION</span><span class="p">]</span>

    <span class="nd">@OldAPIStack</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_checkpoint_info_to_algorithm_state</span><span class="p">(</span>
        <span class="n">checkpoint_info</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">policy_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Collection</span><span class="p">[</span><span class="n">PolicyID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policy_mapping_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">AgentID</span><span class="p">,</span> <span class="n">EpisodeID</span><span class="p">],</span> <span class="n">PolicyID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policies_to_train</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">Collection</span><span class="p">[</span><span class="n">PolicyID</span><span class="p">],</span>
                <span class="n">Callable</span><span class="p">[[</span><span class="n">PolicyID</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SampleBatchType</span><span class="p">]],</span> <span class="nb">bool</span><span class="p">],</span>
            <span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Converts a checkpoint info or object to a proper Algorithm state dict.</span>

<span class="sd">        The returned state dict can be used inside self.__setstate__().</span>

<span class="sd">        Args:</span>
<span class="sd">            checkpoint_info: A checkpoint info dict as returned by</span>
<span class="sd">                `ray.rllib.utils.checkpoints.get_checkpoint_info(</span>
<span class="sd">                [checkpoint dir or AIR Checkpoint])`.</span>
<span class="sd">            policy_ids: Optional list/set of PolicyIDs. If not None, only those policies</span>
<span class="sd">                listed here will be included in the returned state. Note that</span>
<span class="sd">                state items such as filters, the `is_policy_to_train` function, as</span>
<span class="sd">                well as the multi-agent `policy_ids` dict will be adjusted as well,</span>
<span class="sd">                based on this arg.</span>
<span class="sd">            policy_mapping_fn: An optional (updated) policy mapping function</span>
<span class="sd">                to include in the returned state.</span>
<span class="sd">            policies_to_train: An optional list of policy IDs to be trained</span>
<span class="sd">                or a callable taking PolicyID and SampleBatchType and</span>
<span class="sd">                returning a bool (trainable or not?) to include in the returned state.</span>

<span class="sd">        Returns:</span>
<span class="sd">             The state dict usable within the `self.__setstate__()` method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;type&quot;</span><span class="p">]</span> <span class="o">!=</span> <span class="s2">&quot;Algorithm&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`checkpoint` arg passed to &quot;</span>
                <span class="s2">&quot;`Algorithm._checkpoint_info_to_algorithm_state()` must be an &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Algorithm checkpoint (but is </span><span class="si">{</span><span class="n">checkpoint_info</span><span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">)!&quot;</span>
            <span class="p">)</span>

        <span class="n">msgpack</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">checkpoint_info</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;format&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;msgpack&quot;</span><span class="p">:</span>
            <span class="n">msgpack</span> <span class="o">=</span> <span class="n">try_import_msgpack</span><span class="p">(</span><span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;state_file&quot;</span><span class="p">],</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">msgpack</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
                <span class="n">state</span> <span class="o">=</span> <span class="n">msgpack</span><span class="o">.</span><span class="n">unpackb</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">raw</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">state</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

        <span class="c1"># Old API stack: Policies are in separate sub-dirs.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;checkpoint_version&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">version</span><span class="o">.</span><span class="n">Version</span><span class="p">(</span><span class="s2">&quot;0.1&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;worker&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;worker&quot;</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="n">worker_state</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;worker&quot;</span><span class="p">]</span>

            <span class="c1"># Retrieve the set of all required policy IDs.</span>
            <span class="n">policy_ids</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
                <span class="n">policy_ids</span> <span class="k">if</span> <span class="n">policy_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">worker_state</span><span class="p">[</span><span class="s2">&quot;policy_ids&quot;</span><span class="p">]</span>
            <span class="p">)</span>

            <span class="c1"># Remove those policies entirely from filters that are not in</span>
            <span class="c1"># `policy_ids`.</span>
            <span class="n">worker_state</span><span class="p">[</span><span class="s2">&quot;filters&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">pid</span><span class="p">:</span> <span class="nb">filter</span>
                <span class="k">for</span> <span class="n">pid</span><span class="p">,</span> <span class="nb">filter</span> <span class="ow">in</span> <span class="n">worker_state</span><span class="p">[</span><span class="s2">&quot;filters&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">pid</span> <span class="ow">in</span> <span class="n">policy_ids</span>
            <span class="p">}</span>

            <span class="c1"># Get Algorithm class.</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;algorithm_class&quot;</span><span class="p">],</span> <span class="nb">str</span><span class="p">):</span>
                <span class="c1"># Try deserializing from a full classpath.</span>
                <span class="c1"># Or as a last resort: Tune registered algorithm name.</span>
                <span class="n">state</span><span class="p">[</span><span class="s2">&quot;algorithm_class&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">deserialize_type</span><span class="p">(</span>
                    <span class="n">state</span><span class="p">[</span><span class="s2">&quot;algorithm_class&quot;</span><span class="p">]</span>
                <span class="p">)</span> <span class="ow">or</span> <span class="n">get_trainable_cls</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;algorithm_class&quot;</span><span class="p">])</span>
            <span class="c1"># Compile actual config object.</span>
            <span class="n">default_config</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;algorithm_class&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get_default_config</span><span class="p">()</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">default_config</span><span class="p">,</span> <span class="n">AlgorithmConfig</span><span class="p">):</span>
                <span class="n">new_config</span> <span class="o">=</span> <span class="n">default_config</span><span class="o">.</span><span class="n">update_from_dict</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;config&quot;</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_config</span> <span class="o">=</span> <span class="n">Algorithm</span><span class="o">.</span><span class="n">merge_algorithm_configs</span><span class="p">(</span>
                    <span class="n">default_config</span><span class="p">,</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;config&quot;</span><span class="p">]</span>
                <span class="p">)</span>

            <span class="c1"># Remove policies from multiagent dict that are not in `policy_ids`.</span>
            <span class="n">new_policies</span> <span class="o">=</span> <span class="n">new_config</span><span class="o">.</span><span class="n">policies</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">new_policies</span><span class="p">,</span> <span class="p">(</span><span class="nb">set</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                <span class="n">new_policies</span> <span class="o">=</span> <span class="p">{</span><span class="n">pid</span> <span class="k">for</span> <span class="n">pid</span> <span class="ow">in</span> <span class="n">new_policies</span> <span class="k">if</span> <span class="n">pid</span> <span class="ow">in</span> <span class="n">policy_ids</span><span class="p">}</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_policies</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="n">pid</span><span class="p">:</span> <span class="n">spec</span> <span class="k">for</span> <span class="n">pid</span><span class="p">,</span> <span class="n">spec</span> <span class="ow">in</span> <span class="n">new_policies</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">pid</span> <span class="ow">in</span> <span class="n">policy_ids</span>
                <span class="p">}</span>
            <span class="n">new_config</span><span class="o">.</span><span class="n">multi_agent</span><span class="p">(</span>
                <span class="n">policies</span><span class="o">=</span><span class="n">new_policies</span><span class="p">,</span>
                <span class="n">policies_to_train</span><span class="o">=</span><span class="n">policies_to_train</span><span class="p">,</span>
                <span class="o">**</span><span class="p">(</span>
                    <span class="p">{</span><span class="s2">&quot;policy_mapping_fn&quot;</span><span class="p">:</span> <span class="n">policy_mapping_fn</span><span class="p">}</span>
                    <span class="k">if</span> <span class="n">policy_mapping_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="k">else</span> <span class="p">{}</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;config&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_config</span>

            <span class="c1"># Prepare local `worker` state to add policies&#39; states into it,</span>
            <span class="c1"># read from separate policy checkpoint files.</span>
            <span class="n">worker_state</span><span class="p">[</span><span class="s2">&quot;policy_states&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">pid</span> <span class="ow">in</span> <span class="n">policy_ids</span><span class="p">:</span>
                <span class="n">policy_state_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                    <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;checkpoint_dir&quot;</span><span class="p">],</span>
                    <span class="s2">&quot;policies&quot;</span><span class="p">,</span>
                    <span class="n">pid</span><span class="p">,</span>
                    <span class="s2">&quot;policy_state.&quot;</span>
                    <span class="o">+</span> <span class="p">(</span><span class="s2">&quot;msgpck&quot;</span> <span class="k">if</span> <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;format&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;msgpack&quot;</span> <span class="k">else</span> <span class="s2">&quot;pkl&quot;</span><span class="p">),</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">policy_state_file</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;Given checkpoint does not seem to be valid! No policy &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;state file found for PID=</span><span class="si">{</span><span class="n">pid</span><span class="si">}</span><span class="s2">. &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;The file not found is: </span><span class="si">{</span><span class="n">policy_state_file</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="p">)</span>

                <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">policy_state_file</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">msgpack</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">worker_state</span><span class="p">[</span><span class="s2">&quot;policy_states&quot;</span><span class="p">][</span><span class="n">pid</span><span class="p">]</span> <span class="o">=</span> <span class="n">msgpack</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">worker_state</span><span class="p">[</span><span class="s2">&quot;policy_states&quot;</span><span class="p">][</span><span class="n">pid</span><span class="p">]</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

            <span class="c1"># These two functions are never serialized in a msgpack checkpoint (which</span>
            <span class="c1"># does not store code, unlike a cloudpickle checkpoint). Hence the user has</span>
            <span class="c1"># to provide them with the `Algorithm.from_checkpoint()` call.</span>
            <span class="k">if</span> <span class="n">policy_mapping_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">worker_state</span><span class="p">[</span><span class="s2">&quot;policy_mapping_fn&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">policy_mapping_fn</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">policies_to_train</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="c1"># `policies_to_train` might be left None in case all policies should be</span>
                <span class="c1"># trained.</span>
                <span class="ow">or</span> <span class="n">worker_state</span><span class="p">[</span><span class="s2">&quot;is_policy_to_train&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">NOT_SERIALIZABLE</span>
            <span class="p">):</span>
                <span class="n">worker_state</span><span class="p">[</span><span class="s2">&quot;is_policy_to_train&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">policies_to_train</span>

        <span class="k">if</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;config&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">enable_rl_module_and_learner</span><span class="p">:</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;learner_state_dir&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;checkpoint_dir&quot;</span><span class="p">],</span> <span class="s2">&quot;learner&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">state</span>

    <span class="nd">@OldAPIStack</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_create_local_replay_buffer_if_necessary</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">PartialAlgorithmConfigDict</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MultiAgentReplayBuffer</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create a MultiAgentReplayBuffer instance if necessary.</span>

<span class="sd">        Args:</span>
<span class="sd">            config: Algorithm-specific configuration data.</span>

<span class="sd">        Returns:</span>
<span class="sd">            MultiAgentReplayBuffer instance based on algorithm config.</span>
<span class="sd">            None, if local replay buffer is not needed.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;replay_buffer_config&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;replay_buffer_config&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;no_local_replay_buffer&quot;</span>
        <span class="p">):</span>
            <span class="k">return</span>

        <span class="c1"># Add parameters, if necessary.</span>
        <span class="k">if</span> <span class="s2">&quot;EpisodeReplayBuffer&quot;</span> <span class="ow">in</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;replay_buffer_config&quot;</span><span class="p">][</span><span class="s2">&quot;type&quot;</span><span class="p">]:</span>
            <span class="c1"># TODO (simon): Subclassing needs a proper class and therefore</span>
            <span class="c1"># we need at this moment the string checking. Because we add</span>
            <span class="c1"># this keyword argument the old stack ReplayBuffer constructors</span>
            <span class="c1"># will exit with an error b/c tje keyword argument is unknown to them.</span>
            <span class="n">config</span><span class="p">[</span><span class="s2">&quot;replay_buffer_config&quot;</span><span class="p">][</span>
                <span class="s2">&quot;metrics_num_episodes_for_smoothing&quot;</span>
            <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">metrics_num_episodes_for_smoothing</span>

        <span class="k">return</span> <span class="n">from_config</span><span class="p">(</span><span class="n">ReplayBuffer</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;replay_buffer_config&quot;</span><span class="p">])</span>

    <span class="nd">@OldAPIStack</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_run_one_training_iteration_old_api_stack</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span><span class="p">[</span><span class="n">TRAINING_ITERATION_TIMER</span><span class="p">]:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;framework&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;tf2&quot;</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">tf</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
                <span class="n">tf1</span><span class="o">.</span><span class="n">enable_eager_execution</span><span class="p">()</span>

            <span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="n">training_step_results</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">with</span> <span class="n">TrainIterCtx</span><span class="p">(</span><span class="n">algo</span><span class="o">=</span><span class="bp">self</span><span class="p">)</span> <span class="k">as</span> <span class="n">train_iter_ctx</span><span class="p">:</span>
                <span class="k">while</span> <span class="ow">not</span> <span class="n">train_iter_ctx</span><span class="o">.</span><span class="n">should_stop</span><span class="p">(</span><span class="n">training_step_results</span><span class="p">):</span>
                    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span><span class="p">[</span><span class="s2">&quot;restore_workers&quot;</span><span class="p">]:</span>
                        <span class="n">restored</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">restore_env_runners</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="p">)</span>
                        <span class="c1"># Fire the callback for re-created EnvRunners.</span>
                        <span class="k">if</span> <span class="n">restored</span><span class="p">:</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">_make_on_env_runners_recreated_callbacks</span><span class="p">(</span>
                                <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
                                <span class="n">env_runner_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="p">,</span>
                                <span class="n">restored_env_runner_indices</span><span class="o">=</span><span class="n">restored</span><span class="p">,</span>
                            <span class="p">)</span>

                    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span><span class="p">[</span><span class="n">TRAINING_STEP_TIMER</span><span class="p">]:</span>
                        <span class="n">training_step_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_step</span><span class="p">()</span>

                    <span class="k">if</span> <span class="n">training_step_results</span><span class="p">:</span>
                        <span class="n">results</span> <span class="o">=</span> <span class="n">training_step_results</span>

        <span class="k">return</span> <span class="n">results</span><span class="p">,</span> <span class="n">train_iter_ctx</span>

    <span class="nd">@OldAPIStack</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_compile_iteration_results_old_api_stack</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">episodes_this_iter</span><span class="p">,</span> <span class="n">step_ctx</span><span class="p">,</span> <span class="n">iteration_results</span>
    <span class="p">):</span>
        <span class="c1"># Results to be returned.</span>
        <span class="n">results</span><span class="p">:</span> <span class="n">ResultDict</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Evaluation results.</span>
        <span class="k">if</span> <span class="s2">&quot;evaluation&quot;</span> <span class="ow">in</span> <span class="n">iteration_results</span><span class="p">:</span>
            <span class="n">eval_results</span> <span class="o">=</span> <span class="n">iteration_results</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;evaluation&quot;</span><span class="p">)</span>
            <span class="n">iteration_results</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">EVALUATION_RESULTS</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">results</span><span class="p">[</span><span class="s2">&quot;evaluation&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="n">EVALUATION_RESULTS</span><span class="p">]</span> <span class="o">=</span> <span class="n">eval_results</span>

        <span class="c1"># Custom metrics and episode media.</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;custom_metrics&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">iteration_results</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;custom_metrics&quot;</span><span class="p">,</span> <span class="p">{})</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;episode_media&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">iteration_results</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;episode_media&quot;</span><span class="p">,</span> <span class="p">{})</span>

        <span class="c1"># Learner info.</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;info&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="n">LEARNER_INFO</span><span class="p">:</span> <span class="n">iteration_results</span><span class="p">}</span>

        <span class="c1"># Calculate how many (if any) of older, historical episodes we have to add to</span>
        <span class="c1"># `episodes_this_iter` in order to reach the required smoothing window.</span>
        <span class="n">episodes_for_metrics</span> <span class="o">=</span> <span class="n">episodes_this_iter</span><span class="p">[:]</span>
        <span class="n">missing</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">metrics_num_episodes_for_smoothing</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span>
            <span class="n">episodes_this_iter</span>
        <span class="p">)</span>
        <span class="c1"># We have to add some older episodes to reach the smoothing window size.</span>
        <span class="k">if</span> <span class="n">missing</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">episodes_for_metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_episode_history</span><span class="p">[</span><span class="o">-</span><span class="n">missing</span><span class="p">:]</span> <span class="o">+</span> <span class="n">episodes_this_iter</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">episodes_for_metrics</span><span class="p">)</span>
                <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">metrics_num_episodes_for_smoothing</span>
            <span class="p">)</span>
        <span class="c1"># Note that when there are more than `metrics_num_episodes_for_smoothing`</span>
        <span class="c1"># episodes in `episodes_for_metrics`, leave them as-is. In this case, we&#39;ll</span>
        <span class="c1"># compute the stats over that larger number.</span>

        <span class="c1"># Add new episodes to our history and make sure it doesn&#39;t grow larger than</span>
        <span class="c1"># needed.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_episode_history</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">episodes_this_iter</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_episode_history</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_episode_history</span><span class="p">[</span>
            <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">metrics_num_episodes_for_smoothing</span> <span class="p">:</span>
        <span class="p">]</span>
        <span class="n">results</span><span class="p">[</span><span class="n">ENV_RUNNER_RESULTS</span><span class="p">]</span> <span class="o">=</span> <span class="n">summarize_episodes</span><span class="p">(</span>
            <span class="n">episodes_for_metrics</span><span class="p">,</span>
            <span class="n">episodes_this_iter</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">keep_per_episode_custom_metrics</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">results</span><span class="p">[</span>
            <span class="s2">&quot;num_healthy_workers&quot;</span>
        <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">num_healthy_remote_workers</span><span class="p">()</span>
        <span class="n">results</span><span class="p">[</span>
            <span class="s2">&quot;actor_manager_num_outstanding_async_reqs&quot;</span>
        <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">num_in_flight_async_reqs</span><span class="p">()</span>
        <span class="n">results</span><span class="p">[</span>
            <span class="s2">&quot;num_remote_worker_restarts&quot;</span>
        <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">num_remote_worker_restarts</span><span class="p">()</span>

        <span class="c1"># Train-steps- and env/agent-steps this iteration.</span>
        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="n">NUM_AGENT_STEPS_SAMPLED</span><span class="p">,</span>
            <span class="n">NUM_AGENT_STEPS_TRAINED</span><span class="p">,</span>
            <span class="n">NUM_ENV_STEPS_SAMPLED</span><span class="p">,</span>
            <span class="n">NUM_ENV_STEPS_TRAINED</span><span class="p">,</span>
        <span class="p">]:</span>
            <span class="n">results</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">c</span><span class="p">]</span>
        <span class="n">time_taken_sec</span> <span class="o">=</span> <span class="n">step_ctx</span><span class="o">.</span><span class="n">get_time_taken_sec</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">count_steps_by</span> <span class="o">==</span> <span class="s2">&quot;agent_steps&quot;</span><span class="p">:</span>
            <span class="n">results</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED</span> <span class="o">+</span> <span class="s2">&quot;_this_iter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">step_ctx</span><span class="o">.</span><span class="n">sampled</span>
            <span class="n">results</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_TRAINED</span> <span class="o">+</span> <span class="s2">&quot;_this_iter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">step_ctx</span><span class="o">.</span><span class="n">trained</span>
            <span class="n">results</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED</span> <span class="o">+</span> <span class="s2">&quot;_throughput_per_sec&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">step_ctx</span><span class="o">.</span><span class="n">sampled</span> <span class="o">/</span> <span class="n">time_taken_sec</span>
            <span class="p">)</span>
            <span class="n">results</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_TRAINED</span> <span class="o">+</span> <span class="s2">&quot;_throughput_per_sec&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">step_ctx</span><span class="o">.</span><span class="n">trained</span> <span class="o">/</span> <span class="n">time_taken_sec</span>
            <span class="p">)</span>
            <span class="c1"># TODO: For CQL and other algos, count by trained steps.</span>
            <span class="n">results</span><span class="p">[</span><span class="s2">&quot;timesteps_total&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">results</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED</span> <span class="o">+</span> <span class="s2">&quot;_this_iter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">step_ctx</span><span class="o">.</span><span class="n">sampled</span>
            <span class="n">results</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_TRAINED</span> <span class="o">+</span> <span class="s2">&quot;_this_iter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">step_ctx</span><span class="o">.</span><span class="n">trained</span>
            <span class="n">results</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED</span> <span class="o">+</span> <span class="s2">&quot;_throughput_per_sec&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">step_ctx</span><span class="o">.</span><span class="n">sampled</span> <span class="o">/</span> <span class="n">time_taken_sec</span>
            <span class="p">)</span>
            <span class="n">results</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_TRAINED</span> <span class="o">+</span> <span class="s2">&quot;_throughput_per_sec&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">step_ctx</span><span class="o">.</span><span class="n">trained</span> <span class="o">/</span> <span class="n">time_taken_sec</span>
            <span class="p">)</span>
            <span class="c1"># TODO: For CQL and other algos, count by trained steps.</span>
            <span class="n">results</span><span class="p">[</span><span class="s2">&quot;timesteps_total&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED</span><span class="p">]</span>

        <span class="c1"># Forward compatibility with new API stack.</span>
        <span class="n">results</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED_LIFETIME</span><span class="p">]</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s2">&quot;timesteps_total&quot;</span><span class="p">]</span>
        <span class="n">results</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED_LIFETIME</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span>
            <span class="n">NUM_AGENT_STEPS_SAMPLED</span>
        <span class="p">]</span>

        <span class="c1"># TODO: Backward compatibility.</span>
        <span class="n">results</span><span class="p">[</span><span class="n">STEPS_TRAINED_THIS_ITER_COUNTER</span><span class="p">]</span> <span class="o">=</span> <span class="n">step_ctx</span><span class="o">.</span><span class="n">trained</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;agent_timesteps_total&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED</span><span class="p">]</span>

        <span class="c1"># Process timer results.</span>
        <span class="n">timers</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">timer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">timers</span><span class="p">[</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">_time_ms&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">)]</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">timer</span><span class="o">.</span><span class="n">mean</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">timer</span><span class="o">.</span><span class="n">has_units_processed</span><span class="p">():</span>
                <span class="n">timers</span><span class="p">[</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">_throughput&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">)]</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">timer</span><span class="o">.</span><span class="n">mean_throughput</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;timers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">timers</span>

        <span class="c1"># Process counter results.</span>
        <span class="n">counters</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">counter</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">counters</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">counter</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;counters&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">counters</span>
        <span class="c1"># TODO: Backward compatibility.</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;info&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">counters</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">results</span>

    <span class="nd">@OldAPIStack</span>
    <span class="nd">@Deprecated</span><span class="p">(</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;`Algorithm.compute_single_action` should no longer be used. Get the &quot;</span>
        <span class="s2">&quot;RLModule instance through `Algorithm.get_module([module ID])`, then compute &quot;</span>
        <span class="s2">&quot;actions through `RLModule.forward_inference({&#39;obs&#39;: [obs batch]})`.&quot;</span><span class="p">,</span>
        <span class="n">error</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">compute_single_action</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">observation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">prev_action</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prev_reward</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">info</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">EnvInfoDict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">input_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SampleBatch</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policy_id</span><span class="p">:</span> <span class="n">PolicyID</span> <span class="o">=</span> <span class="n">DEFAULT_POLICY_ID</span><span class="p">,</span>
        <span class="n">full_fetch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">explore</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">timestep</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">episode</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">unsquash_action</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">clip_action</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span>
        <span class="n">TensorStructType</span><span class="p">,</span>
        <span class="n">Tuple</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TensorType</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]],</span>
    <span class="p">]:</span>
        <span class="k">if</span> <span class="n">unsquash_action</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">unsquash_action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">normalize_actions</span>
        <span class="k">elif</span> <span class="n">clip_action</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">clip_action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">clip_actions</span>

        <span class="n">err_msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;Provide either `input_dict` OR [`observation`, ...] as &quot;</span>
            <span class="s2">&quot;args to `Algorithm.compute_single_action()`!&quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">input_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">observation</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="n">prev_action</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="n">prev_reward</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="n">state</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="p">),</span> <span class="n">err_msg</span>
            <span class="n">observation</span> <span class="o">=</span> <span class="n">input_dict</span><span class="p">[</span><span class="n">Columns</span><span class="o">.</span><span class="n">OBS</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">observation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="n">err_msg</span>

        <span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_policy</span><span class="p">(</span><span class="n">policy_id</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">policy</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;PolicyID &#39;</span><span class="si">{</span><span class="n">policy_id</span><span class="si">}</span><span class="s2">&#39; not found in PolicyMap of the &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Algorithm&#39;s local worker!&quot;</span>
            <span class="p">)</span>
        <span class="n">pp</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">agent_connectors</span><span class="p">[</span><span class="n">ObsPreprocessorConnector</span><span class="p">]</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">observation</span><span class="p">,</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="nb">dict</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">observation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Observation type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span><span class="si">}</span><span class="s2"> cannot be converted to &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;np.ndarray.&quot;</span>
                <span class="p">)</span>
        <span class="k">if</span> <span class="n">pp</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pp</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;Only one preprocessor should be in the pipeline&quot;</span>
            <span class="n">pp</span> <span class="o">=</span> <span class="n">pp</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">pp</span><span class="o">.</span><span class="n">is_identity</span><span class="p">():</span>
                <span class="n">pp</span><span class="o">.</span><span class="n">in_eval</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">observation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">_input_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">Columns</span><span class="o">.</span><span class="n">OBS</span><span class="p">:</span> <span class="n">observation</span><span class="p">}</span>
                <span class="k">elif</span> <span class="n">input_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">_input_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">Columns</span><span class="o">.</span><span class="n">OBS</span><span class="p">:</span> <span class="n">input_dict</span><span class="p">[</span><span class="n">Columns</span><span class="o">.</span><span class="n">OBS</span><span class="p">]}</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;Either observation or input_dict must be provided.&quot;</span>
                    <span class="p">)</span>

                <span class="n">acd</span> <span class="o">=</span> <span class="n">AgentConnectorDataType</span><span class="p">(</span><span class="s2">&quot;0&quot;</span><span class="p">,</span> <span class="s2">&quot;0&quot;</span><span class="p">,</span> <span class="n">_input_dict</span><span class="p">)</span>
                <span class="n">pp</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">env_id</span><span class="o">=</span><span class="s2">&quot;0&quot;</span><span class="p">)</span>
                <span class="n">ac_o</span> <span class="o">=</span> <span class="n">pp</span><span class="p">([</span><span class="n">acd</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">observation</span> <span class="o">=</span> <span class="n">ac_o</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">Columns</span><span class="o">.</span><span class="n">OBS</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">input_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_dict</span><span class="p">[</span><span class="n">Columns</span><span class="o">.</span><span class="n">OBS</span><span class="p">]</span> <span class="o">=</span> <span class="n">observation</span>
            <span class="n">action</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">extra</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">compute_single_action</span><span class="p">(</span>
                <span class="n">input_dict</span><span class="o">=</span><span class="n">input_dict</span><span class="p">,</span>
                <span class="n">explore</span><span class="o">=</span><span class="n">explore</span><span class="p">,</span>
                <span class="n">timestep</span><span class="o">=</span><span class="n">timestep</span><span class="p">,</span>
                <span class="n">episode</span><span class="o">=</span><span class="n">episode</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">action</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">extra</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">compute_single_action</span><span class="p">(</span>
                <span class="n">obs</span><span class="o">=</span><span class="n">observation</span><span class="p">,</span>
                <span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">,</span>
                <span class="n">prev_action</span><span class="o">=</span><span class="n">prev_action</span><span class="p">,</span>
                <span class="n">prev_reward</span><span class="o">=</span><span class="n">prev_reward</span><span class="p">,</span>
                <span class="n">info</span><span class="o">=</span><span class="n">info</span><span class="p">,</span>
                <span class="n">explore</span><span class="o">=</span><span class="n">explore</span><span class="p">,</span>
                <span class="n">timestep</span><span class="o">=</span><span class="n">timestep</span><span class="p">,</span>
                <span class="n">episode</span><span class="o">=</span><span class="n">episode</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">unsquash_action</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">space_utils</span><span class="o">.</span><span class="n">unsquash_action</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">action_space_struct</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">clip_action</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">space_utils</span><span class="o">.</span><span class="n">clip_action</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">action_space_struct</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">state</span> <span class="ow">or</span> <span class="n">full_fetch</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">action</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">extra</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">action</span>

    <span class="nd">@OldAPIStack</span>
    <span class="nd">@Deprecated</span><span class="p">(</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;`Algorithm.compute_actions` should no longer be used. Get the RLModule &quot;</span>
        <span class="s2">&quot;instance through `Algorithm.get_module([module ID])`, then compute actions &quot;</span>
        <span class="s2">&quot;through `RLModule.forward_inference({&#39;obs&#39;: [obs batch]})`.&quot;</span><span class="p">,</span>
        <span class="n">error</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">compute_actions</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">observations</span><span class="p">:</span> <span class="n">TensorStructType</span><span class="p">,</span>
        <span class="n">state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">prev_action</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prev_reward</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">info</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">EnvInfoDict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policy_id</span><span class="p">:</span> <span class="n">PolicyID</span> <span class="o">=</span> <span class="n">DEFAULT_POLICY_ID</span><span class="p">,</span>
        <span class="n">full_fetch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">explore</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">timestep</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">episodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">unsquash_actions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">clip_actions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">unsquash_actions</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">unsquash_actions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">normalize_actions</span>
        <span class="k">elif</span> <span class="n">clip_actions</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">clip_actions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">clip_actions</span>

        <span class="n">state_defined</span> <span class="o">=</span> <span class="n">state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_policy</span><span class="p">(</span><span class="n">policy_id</span><span class="p">)</span>
        <span class="n">filtered_obs</span><span class="p">,</span> <span class="n">filtered_state</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">agent_id</span><span class="p">,</span> <span class="n">ob</span> <span class="ow">in</span> <span class="n">observations</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">worker</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span>
            <span class="k">if</span> <span class="n">worker</span><span class="o">.</span><span class="n">preprocessors</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">policy_id</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">preprocessed</span> <span class="o">=</span> <span class="n">worker</span><span class="o">.</span><span class="n">preprocessors</span><span class="p">[</span><span class="n">policy_id</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">ob</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">preprocessed</span> <span class="o">=</span> <span class="n">ob</span>
            <span class="n">filtered</span> <span class="o">=</span> <span class="n">worker</span><span class="o">.</span><span class="n">filters</span><span class="p">[</span><span class="n">policy_id</span><span class="p">](</span><span class="n">preprocessed</span><span class="p">,</span> <span class="n">update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">filtered_obs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">filtered</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">state</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">elif</span> <span class="n">agent_id</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
                <span class="n">filtered_state</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="n">agent_id</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">filtered_state</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">get_initial_state</span><span class="p">())</span>

        <span class="n">obs_batch</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">filtered_obs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">state</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">state</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">state</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">filtered_state</span><span class="p">))</span>
            <span class="n">state</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">state</span><span class="p">]</span>

        <span class="n">input_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">Columns</span><span class="o">.</span><span class="n">OBS</span><span class="p">:</span> <span class="n">obs_batch</span><span class="p">}</span>

        <span class="k">if</span> <span class="n">prev_action</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_dict</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">PREV_ACTIONS</span><span class="p">]</span> <span class="o">=</span> <span class="n">prev_action</span>
        <span class="k">if</span> <span class="n">prev_reward</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_dict</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">PREV_REWARDS</span><span class="p">]</span> <span class="o">=</span> <span class="n">prev_reward</span>
        <span class="k">if</span> <span class="n">info</span><span class="p">:</span>
            <span class="n">input_dict</span><span class="p">[</span><span class="n">Columns</span><span class="o">.</span><span class="n">INFOS</span><span class="p">]</span> <span class="o">=</span> <span class="n">info</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
            <span class="n">input_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;state_in_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span>

        <span class="n">actions</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">infos</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">compute_actions_from_input_dict</span><span class="p">(</span>
            <span class="n">input_dict</span><span class="o">=</span><span class="n">input_dict</span><span class="p">,</span>
            <span class="n">explore</span><span class="o">=</span><span class="n">explore</span><span class="p">,</span>
            <span class="n">timestep</span><span class="o">=</span><span class="n">timestep</span><span class="p">,</span>
            <span class="n">episodes</span><span class="o">=</span><span class="n">episodes</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">single_actions</span> <span class="o">=</span> <span class="n">space_utils</span><span class="o">.</span><span class="n">unbatch</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">observations</span><span class="p">,</span> <span class="n">single_actions</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">unsquash_actions</span><span class="p">:</span>
                <span class="n">a</span> <span class="o">=</span> <span class="n">space_utils</span><span class="o">.</span><span class="n">unsquash_action</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">action_space_struct</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">clip_actions</span><span class="p">:</span>
                <span class="n">a</span> <span class="o">=</span> <span class="n">space_utils</span><span class="o">.</span><span class="n">clip_action</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">action_space_struct</span><span class="p">)</span>
            <span class="n">actions</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span>

        <span class="n">unbatched_states</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">agent_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">observations</span><span class="p">):</span>
            <span class="n">unbatched_states</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">states</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">state_defined</span> <span class="ow">or</span> <span class="n">full_fetch</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">actions</span><span class="p">,</span> <span class="n">unbatched_states</span><span class="p">,</span> <span class="n">infos</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">actions</span>

    <span class="nd">@Deprecated</span><span class="p">(</span><span class="n">new</span><span class="o">=</span><span class="s2">&quot;Algorithm.restore_env_runners&quot;</span><span class="p">,</span> <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">restore_workers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="nd">@Deprecated</span><span class="p">(</span>
        <span class="n">new</span><span class="o">=</span><span class="s2">&quot;Algorithm.env_runner_group&quot;</span><span class="p">,</span>
        <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">workers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span>

    <span class="nd">@Deprecated</span><span class="p">(</span>
        <span class="n">new</span><span class="o">=</span><span class="s2">&quot;Algorithm.eval_env_runner_group&quot;</span><span class="p">,</span>
        <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">evaluation_workers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span></div>



<span class="k">class</span><span class="w"> </span><span class="nc">TrainIterCtx</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">algo</span><span class="p">:</span> <span class="n">Algorithm</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">algo</span> <span class="o">=</span> <span class="n">algo</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_start</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_stop</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Before first call to `step()`, `results` is expected to be None -&gt;</span>
        <span class="c1"># Start with self.failures=-1 -&gt; set to 0 before the very first call</span>
        <span class="c1"># to `self.step()`.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">failures</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">time_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sampled</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trained</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_env_steps_sampled</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span>
                <span class="p">(</span><span class="n">ENV_RUNNER_RESULTS</span><span class="p">,</span> <span class="n">NUM_ENV_STEPS_SAMPLED_LIFETIME</span><span class="p">),</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_env_steps_trained</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span>
                <span class="p">(</span><span class="n">LEARNER_RESULTS</span><span class="p">,</span> <span class="n">ALL_MODULES</span><span class="p">,</span> <span class="n">NUM_ENV_STEPS_TRAINED_LIFETIME</span><span class="p">),</span>
                <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_agent_steps_sampled</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span>
                    <span class="p">(</span><span class="n">ENV_RUNNER_RESULTS</span><span class="p">,</span> <span class="n">NUM_AGENT_STEPS_SAMPLED_LIFETIME</span><span class="p">),</span> <span class="n">default</span><span class="o">=</span><span class="p">{}</span>
                <span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_agent_steps_trained</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span>
                    <span class="p">(</span><span class="n">LEARNER_RESULTS</span><span class="p">,</span> <span class="n">NUM_AGENT_STEPS_TRAINED_LIFETIME</span><span class="p">),</span> <span class="n">default</span><span class="o">=</span><span class="p">{}</span>
                <span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_env_steps_sampled</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_env_steps_trained</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_TRAINED</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_agent_steps_sampled</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_agent_steps_trained</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_TRAINED</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">failure_tolerance</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_consecutive_env_runner_failures_tolerance</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_stop</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_time_taken_sec</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the time we spent in the context in seconds.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_stop</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_start</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">should_stop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">results</span><span class="p">):</span>
        <span class="c1"># Before first call to `step()`.</span>
        <span class="k">if</span> <span class="n">results</span> <span class="ow">in</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">False</span><span class="p">]:</span>
            <span class="c1"># Fail after n retries.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">failures</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">failures</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">failure_tolerance</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;More than `num_consecutive_env_runner_failures_tolerance=&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">failure_tolerance</span><span class="si">}</span><span class="s2">` consecutive worker failures! &quot;</span>
                    <span class="s2">&quot;Exiting.&quot;</span>
                <span class="p">)</span>
            <span class="c1"># Continue to very first `step()` call or retry `step()` after</span>
            <span class="c1"># a (tolerable) failure.</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="c1"># Stopping criteria.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">count_steps_by</span> <span class="o">==</span> <span class="s2">&quot;agent_steps&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">sampled</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="nb">sum</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span>
                            <span class="p">(</span><span class="n">ENV_RUNNER_RESULTS</span><span class="p">,</span> <span class="n">NUM_AGENT_STEPS_SAMPLED_LIFETIME</span><span class="p">),</span>
                            <span class="n">default</span><span class="o">=</span><span class="p">{},</span>
                        <span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
                    <span class="p">)</span>
                    <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_agent_steps_sampled</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">trained</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="nb">sum</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span>
                            <span class="p">(</span><span class="n">LEARNER_RESULTS</span><span class="p">,</span> <span class="n">NUM_AGENT_STEPS_TRAINED_LIFETIME</span><span class="p">),</span>
                            <span class="n">default</span><span class="o">=</span><span class="p">{},</span>
                        <span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
                    <span class="p">)</span>
                    <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_agent_steps_trained</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">sampled</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span>
                        <span class="p">(</span><span class="n">ENV_RUNNER_RESULTS</span><span class="p">,</span> <span class="n">NUM_ENV_STEPS_SAMPLED_LIFETIME</span><span class="p">),</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span>
                    <span class="p">)</span>
                    <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_env_steps_sampled</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">trained</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span>
                        <span class="p">(</span><span class="n">LEARNER_RESULTS</span><span class="p">,</span> <span class="n">ALL_MODULES</span><span class="p">,</span> <span class="n">NUM_ENV_STEPS_TRAINED_LIFETIME</span><span class="p">),</span>
                        <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_env_steps_trained</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">count_steps_by</span> <span class="o">==</span> <span class="s2">&quot;agent_steps&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">sampled</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED</span><span class="p">]</span>
                    <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_agent_steps_sampled</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">trained</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_TRAINED</span><span class="p">]</span>
                    <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_agent_steps_trained</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">sampled</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED</span><span class="p">]</span>
                    <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_env_steps_sampled</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">trained</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_TRAINED</span><span class="p">]</span>
                    <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_env_steps_trained</span>
                <span class="p">)</span>

        <span class="n">min_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">min_time_s_per_iteration</span>
        <span class="n">min_sample_ts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">min_sample_timesteps_per_iteration</span>
        <span class="n">min_train_ts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">min_train_timesteps_per_iteration</span>

        <span class="c1"># Repeat if not enough time has passed or if not enough</span>
        <span class="c1"># env|train timesteps have been processed (or these min</span>
        <span class="c1"># values are not provided by the user).</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="p">(</span><span class="ow">not</span> <span class="n">min_t</span> <span class="ow">or</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_start</span> <span class="o">&gt;=</span> <span class="n">min_t</span><span class="p">)</span>
            <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">min_sample_ts</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampled</span> <span class="o">&gt;=</span> <span class="n">min_sample_ts</span><span class="p">)</span>
            <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">min_train_ts</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">trained</span> <span class="o">&gt;=</span> <span class="n">min_train_ts</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
</pre></div>

                </article>
              
              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="footer-content-items footer-content__inner">
  
    <div class="footer-content-item"><div id="csat">
  <div id="csat-feedback-received" class="csat-hidden">
    <span>Thanks for the feedback!</span>
  </div>
  <div id="csat-inputs">
    <span>Was this helpful?</span>
    <div id="csat-yes" class="csat-button">
      <svg id="csat-yes-icon" class="csat-hidden csat-icon" width="18" height="13" viewBox="0 0 18 13" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 10.172L16.1922 0.979004L17.6072 2.393L7.00023 13L0.63623 6.636L2.05023 5.222L7.00023 10.172Z" />
      </svg>
      <span>Yes<span>
    </div>
    <div id="csat-no" class="csat-button">
      <svg id="csat-no-icon" class="csat-hidden csat-icon" width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 5.58599L11.9502 0.635986L13.3642 2.04999L8.41423 6.99999L13.3642 11.95L11.9502 13.364L7.00023 8.41399L2.05023 13.364L0.63623 11.95L5.58623 6.99999L0.63623 2.04999L2.05023 0.635986L7.00023 5.58599Z" />
      </svg>
      <span>No<span>
    </div>
  </div>
  <div id="csat-textarea-group" class="csat-hidden">
    <span id="csat-feedback-label">Feedback</span>
    <textarea id="csat-textarea"></textarea>
    <div id="csat-submit">Submit</div>
  </div>
</div></div>
  
</div>

          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794"></script>
<script src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
  <p class="copyright">
    
      Â© Copyright 2025, The Ray Team.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">
  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.3.7.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.14.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>