<!-- prettier-ignore -->

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>ray.rllib.env.multi_agent_episode &#8212; Ray 3.0.0.dev0</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../../../../_static/styles/bootstrap.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />

  
  <link href="../../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=ac02cc09edc035673794" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/termynal.css?v=2fc3cb5e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/csat.css?v=c8f39c76" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/assistant.css?v=d345c55e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/custom.css?v=633d7681" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/docsearch.css?v=45fb5152" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/pydata-docsearch-custom.css?v=7fb92720" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794" />
<link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794" />
  <script src="../../../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=ac02cc09edc035673794"></script>

    <script src="../../../../_static/documentation_options.js?v=d1493c90"></script>
    <script src="../../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../../_static/copybutton.js?v=9a29e97e"></script>
    <script defer="defer" src="../../../../_static/js/termynal.js?v=67cfcf08"></script>
    <script defer="defer" src="../../../../_static/js/custom.js?v=9e3b357f"></script>
    <script src="../../../../_static/js/csat.js?v=8e649b1c"></script>
    <script defer="defer" src="../../../../_static/js/assistant.js?v=73fdc522"></script>
    <script defer="defer" src="../../../../_static/docsearch.js?v=77274085"></script>
    <script defer="defer" src="../../../../_static/docsearch_config.js?v=d25523ed"></script>
    <script src="../../../../_static/design-tabs.js?v=36754332"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/ray/rllib/env/multi_agent_episode';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.14.1';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://antgroup.github.io/ant-ray/versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'main';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = false;
        </script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/_modules/ray/rllib/env/multi_agent_episode.html" />
    <link rel="icon" href="../../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" /><!-- Extra header to include at the top of each template.
Kept separately so that it can easily be included in any templates
that need to be overridden for individual pages; e.g. included
both in the usual template (layout.html) as well as (index.html). -->

<link rel="preconnect" href="https://fonts.googleapis.com" />
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
<link
  href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;900&family=Roboto:wght@400;500;700&display=swap"
  rel="stylesheet"
/>
<link
  rel="stylesheet"
  title="light"
  href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css"
  disabled="disabled"
/>
<link
  rel="stylesheet"
  title="dark"
  href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css"
  disabled="disabled"
/>
<link
  href="https://cdn.jsdelivr.net/npm/remixicon@4.1.0/fonts/remixicon.css"
  rel="stylesheet"
/>

<!-- Used for text embedded in html on the index page  -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- Parser used to call hljs on responses from Ray Assistant -->
<script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

<!-- Sanitizer for Ray Assistant AI -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/dompurify/2.3.3/purify.min.js"></script>

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<!-- Google Tag Manager -->
<script>
  (function (w, d, s, l, i) {
    w[l] = w[l] || [];
    w[l].push({
      'gtm.start': new Date().getTime(),
      event: 'gtm.js',
    });
    var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s),
      dl = l != 'dataLayer' ? '&l=' + l : '';
    j.async = true;
    j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
    f.parentNode.insertBefore(j, f);
  })(window, document, 'script', 'dataLayer', 'GTM-P8H6KQG');
</script>
<!-- End Google Tag Manager -->
<!-- Data to be shared with JS on every page -->
<script>
  window.data = {
      copyIconSrc: "../../../../_static/copy-button.svg"
  };
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  

<a class="navbar-brand logo" href="../../../../index.html">
  <svg width="400" height="201" viewBox="0 0 400 201" xmlns="http://www.w3.org/2000/svg">
<path id="ray-text" d="M325.949 134.356V109.785L302.442 66.6406H314.244L330.495 97.3062H331.946L348.198 66.6406H360L336.493 109.785V134.356H325.949ZM253.043 134.364L272.391 66.648H290.771L310.021 134.364H299.283L294.834 118.402H268.328L263.878 134.364H253.043ZM270.94 108.728H292.222L282.354 73.1294H280.807L270.94 108.728ZM198.887 134.364V66.648H227.327C231.519 66.648 235.195 67.3896 238.355 68.8729C241.58 70.2918 244.063 72.3555 245.804 75.0641C247.61 77.7727 248.513 80.9973 248.513 84.7378V85.8019C248.513 90.0583 247.481 93.4763 245.417 96.0559C243.418 98.5711 240.967 100.345 238.065 101.376V102.924C240.516 103.053 242.483 103.892 243.966 105.439C245.449 106.923 246.191 109.083 246.191 111.921V134.364H235.647V113.372C235.647 111.631 235.195 110.244 234.292 109.212C233.39 108.18 231.938 107.664 229.939 107.664H209.334V134.364H198.887ZM209.334 98.1842H226.166C229.907 98.1842 232.809 97.249 234.873 95.3788C236.937 93.4441 237.968 90.8322 237.968 87.5431V86.7692C237.968 83.4802 236.937 80.9005 234.873 79.0303C232.874 77.0956 229.971 76.1282 226.166 76.1282H209.334V98.1842Z" fill="black"/>
<path fill-rule="evenodd" clip-rule="evenodd" d="M143.63 101.311L98.3087 146.632L94.9903 143.313L140.311 97.9925L143.63 101.311ZM141.953 102.334L51.4454 102.334V97.6409L141.953 97.6409V102.334ZM94.992 55.9863L140.313 101.307L143.631 97.9886L98.3105 52.6679L94.992 55.9863Z" fill="#02A0CF"/>
<path d="M40 88.3163H62.6604V110.977H40V88.3163ZM85.3207 88.3163H107.981V110.977H85.3207V88.3163ZM85.3207 43H107.981V65.6604H85.3207V43ZM85.3207 133.645H107.981V156.306H85.3207V133.645ZM130.641 88.3163H153.301V110.977H130.641V88.3163Z" fill="#02A0CF"/>
</svg>

</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <div class="navbar-content docutils container">
<ul class="navbar-toplevel">
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/getting-started.html" title="Get Started"><span class="navbar-link-title">Get Started</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/use-cases.html" title="Use Cases"><span class="navbar-link-title">Use Cases</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/examples.html" title="Example Gallery"><span class="navbar-link-title">Example Gallery</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/installation.html" title="Library"><span class="navbar-link-title">Library</span></a></p>
<i class="fa-solid fa-chevron-down"></i></div>
<div class="navbar-dropdown docutils container">
<ul class="navbar-sublevel">
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-core/walkthrough.html" title="Ray Core"><span class="navbar-link-title">Ray Core</span>Scale general Python applications</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../data/data.html" title="Ray Data"><span class="navbar-link-title">Ray Data</span>Scale data ingest and preprocessing</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../train/train.html" title="Ray Train"><span class="navbar-link-title">Ray Train</span>Scale machine learning training</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../tune/index.html" title="Ray Tune"><span class="navbar-link-title">Ray Tune</span>Scale hyperparameter tuning</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../serve/index.html" title="Ray Serve"><span class="navbar-link-title">Ray Serve</span>Scale model serving</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../rllib/index.html" title="Ray RLlib"><span class="navbar-link-title">Ray RLlib</span>Scale reinforcement learning</a></p>
</div>
</li>
</ul>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../index.html" title="Docs"><span class="navbar-link-title">Docs</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://discuss.ray.io" title="Resources"><span class="navbar-link-title">Resources</span></a></p>
<i class="fa-solid fa-chevron-down"></i></div>
<div class="navbar-dropdown docutils container">
<ul class="navbar-sublevel">
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://discuss.ray.io" title="Discussion Forum"><span class="navbar-link-title">Discussion Forum</span>Get your Ray questions answered</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://github.com/ray-project/ray-educational-materials" title="Training"><span class="navbar-link-title">Training</span>Hands-on learning</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/blog" title="Blog"><span class="navbar-link-title">Blog</span>Updates, best practices, user-stories</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/events" title="Events"><span class="navbar-link-title">Events</span>Webinars, meetups, office hours</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/blog/how-ray-and-anyscale-make-it-easy-to-do-massive-scale-machine-learning-on" title="Success Stories"><span class="navbar-link-title">Success Stories</span>Real-world workload examples</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/ray-libraries.html" title="Ecosystem"><span class="navbar-link-title">Ecosystem</span>Libraries integrated with Ray</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.ray.io/community" title="Community"><span class="navbar-link-title">Community</span>Connect with us</a></p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>

</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          <div id="docsearch"></div>
        </div>
      
      
        <div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
      
        <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="versionswitcherbutton" type="button" role="button" class="version-switcher__button btn btn-sm navbar-btn dropdown-toggle" data-bs-toggle="dropdown" aria-haspopup="listbox" aria-controls="versionswitcherlist" aria-label="Version switcher list">
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="versionswitcherlist" class="version-switcher__menu dropdown-menu list-group-flush py-0" role="listbox" aria-labelledby="versionswitcherbutton">
    <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
      
        <div class="navbar-item"><a
  id="try-anyscale-href"
  href="https://www.anyscale.com/ray-on-anyscale/?utm_source=ray_docs&utm_medium=docs&utm_campaign=navbar"
>
  <div id="try-anyscale-text">
    <span>Try Ray on Anyscale</span>
  </div>
</a></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile"><div id="docsearch"></div>
    </div>
  

  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <div class="navbar-content docutils container">
<ul class="navbar-toplevel">
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/getting-started.html" title="Get Started"><span class="navbar-link-title">Get Started</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/use-cases.html" title="Use Cases"><span class="navbar-link-title">Use Cases</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/examples.html" title="Example Gallery"><span class="navbar-link-title">Example Gallery</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/installation.html" title="Library"><span class="navbar-link-title">Library</span></a></p>
<i class="fa-solid fa-chevron-down"></i></div>
<div class="navbar-dropdown docutils container">
<ul class="navbar-sublevel">
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-core/walkthrough.html" title="Ray Core"><span class="navbar-link-title">Ray Core</span>Scale general Python applications</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../data/data.html" title="Ray Data"><span class="navbar-link-title">Ray Data</span>Scale data ingest and preprocessing</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../train/train.html" title="Ray Train"><span class="navbar-link-title">Ray Train</span>Scale machine learning training</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../tune/index.html" title="Ray Tune"><span class="navbar-link-title">Ray Tune</span>Scale hyperparameter tuning</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../serve/index.html" title="Ray Serve"><span class="navbar-link-title">Ray Serve</span>Scale model serving</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../rllib/index.html" title="Ray RLlib"><span class="navbar-link-title">Ray RLlib</span>Scale reinforcement learning</a></p>
</div>
</li>
</ul>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../index.html" title="Docs"><span class="navbar-link-title">Docs</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://discuss.ray.io" title="Resources"><span class="navbar-link-title">Resources</span></a></p>
<i class="fa-solid fa-chevron-down"></i></div>
<div class="navbar-dropdown docutils container">
<ul class="navbar-sublevel">
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://discuss.ray.io" title="Discussion Forum"><span class="navbar-link-title">Discussion Forum</span>Get your Ray questions answered</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://github.com/ray-project/ray-educational-materials" title="Training"><span class="navbar-link-title">Training</span>Hands-on learning</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/blog" title="Blog"><span class="navbar-link-title">Blog</span>Updates, best practices, user-stories</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/events" title="Events"><span class="navbar-link-title">Events</span>Webinars, meetups, office hours</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/blog/how-ray-and-anyscale-make-it-easy-to-do-massive-scale-machine-learning-on" title="Success Stories"><span class="navbar-link-title">Success Stories</span>Real-world workload examples</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/ray-libraries.html" title="Ecosystem"><span class="navbar-link-title">Ecosystem</span>Libraries integrated with Ray</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.ray.io/community" title="Community"><span class="navbar-link-title">Community</span>Connect with us</a></p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>

</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
        
          <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="versionswitcherbutton" type="button" role="button" class="version-switcher__button btn btn-sm navbar-btn dropdown-toggle" data-bs-toggle="dropdown" aria-haspopup="listbox" aria-controls="versionswitcherlist" aria-label="Version switcher list">
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="versionswitcherlist" class="version-switcher__menu dropdown-menu list-group-flush py-0" role="listbox" aria-labelledby="versionswitcherbutton">
    <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
        
          <div class="navbar-item"><a
  id="try-anyscale-href"
  href="https://www.anyscale.com/ray-on-anyscale/?utm_source=ray_docs&utm_medium=docs&utm_campaign=navbar"
>
  <div id="try-anyscale-text">
    <span>Try Ray on Anyscale</span>
  </div>
</a></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item"><nav id="main-sidebar" class="bd-docs-nav bd-links" aria-label="Section Navigation">
  <div class="bd-toc-item navbar-nav"><ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../ray-overview/index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ray-overview/getting-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ray-overview/installation.html">Installation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../ray-overview/use-cases.html">Use Cases</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-air/getting-started.html">Ray for ML Infrastructure</a></li>

</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ray-overview/examples.html">Example Gallery</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ray-overview/ray-libraries.html">Ecosystem</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../ray-core/walkthrough.html">Ray Core</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-core/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-core/user-guide.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/tasks.html">Tasks</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/tasks/nested-tasks.html">Nested Remote Functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/tasks/generators.html">Dynamic generators</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/actors.html">Actors</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/named-actors.html">Named Actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/terminating-actors.html">Terminating Actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/async_api.html">AsyncIO / Concurrency for Actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/concurrency_group_api.html">Limiting Concurrency Per-Method with Concurrency Groups</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/actor-utils.html">Utility Classes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/out-of-band-communication.html">Out-of-band Communication</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/task-orders.html">Actor Task Execution Order</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/objects.html">Objects</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/objects/serialization.html">Serialization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/objects/object-spilling.html">Object Spilling</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/handling-dependencies.html">Environment Dependencies</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/scheduling/index.html">Scheduling</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/scheduling/resources.html">Resources</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/scheduling/accelerators.html">Accelerator Support</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/scheduling/placement-group.html">Placement Groups</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/scheduling/memory-management.html">Memory Management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/scheduling/ray-oom-prevention.html">Out-Of-Memory Prevention</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/fault-tolerance.html">Fault Tolerance</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/fault_tolerance/tasks.html">Task Fault Tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/fault_tolerance/actors.html">Actor Fault Tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/fault_tolerance/objects.html">Object Fault Tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/fault_tolerance/nodes.html">Node Fault Tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/fault_tolerance/gcs.html">GCS Fault Tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/fault_tolerance/head-ha.html">Head High-Availability Feature</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/patterns/index.html">Design Patterns &amp; Anti-patterns</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/nested-tasks.html">Pattern: Using nested tasks to achieve nested parallelism</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/generators.html">Pattern: Using generators to reduce heap memory usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/limit-pending-tasks.html">Pattern: Using ray.wait to limit the number of pending tasks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/limit-running-tasks.html">Pattern: Using resources to limit the number of concurrently running tasks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/concurrent-operations-async-actor.html">Pattern: Using asyncio to run actor methods concurrently</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/actor-sync.html">Pattern: Using an actor to synchronize other tasks and actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/tree-of-actors.html">Pattern: Using a supervisor actor to manage a tree of actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/pipelining.html">Pattern: Using pipelining to increase throughput</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/return-ray-put.html">Anti-pattern: Returning ray.put() ObjectRefs from a task harms performance and fault tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/ray-get-loop.html">Anti-pattern: Calling ray.get in a loop harms parallelism</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/unnecessary-ray-get.html">Anti-pattern: Calling ray.get unnecessarily harms performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/ray-get-submission-order.html">Anti-pattern: Processing results in submission order using ray.get increases runtime</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/ray-get-too-many-objects.html">Anti-pattern: Fetching too many objects at once with ray.get causes failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/too-fine-grained-tasks.html">Anti-pattern: Over-parallelizing with too fine-grained tasks harms speedup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/redefine-task-actor-loop.html">Anti-pattern: Redefining the same remote function or class harms performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/pass-large-arg-by-value.html">Anti-pattern: Passing the same large argument by value repeatedly harms performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/closure-capture-large-objects.html">Anti-pattern: Closure capturing large objects harms performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/global-variables.html">Anti-pattern: Using global variables to share state between tasks and actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/out-of-band-object-ref-serialization.html">Anti-pattern: Serialize ray.ObjectRef out of band</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/fork-new-processes.html">Anti-pattern: Forking new processes in application code</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/compiled-graph/ray-compiled-graph.html">Ray Compiled Graph (beta)</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/compiled-graph/quickstart.html">Quickstart</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/compiled-graph/profiling.html">Profiling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/compiled-graph/overlap.html">Experimental: Overlapping communication and computation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/compiled-graph/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/compiled-graph/compiled-graph-api.html">Compiled Graph API</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/advanced-topics.html">Advanced Topics</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/tips-for-first-time.html">Tips for first-time users</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/starting-ray.html">Starting Ray</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/ray-generator.html">Ray Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/namespaces.html">Using Namespaces</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/cross-language.html">Cross-Language Programming</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/using-ray-with-jupyter.html">Working with Jupyter Notebooks &amp; JupyterLab</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/ray-dag.html">Lazy Computation Graphs with the Ray DAG API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/miscellaneous.html">Miscellaneous Topics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/runtime_env_auth.html">Authenticating Remote URIs in runtime_env</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/user-spawn-processes.html">Lifetimes of a User-Spawn Process</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-core/examples/overview.html">Examples</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/automl_for_time_series.html">Simple AutoML for time series with Ray Core</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/batch_prediction.html">Batch Prediction with Ray Core</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/gentle_walkthrough.html">A Gentle Introduction to Ray Core by Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/highly_parallel.html">Using Ray for Highly Parallelizable Tasks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/map_reduce.html">A Simple MapReduce Example with Ray Core</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/monte_carlo_pi.html">Monte Carlo Estimation of Ï€</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/plot_hyperparameter.html">Simple Parallel Model Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/plot_parameter_server.html">Parameter Server</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/plot_pong_example.html">Learning to Play Pong</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/web-crawler.html">Speed up your web crawler by parallelizing it with Ray</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-core/api/index.html">Ray Core API</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/api/core.html">Core API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/api/scheduling.html">Scheduling API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/api/runtime-env.html">Runtime Env API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/api/utility.html">Utility</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/api/exceptions.html">Exceptions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/api/cli.html">Ray Core CLI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/reference/cli.html">State CLI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/reference/api.html">State API</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../data/data.html">Ray Data</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../data/quickstart.html">Ray Data Quickstart</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../data/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../data/user-guide.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/loading-data.html">Loading Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/inspecting-data.html">Inspecting Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/transforming-data.html">Transforming Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/iterating-over-data.html">Iterating over Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/shuffling-data.html">Shuffling Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/saving-data.html">Saving Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/working-with-images.html">Working with Images</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/working-with-text.html">Working with Text</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/working-with-tensors.html">Working with Tensors / NumPy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/working-with-pytorch.html">Working with PyTorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/working-with-llms.html">Working with LLMs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/monitoring-your-workload.html">Monitoring Your Workload</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/execution-configurations.html">Execution Configurations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/batch_inference.html">End-to-end: Offline Batch Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/performance-tips.html">Advanced: Performance Tips and Tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/custom-datasource-example.html">Advanced: Read and Write Custom File Types</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../data/examples.html">Examples</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../data/api/api.html">Ray Data API</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/input_output.html">Input/Output</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/dataset.html">Dataset API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/data_iterator.html">DataIterator API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/execution_options.html">ExecutionOptions API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/aggregate.html">Aggregation API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/grouped_data.html">GroupedData API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/data_context.html">Global configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/preprocessor.html">Preprocessor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/llm.html">Large Language Model (LLM) API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/from_other_data_libs.html">API Guide for Users from Other Data Libraries</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../data/comparisons.html">Comparing Ray Data to other systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../data/data-internals.html">Ray Data Internals</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../train/train.html">Ray Train</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/overview.html">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/getting-started-pytorch.html">PyTorch Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/getting-started-pytorch-lightning.html">PyTorch Lightning Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/getting-started-transformers.html">Hugging Face Transformers Guide</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../train/more-frameworks.html">More Frameworks</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/huggingface-accelerate.html">Hugging Face Accelerate Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/deepspeed.html">DeepSpeed Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/distributed-tensorflow-keras.html">TensorFlow and Keras Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/examples/xgboost/distributed-xgboost-lightgbm.html">XGBoost and LightGBM Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/horovod.html">Horovod Guide</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../train/user-guides.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/data-loading-preprocessing.html">Data Loading and Preprocessing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/using-gpus.html">Configuring Scale and GPUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/persistent-storage.html">Configuring Persistent Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/monitoring-logging.html">Monitoring and Logging Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/checkpoints.html">Saving and Loading Checkpoints</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/experiment-tracking.html">Experiment Tracking</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/results.html">Inspecting Training Results</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/fault-tolerance.html">Handling Failures and Node Preemption</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/reproducibility.html">Reproducibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/hyperparameter-optimization.html">Hyperparameter Optimization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/examples.html">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/benchmarks.html">Benchmarks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/api/api.html">Ray Train API</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../tune/index.html">Ray Tune</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-20"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../tune/getting-started.html">Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../tune/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../tune/tutorials/overview.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-21"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-run.html">Running Basic Experiments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-output.html">Logging and Outputs in Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-resources.html">Setting Trial Resources</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-search-spaces.html">Using Search Spaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-stopping.html">How to Define Stopping Criteria for a Ray Tune Experiment</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-trial-checkpoints.html">How to Save and Load Trial Checkpoints</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-storage.html">How to Configure Persistent Storage in Ray Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-fault-tolerance.html">How to Enable Fault Tolerance in Ray Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-metrics.html">Using Callbacks and Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune_get_data_in_and_out.html">Getting Data in and out of Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/tune_analyze_results.html">Analyzing Tune Experiment Results</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../tune/examples/pbt_guide.html">A Guide to Population Based Training with Tune</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-22"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/pbt_visualization/pbt_visualization.html">Visualizing and Understanding PBT</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-distributed.html">Deploying Tune in the Cloud</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-lifecycle.html">Tune Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-scalability.html">Scalability Benchmarks</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../tune/examples/index.html">Ray Tune Examples</a><input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-23"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../tune/examples/ml-frameworks.html">Examples using Ray Tune with ML Frameworks</a><input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-24"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/tune-pytorch-cifar.html">PyTorch Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/tune-pytorch-lightning.html">PyTorch Lightning Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/tune-xgboost.html">XGBoost Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/lightgbm_example.html">LightGBM Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/pbt_transformers.html">Hugging Face Transformers Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/pbt_ppo_example.html">Ray RLlib Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/tune_mnist_keras.html">Keras Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/horovod_simple.html">Horovod Example</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../tune/examples/experiment-tracking.html">Tune Experiment Tracking Examples</a><input class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-25"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/tune-wandb.html">Weights &amp; Biases Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/tune-mlflow.html">MLflow Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/tune-aim.html">Aim Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/tune-comet.html">Comet Example</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../tune/examples/hpo-frameworks.html">Tune Hyperparameter Optimization Framework Examples</a><input class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-26"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/ax_example.html">Ax Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/hyperopt_example.html">HyperOpt Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/bayesopt_example.html">Bayesopt Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/bohb_example.html">BOHB Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/nevergrad_example.html">Nevergrad Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/optuna_example.html">Optuna Example</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/other-examples.html">Other Examples</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/exercises.html">Exercises</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../tune/faq.html">Ray Tune FAQ</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../tune/api/api.html">Ray Tune API</a><input class="toctree-checkbox" id="toctree-checkbox-27" name="toctree-checkbox-27" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-27"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/execution.html">Tune Execution (tune.Tuner)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/result_grid.html">Tune Experiment Results (tune.ResultGrid)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/trainable.html">Training in Tune (tune.Trainable, tune.report)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/search_space.html">Tune Search Space API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/suggestion.html">Tune Search Algorithms (tune.search)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/schedulers.html">Tune Trial Schedulers (tune.schedulers)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/stoppers.html">Tune Stopping Mechanisms (tune.stopper)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/reporters.html">Tune Console Output (Reporters)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/syncing.html">Syncing in Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/logging.html">Tune Loggers (tune.logger)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/callbacks.html">Tune Callbacks (tune.Callback)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/env.html">Environment variables used by Ray Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/integration.html">External library integrations for Ray Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/internals.html">Tune Internals</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/cli.html">Tune CLI (Experimental)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../serve/index.html">Ray Serve</a><input class="toctree-checkbox" id="toctree-checkbox-28" name="toctree-checkbox-28" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-28"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/getting_started.html">Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/develop-and-deploy.html">Develop and Deploy an ML Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/model_composition.html">Deploy Compositions of Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/multi-app.html">Deploy Multiple Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/model-multiplexing.html">Model Multiplexing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/configure-serve-deployment.html">Configure Ray Serve deployments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/http-guide.html">Set Up FastAPI and HTTP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/llm/serving-llms.html">Serving LLMs</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../serve/production-guide/index.html">Production Guide</a><input class="toctree-checkbox" id="toctree-checkbox-29" name="toctree-checkbox-29" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-29"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/production-guide/config.html">Serve Config Files</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/production-guide/kubernetes.html">Deploy on Kubernetes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/production-guide/docker.html">Custom Docker Images</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/production-guide/fault-tolerance.html">Add End-to-End Fault Tolerance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/production-guide/handling-dependencies.html">Handle Dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/production-guide/best-practices.html">Best practices in production</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/monitoring.html">Monitor Your Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/resource-allocation.html">Resource Allocation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/autoscaling-guide.html">Ray Serve Autoscaling</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../serve/advanced-guides/index.html">Advanced Guides</a><input class="toctree-checkbox" id="toctree-checkbox-30" name="toctree-checkbox-30" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-30"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/app-builder-guide.html">Pass Arguments to Applications</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/advanced-autoscaling.html">Advanced Ray Serve Autoscaling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/performance.html">Performance Tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/dyn-req-batch.html">Dynamic Request Batching</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/inplace-updates.html">Updating Applications In-Place</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/dev-workflow.html">Development Workflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/grpc-guide.html">Set Up a gRPC Service</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/managing-java-deployments.html">Experimental Java API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/deploy-vm.html">Deploy on VM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/multi-app-container.html">Run Multiple Applications in Different Containers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/architecture.html">Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/examples.html">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/api/index.html">Ray Serve API</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../rllib/index.html">Ray RLlib</a><input class="toctree-checkbox" id="toctree-checkbox-31" name="toctree-checkbox-31" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-31"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../rllib/getting-started.html">Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../rllib/key-concepts.html">Key concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../rllib/rllib-env.html">Environments</a><input class="toctree-checkbox" id="toctree-checkbox-32" name="toctree-checkbox-32" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-32"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/multi-agent-envs.html">Multi-Agent Environments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/hierarchical-envs.html">Hierarchical Environments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/external-envs.html">External Environments and Applications</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../rllib/algorithm-config.html">AlgorithmConfig API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../rllib/rllib-algorithms.html">Algorithms</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../rllib/user-guides.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-33" name="toctree-checkbox-33" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-33"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-advanced-api.html">Advanced Python APIs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-callback.html">Callbacks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/checkpoints.html">Checkpointing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/metrics-logger.html">MetricsLogger API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/single-agent-episode.html">Episodes</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-replay-buffers.html">Replay Buffers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-offline.html">Working with offline data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rl-modules.html">RL Modules</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-learner.html">Learner (Alpha)</a></li>



<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-torch2x.html">Using RLlib with torch 2.x compile</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-fault-tolerance.html">Fault Tolerance And Elastic Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-dev.html">Install RLlib for Development</a></li>




<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/scaling-guide.html">RLlib scaling guide</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../rllib/rllib-examples.html">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../rllib/new-api-stack-migration-guide.html">New API stack migration guide</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../rllib/package_ref/index.html">Ray RLlib API</a><input class="toctree-checkbox" id="toctree-checkbox-34" name="toctree-checkbox-34" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-34"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/algorithm-config.html">Algorithm Configuration API</a><input class="toctree-checkbox" id="toctree-checkbox-35" name="toctree-checkbox-35" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-35"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_algo.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_algo</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner_group.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner_group</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.is_multi_agent.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.is_multi_agent</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.is_offline.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.is_offline</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learner_class.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learner_class</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.model_config.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.model_config</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.rl_module_spec.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.rl_module_spec</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.total_train_batch_size.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.total_train_batch_size</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_learner_class.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_learner_class</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_rl_module_spec.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_rl_module_spec</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_evaluation_config_object.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_evaluation_config_object</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_multi_rl_module_spec.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_multi_rl_module_spec</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_multi_agent_setup.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_multi_agent_setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_rollout_fragment_length.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_rollout_fragment_length</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.copy.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.copy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.validate.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.validate</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.freeze.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.freeze</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/algorithm.html">Algorithms</a><input class="toctree-checkbox" id="toctree-checkbox-36" name="toctree-checkbox-36" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-36"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.html">ray.rllib.algorithms.algorithm.Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.setup.html">ray.rllib.algorithms.algorithm.Algorithm.setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_default_config.html">ray.rllib.algorithms.algorithm.Algorithm.get_default_config</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.env_runner.html">ray.rllib.algorithms.algorithm.Algorithm.env_runner</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.eval_env_runner.html">ray.rllib.algorithms.algorithm.Algorithm.eval_env_runner</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.train.html">ray.rllib.algorithms.algorithm.Algorithm.train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.training_step.html">ray.rllib.algorithms.algorithm.Algorithm.training_step</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.save_to_path.html">ray.rllib.algorithms.algorithm.Algorithm.save_to_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.restore_from_path.html">ray.rllib.algorithms.algorithm.Algorithm.restore_from_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.from_checkpoint.html">ray.rllib.algorithms.algorithm.Algorithm.from_checkpoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_state.html">ray.rllib.algorithms.algorithm.Algorithm.get_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.set_state.html">ray.rllib.algorithms.algorithm.Algorithm.set_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.evaluate.html">ray.rllib.algorithms.algorithm.Algorithm.evaluate</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_module.html">ray.rllib.algorithms.algorithm.Algorithm.get_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.add_policy.html">ray.rllib.algorithms.algorithm.Algorithm.add_policy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.remove_policy.html">ray.rllib.algorithms.algorithm.Algorithm.remove_policy</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/callback.html">Callback APIs</a><input class="toctree-checkbox" id="toctree-checkbox-37" name="toctree-checkbox-37" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-37"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.html">ray.rllib.callbacks.callbacks.RLlibCallback</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_algorithm_init.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_algorithm_init</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_sample_end.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_sample_end</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_train_result.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_train_result</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_evaluate_start.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_evaluate_start</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_evaluate_end.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_evaluate_end</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_env_runners_recreated.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_env_runners_recreated</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_checkpoint_loaded.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_checkpoint_loaded</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_environment_created.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_environment_created</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_created.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_created</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_start.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_start</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_step.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_step</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_end.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_end</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/env.html">Environments</a><input class="toctree-checkbox" id="toctree-checkbox-38" name="toctree-checkbox-38" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-38"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/env/env_runner.html">EnvRunner API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/env/single_agent_env_runner.html">SingleAgentEnvRunner API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/env/single_agent_episode.html">SingleAgentEpisode API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/env/multi_agent_env.html">MultiAgentEnv API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/env/multi_agent_env_runner.html">MultiAgentEnvRunner API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/env/multi_agent_episode.html">MultiAgentEpisode API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/env/utils.html">Env Utils</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/rl_modules.html">RLModule APIs</a><input class="toctree-checkbox" id="toctree-checkbox-39" name="toctree-checkbox-39" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-39"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.build.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.build</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.module_class.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.module_class</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.observation_space.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.observation_space</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.action_space.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.action_space</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.inference_only.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.inference_only</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.learner_only.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.learner_only</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.model_config.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.model_config</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec.build.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec.build</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.default_model_config.DefaultModelConfig.html">ray.rllib.core.rl_module.default_model_config.DefaultModelConfig</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.html">ray.rllib.core.rl_module.rl_module.RLModule</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.observation_space.html">ray.rllib.core.rl_module.rl_module.RLModule.observation_space</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.action_space.html">ray.rllib.core.rl_module.rl_module.RLModule.action_space</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.inference_only.html">ray.rllib.core.rl_module.rl_module.RLModule.inference_only</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.model_config.html">ray.rllib.core.rl_module.rl_module.RLModule.model_config</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.setup.html">ray.rllib.core.rl_module.rl_module.RLModule.setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.as_multi_rl_module.html">ray.rllib.core.rl_module.rl_module.RLModule.as_multi_rl_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.forward_exploration.html">ray.rllib.core.rl_module.rl_module.RLModule.forward_exploration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.forward_inference.html">ray.rllib.core.rl_module.rl_module.RLModule.forward_inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.forward_train.html">ray.rllib.core.rl_module.rl_module.RLModule.forward_train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule._forward.html">ray.rllib.core.rl_module.rl_module.RLModule._forward</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule._forward_exploration.html">ray.rllib.core.rl_module.rl_module.RLModule._forward_exploration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule._forward_inference.html">ray.rllib.core.rl_module.rl_module.RLModule._forward_inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule._forward_train.html">ray.rllib.core.rl_module.rl_module.RLModule._forward_train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.save_to_path.html">ray.rllib.core.rl_module.rl_module.RLModule.save_to_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.restore_from_path.html">ray.rllib.core.rl_module.rl_module.RLModule.restore_from_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.from_checkpoint.html">ray.rllib.core.rl_module.rl_module.RLModule.from_checkpoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.get_state.html">ray.rllib.core.rl_module.rl_module.RLModule.get_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.set_state.html">ray.rllib.core.rl_module.rl_module.RLModule.set_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.setup.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.as_multi_rl_module.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.as_multi_rl_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.add_module.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.add_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.remove_module.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.remove_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.save_to_path.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.save_to_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.restore_from_path.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.restore_from_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.from_checkpoint.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.from_checkpoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.get_state.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.get_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.set_state.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.set_state</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/distributions.html">Distribution API</a><input class="toctree-checkbox" id="toctree-checkbox-40" name="toctree-checkbox-40" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-40"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.distributions.Distribution.html">ray.rllib.models.distributions.Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.distributions.Distribution.from_logits.html">ray.rllib.models.distributions.Distribution.from_logits</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.distributions.Distribution.sample.html">ray.rllib.models.distributions.Distribution.sample</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.distributions.Distribution.rsample.html">ray.rllib.models.distributions.Distribution.rsample</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.distributions.Distribution.logp.html">ray.rllib.models.distributions.Distribution.logp</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.distributions.Distribution.kl.html">ray.rllib.models.distributions.Distribution.kl</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/learner.html">LearnerGroup API</a><input class="toctree-checkbox" id="toctree-checkbox-41" name="toctree-checkbox-41" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-41"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learners.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learners</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner_group.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner_group</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner_group.LearnerGroup.html">ray.rllib.core.learner.learner_group.LearnerGroup</a></li>
</ul>
</li>

<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/offline.html">Offline RL API</a><input class="toctree-checkbox" id="toctree-checkbox-42" name="toctree-checkbox-42" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-42"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.offline_data.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.offline_data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learners.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learners</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.env_runners.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.env_runners</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner.html">ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_data.OfflineData.html">ray.rllib.offline.offline_data.OfflineData</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_data.OfflineData.__init__.html">ray.rllib.offline.offline_data.OfflineData.__init__</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_data.OfflineData.sample.html">ray.rllib.offline.offline_data.OfflineData.sample</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_data.OfflineData.default_map_batches_kwargs.html">ray.rllib.offline.offline_data.OfflineData.default_map_batches_kwargs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_data.OfflineData.default_iter_batches_kwargs.html">ray.rllib.offline.offline_data.OfflineData.default_iter_batches_kwargs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.__init__.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner.__init__</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.SCHEMA.html">ray.rllib.offline.offline_prelearner.SCHEMA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.__call__.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner.__call__</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner._map_to_episodes.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner._map_to_episodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner._map_sample_batch_to_episode.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner._map_sample_batch_to_episode</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner._should_module_be_updated.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner._should_module_be_updated</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.default_prelearner_buffer_class.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner.default_prelearner_buffer_class</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.default_prelearner_buffer_kwargs.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner.default_prelearner_buffer_kwargs</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/replay-buffers.html">Replay Buffer API</a><input class="toctree-checkbox" id="toctree-checkbox-43" name="toctree-checkbox-43" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-43"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.StorageUnit.html">ray.rllib.utils.replay_buffers.replay_buffer.StorageUnit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.prioritized_replay_buffer.PrioritizedReplayBuffer.html">ray.rllib.utils.replay_buffers.prioritized_replay_buffer.PrioritizedReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.reservoir_replay_buffer.ReservoirReplayBuffer.html">ray.rllib.utils.replay_buffers.reservoir_replay_buffer.ReservoirReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.sample.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.sample</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.add.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.add</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.get_state.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.get_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.set_state.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.set_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.multi_agent_replay_buffer.MultiAgentReplayBuffer.html">ray.rllib.utils.replay_buffers.multi_agent_replay_buffer.MultiAgentReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.multi_agent_prioritized_replay_buffer.MultiAgentPrioritizedReplayBuffer.html">ray.rllib.utils.replay_buffers.multi_agent_prioritized_replay_buffer.MultiAgentPrioritizedReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.utils.update_priorities_in_replay_buffer.html">ray.rllib.utils.replay_buffers.utils.update_priorities_in_replay_buffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.utils.sample_min_n_steps_from_buffer.html">ray.rllib.utils.replay_buffers.utils.sample_min_n_steps_from_buffer</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/utils.html">RLlib Utilities</a><input class="toctree-checkbox" id="toctree-checkbox-44" name="toctree-checkbox-44" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-44"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.peek.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger.peek</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_value.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_value</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_dict.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_dict</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.merge_and_log_n_dicts.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger.merge_and_log_n_dicts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_time.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_time</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.schedules.scheduler.Scheduler.html">ray.rllib.utils.schedules.scheduler.Scheduler</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.schedules.scheduler.Scheduler.validate.html">ray.rllib.utils.schedules.scheduler.Scheduler.validate</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.schedules.scheduler.Scheduler.get_current_value.html">ray.rllib.utils.schedules.scheduler.Scheduler.get_current_value</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.schedules.scheduler.Scheduler.update.html">ray.rllib.utils.schedules.scheduler.Scheduler.update</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.schedules.scheduler.Scheduler._create_tensor_variable.html">ray.rllib.utils.schedules.scheduler.Scheduler._create_tensor_variable</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.framework.try_import_torch.html">ray.rllib.utils.framework.try_import_torch</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.clip_gradients.html">ray.rllib.utils.torch_utils.clip_gradients</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.compute_global_norm.html">ray.rllib.utils.torch_utils.compute_global_norm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.convert_to_torch_tensor.html">ray.rllib.utils.torch_utils.convert_to_torch_tensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.explained_variance.html">ray.rllib.utils.torch_utils.explained_variance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.flatten_inputs_to_1d_tensor.html">ray.rllib.utils.torch_utils.flatten_inputs_to_1d_tensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.global_norm.html">ray.rllib.utils.torch_utils.global_norm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.one_hot.html">ray.rllib.utils.torch_utils.one_hot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.reduce_mean_ignore_inf.html">ray.rllib.utils.torch_utils.reduce_mean_ignore_inf</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.sequence_mask.html">ray.rllib.utils.torch_utils.sequence_mask</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.set_torch_seed.html">ray.rllib.utils.torch_utils.set_torch_seed</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.softmax_cross_entropy_with_logits.html">ray.rllib.utils.torch_utils.softmax_cross_entropy_with_logits</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.update_target_network.html">ray.rllib.utils.torch_utils.update_target_network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.aligned_array.html">ray.rllib.utils.numpy.aligned_array</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.concat_aligned.html">ray.rllib.utils.numpy.concat_aligned</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.convert_to_numpy.html">ray.rllib.utils.numpy.convert_to_numpy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.fc.html">ray.rllib.utils.numpy.fc</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.flatten_inputs_to_1d_tensor.html">ray.rllib.utils.numpy.flatten_inputs_to_1d_tensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.make_action_immutable.html">ray.rllib.utils.numpy.make_action_immutable</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.huber_loss.html">ray.rllib.utils.numpy.huber_loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.l2_loss.html">ray.rllib.utils.numpy.l2_loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.lstm.html">ray.rllib.utils.numpy.lstm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.one_hot.html">ray.rllib.utils.numpy.one_hot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.relu.html">ray.rllib.utils.numpy.relu</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.sigmoid.html">ray.rllib.utils.numpy.sigmoid</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.softmax.html">ray.rllib.utils.numpy.softmax</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.checkpoints.try_import_msgpack.html">ray.rllib.utils.checkpoints.try_import_msgpack</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.checkpoints.Checkpointable.html">ray.rllib.utils.checkpoints.Checkpointable</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../ray-more-libs/index.html">More Libraries</a><input class="toctree-checkbox" id="toctree-checkbox-45" name="toctree-checkbox-45" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-45"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-more-libs/joblib.html">Distributed Scikit-learn / Joblib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-more-libs/multiprocessing.html">Distributed multiprocessing.Pool</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-more-libs/ray-collective.html">Ray Collective Communication Lib</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-more-libs/dask-on-ray.html">Using Dask on Ray</a><input class="toctree-checkbox" id="toctree-checkbox-46" name="toctree-checkbox-46" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-46"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.RayDaskCallback.html">ray.util.dask.RayDaskCallback</a><input class="toctree-checkbox" id="toctree-checkbox-47" name="toctree-checkbox-47" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-47"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.RayDaskCallback.ray_active.html">ray.util.dask.RayDaskCallback.ray_active</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_presubmit.html">ray.util.dask.callbacks.RayDaskCallback._ray_presubmit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_postsubmit.html">ray.util.dask.callbacks.RayDaskCallback._ray_postsubmit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_pretask.html">ray.util.dask.callbacks.RayDaskCallback._ray_pretask</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_posttask.html">ray.util.dask.callbacks.RayDaskCallback._ray_posttask</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_postsubmit_all.html">ray.util.dask.callbacks.RayDaskCallback._ray_postsubmit_all</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_finish.html">ray.util.dask.callbacks.RayDaskCallback._ray_finish</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-more-libs/raydp.html">Using Spark on Ray (RayDP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-more-libs/mars-on-ray.html">Using Mars on Ray</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-more-libs/modin/index.html">Using Pandas on Ray (Modin)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../workflows/index.html">Ray Workflows (Deprecated)</a><input class="toctree-checkbox" id="toctree-checkbox-48" name="toctree-checkbox-48" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-48"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/basics.html">Getting Started</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/management.html">Workflow Management</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/metadata.html">Workflow Metadata</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/events.html">Events</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/comparison.html">API Comparisons</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/advanced.html">Advanced Topics</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../workflows/api/api.html">Ray Workflows API</a><input class="toctree-checkbox" id="toctree-checkbox-49" name="toctree-checkbox-49" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-49"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../workflows/api/execution.html">Workflow Execution API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../workflows/api/management.html">Workflow Management API</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../cluster/getting-started.html">Ray Clusters</a><input class="toctree-checkbox" id="toctree-checkbox-50" name="toctree-checkbox-50" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-50"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../cluster/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/index.html">Deploying on Kubernetes</a><input class="toctree-checkbox" id="toctree-checkbox-51" name="toctree-checkbox-51" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-51"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/getting-started.html">Getting Started with KubeRay</a><input class="toctree-checkbox" id="toctree-checkbox-52" name="toctree-checkbox-52" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-52"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/getting-started/raycluster-quick-start.html">RayCluster Quickstart</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/getting-started/rayjob-quick-start.html">RayJob Quickstart</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/getting-started/rayservice-quick-start.html">RayService Quickstart</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-53" name="toctree-checkbox-53" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-53"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/rayservice.html">Deploy Ray Serve Apps</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/rayservice-high-availability.html">RayService high availability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/observability.html">KubeRay Observability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/upgrade-guide.html">KubeRay upgrade guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/k8s-cluster-setup.html">Managed Kubernetes services</a></li>



<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/storage.html">Best Practices for Storage and Dependencies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/config.html">RayCluster Configuration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/configuring-autoscaling.html">KubeRay Autoscaling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/kuberay-gcs-ft.html">GCS fault tolerance in KubeRay</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/kuberay-gcs-persistent-ft.html">Tuning Redis for a Persistent Fault Tolerant GCS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/gke-gcs-bucket.html">Configuring KubeRay to use Google Cloud Storage Buckets in GKE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/persist-kuberay-custom-resource-logs.html">Persist KubeRay custom resource logs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/persist-kuberay-operator-logs.html">Persist KubeRay Operator Logs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/gpu.html">Using GPUs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/tpu.html">Use TPUs with KubeRay</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/rayserve-dev-doc.html">Developing Ray Serve Python scripts on a RayCluster</a></li>









<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/pod-command.html">Specify container commands for Ray head/worker Pods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/helm-chart-rbac.html">Helm Chart RBAC</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/tls.html">TLS Authentication</a></li>






<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/k8s-autoscaler.html">(Advanced) Understanding the Ray Autoscaler in the Context of Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.html">(Advanced) Deploying a static Ray cluster without KubeRay</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/kubectl-plugin.html">Use kubectl plugin (beta)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/kuberay-auth.html">Configure Ray clusters with authentication and access control using KubeRay</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/reduce-image-pull-latency.html">Reducing image pull latency on Kubernetes</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/examples.html">Examples</a><input class="toctree-checkbox" id="toctree-checkbox-54" name="toctree-checkbox-54" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-54"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/ml-example.html">Ray Train XGBoostTrainer on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/gpu-training-example.html">Train PyTorch ResNet model with GPUs on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/mnist-training-example.html">Train a PyTorch model on Fashion MNIST with CPUs on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/stable-diffusion-rayservice.html">Serve a StableDiffusion text-to-image model on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/tpu-serve-stable-diffusion.html">Serve a Stable Diffusion model on GKE with TPUs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/mobilenet-rayservice.html">Serve a MobileNet image classifier on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/text-summarizer-rayservice.html">Serve a text summarizer on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/rayjob-batch-inference-example.html">RayJob Batch Inference Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/rayjob-kueue-priority-scheduling.html">Priority Scheduling with RayJob and Kueue</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/rayjob-kueue-gang-scheduling.html">Gang Scheduling with RayJob and Kueue</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/distributed-checkpointing-with-gcsfuse.html">Distributed checkpointing with KubeRay and GCSFuse</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/modin-example.html">Use Modin with Ray on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/vllm-rayservice.html">Serve a Large Language Model with vLLM on Kubernetes</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem.html">KubeRay Ecosystem</a><input class="toctree-checkbox" id="toctree-checkbox-55" name="toctree-checkbox-55" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-55"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/ingress.html">Ingress</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/prometheus-grafana.html">Using Prometheus and Grafana</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/pyspy.html">Profiling with py-spy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/volcano.html">KubeRay integration with Volcano</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/yunikorn.html">KubeRay integration with Apache YuniKorn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/kueue.html">Gang scheduling and priority scheduling for RayJob with Kueue</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/istio.html">mTLS and L7 observability with Istio</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/benchmarks.html">KubeRay Benchmarks</a><input class="toctree-checkbox" id="toctree-checkbox-56" name="toctree-checkbox-56" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-56"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/benchmarks/memory-scalability-benchmark.html">KubeRay memory and scalability benchmark</a></li>

</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/troubleshooting.html">KubeRay Troubleshooting</a><input class="toctree-checkbox" id="toctree-checkbox-57" name="toctree-checkbox-57" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-57"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/troubleshooting/troubleshooting.html">Troubleshooting guide</a></li>

<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/troubleshooting/rayservice-troubleshooting.html">RayService troubleshooting</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/kubernetes/references.html">API Reference</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../cluster/vms/index.html">Deploying on VMs</a><input class="toctree-checkbox" id="toctree-checkbox-58" name="toctree-checkbox-58" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-58"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/vms/getting-started.html">Getting Started</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/vms/user-guides/index.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-59" name="toctree-checkbox-59" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-59"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/user-guides/launching-clusters/index.html">Launching Ray Clusters on AWS, GCP, Azure, vSphere, On-Prem</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/user-guides/large-cluster-best-practices.html">Best practices for deploying large clusters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/user-guides/configuring-autoscaling.html">Configuring Autoscaling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/user-guides/logging.html">Log Persistence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/user-guides/community/index.html">Community Supported Cluster Managers</a></li>

</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/vms/examples/index.html">Examples</a><input class="toctree-checkbox" id="toctree-checkbox-60" name="toctree-checkbox-60" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-60"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/examples/ml-example.html">Ray Train XGBoostTrainer on VMs</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/vms/references/index.html">API References</a><input class="toctree-checkbox" id="toctree-checkbox-61" name="toctree-checkbox-61" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-61"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/references/ray-cluster-cli.html">Cluster Launcher Commands</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/references/ray-cluster-configuration.html">Cluster YAML Configuration Options</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../cluster/metrics.html">Collecting and monitoring metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../cluster/configure-manage-dashboard.html">Configuring and Managing Ray Dashboard</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../cluster/running-applications/index.html">Applications Guide</a><input class="toctree-checkbox" id="toctree-checkbox-62" name="toctree-checkbox-62" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-62"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/index.html">Ray Jobs Overview</a><input class="toctree-checkbox" id="toctree-checkbox-63" name="toctree-checkbox-63" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-63"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/quickstart.html">Quickstart using the Ray Jobs CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/sdk.html">Python SDK Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/jobs-package-ref.html">Python SDK API Reference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/cli.html">Ray Jobs CLI API Reference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/rest.html">Ray Jobs REST API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/ray-client.html">Ray Client</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/running-applications/autoscaling/reference.html">Programmatic Cluster Scaling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../cluster/faq.html">FAQ</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../cluster/package-overview.html">Ray Cluster Management API</a><input class="toctree-checkbox" id="toctree-checkbox-64" name="toctree-checkbox-64" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-64"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/cli.html">Cluster Management CLI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/jobs-package-ref.html">Python SDK API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/cli.html">Ray Jobs CLI API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/running-applications/autoscaling/reference.html">Programmatic Cluster Scaling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../cluster/usage-stats.html">Usage Stats Collection</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../virtual-cluster/getting-started.html">Ray Virtual Clusters</a><input class="toctree-checkbox" id="toctree-checkbox-65" name="toctree-checkbox-65" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-65"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../virtual-cluster/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../virtual-cluster/design-overview.html">Design Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../virtual-cluster/management.html">Virtual Cluster Management API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../virtual-cluster/cli.html">Virtual Cluster CLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../virtual-cluster/examples.html">Examples</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../ray-observability/index.html">Monitoring and Debugging</a><input class="toctree-checkbox" id="toctree-checkbox-66" name="toctree-checkbox-66" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-66"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-observability/getting-started.html">Ray Dashboard</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-observability/ray-distributed-debugger.html">Ray Distributed Debugger</a></li>



<li class="toctree-l2"><a class="reference internal" href="../../../../ray-observability/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-observability/user-guides/index.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-67" name="toctree-checkbox-67" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-67"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/index.html">Debugging Applications</a><input class="toctree-checkbox" id="toctree-checkbox-68" name="toctree-checkbox-68" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-68"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/general-debugging.html">General Debugging</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/debug-memory.html">Debugging Memory Issues</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/debug-hangs.html">Debugging Hangs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/debug-failures.html">Debugging Failures</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/optimize-performance.html">Optimizing Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-observability/ray-distributed-debugger.html">Ray Distributed Debugger</a></li>



<li class="toctree-l4"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/ray-debugging.html">Using the Ray Debugger</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/user-guides/cli-sdk.html">Monitoring with the CLI or SDK</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/user-guides/configure-logging.html">Configuring Logging</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/user-guides/profiling.html">Profiling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/user-guides/add-app-metrics.html">Adding Application-Level Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/user-guides/ray-tracing.html">Tracing</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-observability/reference/index.html">Reference</a><input class="toctree-checkbox" id="toctree-checkbox-69" name="toctree-checkbox-69" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-69"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/reference/api.html">State API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/reference/cli.html">State CLI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/reference/system-metrics.html">System Metrics</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../ray-contribute/index.html">Developer Guides</a><input class="toctree-checkbox" id="toctree-checkbox-70" name="toctree-checkbox-70" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-70"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-contribute/stability.html">API Stability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-contribute/api-policy.html">API Policy</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-contribute/getting-involved.html">Getting Involved / Contributing</a><input class="toctree-checkbox" id="toctree-checkbox-71" name="toctree-checkbox-71" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-71"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/development.html">Building Ray from Source</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/ci.html">CI Testing Workflow on PRs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/docs.html">Contributing to the Ray Documentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/writing-code-snippets.html">How to write code snippets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/fake-autoscaler.html">Testing Autoscaling Locally</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/testing-tips.html">Tips for testing Ray programs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/debugging.html">Debugging for Ray Developers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/profiling.html">Profiling for Ray Developers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-core/configure.html">Configuring Ray</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-contribute/whitepaper.html">Architecture Whitepapers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ray-references/glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ray-security/index.html">Security</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">ray.rllib.en...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <h1>Source code for ray.rllib.env.multi_agent_episode</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Any</span><span class="p">,</span>
    <span class="n">Callable</span><span class="p">,</span>
    <span class="n">Collection</span><span class="p">,</span>
    <span class="n">DefaultDict</span><span class="p">,</span>
    <span class="n">Dict</span><span class="p">,</span>
    <span class="n">List</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">Set</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">import</span> <span class="nn">uuid</span>

<span class="kn">import</span> <span class="nn">gymnasium</span> <span class="k">as</span> <span class="nn">gym</span>

<span class="kn">from</span> <span class="nn">ray.rllib.env.single_agent_episode</span> <span class="kn">import</span> <span class="n">SingleAgentEpisode</span>
<span class="kn">from</span> <span class="nn">ray.rllib.env.utils.infinite_lookback_buffer</span> <span class="kn">import</span> <span class="n">InfiniteLookbackBuffer</span>
<span class="kn">from</span> <span class="nn">ray.rllib.policy.sample_batch</span> <span class="kn">import</span> <span class="n">MultiAgentBatch</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils</span> <span class="kn">import</span> <span class="n">force_list</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.deprecation</span> <span class="kn">import</span> <span class="n">Deprecated</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.error</span> <span class="kn">import</span> <span class="n">MultiAgentEnvError</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.spaces.space_utils</span> <span class="kn">import</span> <span class="n">batch</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.typing</span> <span class="kn">import</span> <span class="n">AgentID</span><span class="p">,</span> <span class="n">ModuleID</span><span class="p">,</span> <span class="n">MultiAgentDict</span>
<span class="kn">from</span> <span class="nn">ray.util.annotations</span> <span class="kn">import</span> <span class="n">PublicAPI</span>


<span class="c1"># TODO (simon): Include cases in which the number of agents in an</span>
<span class="c1">#  episode are shrinking or growing during the episode itself.</span>
<div class="viewcode-block" id="MultiAgentEpisode">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode">[docs]</a>
<span class="nd">@PublicAPI</span><span class="p">(</span><span class="n">stability</span><span class="o">=</span><span class="s2">&quot;alpha&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">MultiAgentEpisode</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Stores multi-agent episode data.</span>

<span class="sd">    The central attribute of the class is the timestep mapping</span>
<span class="sd">    `self.env_t_to_agent_t` that maps AgentIDs to their specific environment steps to</span>
<span class="sd">    the agent&#39;s own scale/timesteps.</span>

<span class="sd">    Each AgentID in the `MultiAgentEpisode` has its own `SingleAgentEpisode` object</span>
<span class="sd">    in which this agent&#39;s data is stored. Together with the env_t_to_agent_t mapping,</span>
<span class="sd">    we can extract information either on any individual agent&#39;s time scale or from</span>
<span class="sd">    the (global) multi-agent environment time scale.</span>

<span class="sd">    Extraction of data from a MultiAgentEpisode happens via the getter APIs, e.g.</span>
<span class="sd">    `get_observations()`, which work analogous to the ones implemented in the</span>
<span class="sd">    `SingleAgentEpisode` class.</span>

<span class="sd">    Note that recorded `terminateds`/`truncateds` come as simple</span>
<span class="sd">    `MultiAgentDict`s mapping AgentID to bools and thus have no assignment to a</span>
<span class="sd">    certain timestep (analogous to a SingleAgentEpisode&#39;s single `terminated/truncated`</span>
<span class="sd">    boolean flag). Instead we assign it to the last observation recorded.</span>
<span class="sd">    Theoretically, there could occur edge cases in some environments</span>
<span class="sd">    where an agent receives partial rewards and then terminates without</span>
<span class="sd">    a last observation. In these cases, we duplicate the last observation.</span>

<span class="sd">    Also, if no initial observation has been received yet for an agent, but</span>
<span class="sd">    some  rewards for this same agent already occurred, we delete the agent&#39;s data</span>
<span class="sd">    up to here, b/c there is nothing to learn from these &quot;premature&quot; rewards.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;id_&quot;</span><span class="p">,</span>
        <span class="s2">&quot;agent_to_module_mapping_fn&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_agent_to_module_mapping&quot;</span><span class="p">,</span>
        <span class="s2">&quot;observation_space&quot;</span><span class="p">,</span>
        <span class="s2">&quot;action_space&quot;</span><span class="p">,</span>
        <span class="s2">&quot;env_t_started&quot;</span><span class="p">,</span>
        <span class="s2">&quot;env_t&quot;</span><span class="p">,</span>
        <span class="s2">&quot;agent_t_started&quot;</span><span class="p">,</span>
        <span class="s2">&quot;env_t_to_agent_t&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_hanging_actions_end&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_hanging_extra_model_outputs_end&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_hanging_rewards_end&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_hanging_rewards_begin&quot;</span><span class="p">,</span>
        <span class="s2">&quot;is_terminated&quot;</span><span class="p">,</span>
        <span class="s2">&quot;is_truncated&quot;</span><span class="p">,</span>
        <span class="s2">&quot;agent_episodes&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_last_step_time&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_len_lookback_buffers&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_start_time&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_temporary_timestep_data&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">SKIP_ENV_TS_TAG</span> <span class="o">=</span> <span class="s2">&quot;S&quot;</span>

<div class="viewcode-block" id="MultiAgentEpisode.__init__">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.__init__.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">id_</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">observations</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">observation_space</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">gym</span><span class="o">.</span><span class="n">Space</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">infos</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">actions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">action_space</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">gym</span><span class="o">.</span><span class="n">Space</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">rewards</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">terminateds</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">truncateds</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">extra_model_outputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">env_t_started</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">agent_t_started</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="n">AgentID</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">len_lookback_buffer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="n">agent_episode_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="n">AgentID</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">agent_module_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="n">AgentID</span><span class="p">,</span> <span class="n">ModuleID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">agent_to_module_mapping_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Callable</span><span class="p">[[</span><span class="n">AgentID</span><span class="p">,</span> <span class="s2">&quot;MultiAgentEpisode&quot;</span><span class="p">],</span> <span class="n">ModuleID</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes a `MultiAgentEpisode`.</span>

<span class="sd">        Args:</span>
<span class="sd">            id_: Optional. Either a string to identify an episode or None.</span>
<span class="sd">                If None, a hexadecimal id is created. In case of providing</span>
<span class="sd">                a string, make sure that it is unique, as episodes get</span>
<span class="sd">                concatenated via this string.</span>
<span class="sd">            observations: A list of dictionaries mapping agent IDs to observations.</span>
<span class="sd">                Can be None. If provided, should match all other episode data</span>
<span class="sd">                (actions, rewards, etc.) in terms of list lengths and agent IDs.</span>
<span class="sd">            observation_space: An optional gym.spaces.Dict mapping agent IDs to</span>
<span class="sd">                individual agents&#39; spaces, which all (individual agents&#39;) observations</span>
<span class="sd">                should abide to. If not None and this MultiAgentEpisode is numpy&#39;ized</span>
<span class="sd">                (via the `self.to_numpy()` method), and data is appended or set, the new</span>
<span class="sd">                data will be checked for correctness.</span>
<span class="sd">            infos: A list of dictionaries mapping agent IDs to info dicts.</span>
<span class="sd">                Can be None. If provided, should match all other episode data</span>
<span class="sd">                (observations, rewards, etc.) in terms of list lengths and agent IDs.</span>
<span class="sd">            actions: A list of dictionaries mapping agent IDs to actions.</span>
<span class="sd">                Can be None. If provided, should match all other episode data</span>
<span class="sd">                (observations, rewards, etc.) in terms of list lengths and agent IDs.</span>
<span class="sd">            action_space: An optional gym.spaces.Dict mapping agent IDs to</span>
<span class="sd">                individual agents&#39; spaces, which all (individual agents&#39;) actions</span>
<span class="sd">                should abide to. If not None and this MultiAgentEpisode is numpy&#39;ized</span>
<span class="sd">                (via the `self.to_numpy()` method), and data is appended or set, the new</span>
<span class="sd">                data will be checked for correctness.</span>
<span class="sd">            rewards: A list of dictionaries mapping agent IDs to rewards.</span>
<span class="sd">                Can be None. If provided, should match all other episode data</span>
<span class="sd">                (actions, rewards, etc.) in terms of list lengths and agent IDs.</span>
<span class="sd">            terminateds: A boolean defining if an environment has</span>
<span class="sd">                terminated OR a MultiAgentDict mapping individual agent ids</span>
<span class="sd">                to boolean flags indicating whether individual agents have terminated.</span>
<span class="sd">                A special __all__ key in these dicts indicates, whether the episode</span>
<span class="sd">                is terminated for all agents.</span>
<span class="sd">                The default is `False`, i.e. the episode has not been terminated.</span>
<span class="sd">            truncateds: A boolean defining if the environment has been</span>
<span class="sd">                truncated OR a MultiAgentDict mapping individual agent ids</span>
<span class="sd">                to boolean flags indicating whether individual agents have been</span>
<span class="sd">                truncated. A special __all__ key in these dicts indicates, whether the</span>
<span class="sd">                episode is truncated for all agents.</span>
<span class="sd">                The default is `False`, i.e. the episode has not been truncated.</span>
<span class="sd">            extra_model_outputs: A list of dictionaries mapping agent IDs to their</span>
<span class="sd">                corresponding extra model outputs. Each of these &quot;outputs&quot; is a dict</span>
<span class="sd">                mapping keys (str) to model output values, for example for</span>
<span class="sd">                `key=STATE_OUT`, the values would be the internal state outputs for</span>
<span class="sd">                that agent.</span>
<span class="sd">            env_t_started: The env timestep (int) that defines the starting point</span>
<span class="sd">                of the episode. This is only larger zero, if an already ongoing episode</span>
<span class="sd">                chunk is being created, for example by slicing an ongoing episode or</span>
<span class="sd">                by calling the `cut()` method on an ongoing episode.</span>
<span class="sd">            agent_t_started: A dict mapping AgentIDs to the respective agent&#39;s (local)</span>
<span class="sd">                timestep at which its SingleAgentEpisode chunk started.</span>
<span class="sd">            len_lookback_buffer: The size of the lookback buffers to keep in</span>
<span class="sd">                front of this Episode for each type of data (observations, actions,</span>
<span class="sd">                etc..). If larger 0, will interpret the first `len_lookback_buffer`</span>
<span class="sd">                items in each type of data as NOT part of this actual</span>
<span class="sd">                episode chunk, but instead serve as &quot;historical&quot; record that may be</span>
<span class="sd">                viewed and used to derive new data from. For example, it might be</span>
<span class="sd">                necessary to have a lookback buffer of four if you would like to do</span>
<span class="sd">                observation frame stacking and your episode has been cut and you are now</span>
<span class="sd">                operating on a new chunk (continuing from the cut one). Then, for the</span>
<span class="sd">                first 3 items, you would have to be able to look back into the old</span>
<span class="sd">                chunk&#39;s data.</span>
<span class="sd">                If `len_lookback_buffer` is &quot;auto&quot; (default), will interpret all</span>
<span class="sd">                provided data in the constructor as part of the lookback buffers.</span>
<span class="sd">            agent_episode_ids: An optional dict mapping AgentIDs</span>
<span class="sd">                to their corresponding `SingleAgentEpisode`. If None, each</span>
<span class="sd">                `SingleAgentEpisode` in `MultiAgentEpisode.agent_episodes`</span>
<span class="sd">                will generate a hexadecimal code. If a dictionary is provided,</span>
<span class="sd">                make sure that IDs are unique, because the agents&#39; `SingleAgentEpisode`</span>
<span class="sd">                instances are concatenated or recreated by it.</span>
<span class="sd">            agent_module_ids: An optional dict mapping AgentIDs to their respective</span>
<span class="sd">                ModuleIDs (these mapping are always valid for an entire episode and</span>
<span class="sd">                thus won&#39;t change during the course of this episode). If a mapping from</span>
<span class="sd">                agent to module has already been provided via this dict, the (optional)</span>
<span class="sd">                `agent_to_module_mapping_fn` will NOT be used again to map the same</span>
<span class="sd">                agent (agents do not change their assigned module in the course of</span>
<span class="sd">                one episode).</span>
<span class="sd">            agent_to_module_mapping_fn: A callable taking an AgentID and a</span>
<span class="sd">                MultiAgentEpisode as args and returning a ModuleID. Used to map agents</span>
<span class="sd">                that have not been mapped yet (because they just entered this episode)</span>
<span class="sd">                to a ModuleID. The resulting ModuleID is only stored inside the agent&#39;s</span>
<span class="sd">                SingleAgentEpisode object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">id_</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">id_</span> <span class="ow">or</span> <span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">()</span><span class="o">.</span><span class="n">hex</span>
        <span class="k">if</span> <span class="n">agent_to_module_mapping_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm_config</span> <span class="kn">import</span> <span class="n">AlgorithmConfig</span>

            <span class="n">agent_to_module_mapping_fn</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">AlgorithmConfig</span><span class="o">.</span><span class="n">DEFAULT_AGENT_TO_MODULE_MAPPING_FN</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">agent_to_module_mapping_fn</span> <span class="o">=</span> <span class="n">agent_to_module_mapping_fn</span>
        <span class="c1"># In case a user - e.g. via callbacks - already forces a mapping to happen</span>
        <span class="c1"># via the `module_for()` API even before the agent has entered the episode</span>
        <span class="c1"># (and has its SingleAgentEpisode created), we store all aldeary done mappings</span>
        <span class="c1"># in this dict here.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_agent_to_module_mapping</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">AgentID</span><span class="p">,</span> <span class="n">ModuleID</span><span class="p">]</span> <span class="o">=</span> <span class="n">agent_module_ids</span> <span class="ow">or</span> <span class="p">{}</span>

        <span class="c1"># Lookback buffer length is not provided. Interpret all provided data as</span>
        <span class="c1"># lookback buffer.</span>
        <span class="k">if</span> <span class="n">len_lookback_buffer</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
            <span class="n">len_lookback_buffer</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">rewards</span> <span class="ow">or</span> <span class="p">[])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_len_lookback_buffers</span> <span class="o">=</span> <span class="n">len_lookback_buffer</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span> <span class="o">=</span> <span class="n">observation_space</span> <span class="ow">or</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="n">action_space</span> <span class="ow">or</span> <span class="p">{}</span>

        <span class="n">terminateds</span> <span class="o">=</span> <span class="n">terminateds</span> <span class="ow">or</span> <span class="p">{}</span>
        <span class="n">truncateds</span> <span class="o">=</span> <span class="n">truncateds</span> <span class="ow">or</span> <span class="p">{}</span>

        <span class="c1"># The global last timestep of the episode and the timesteps when this chunk</span>
        <span class="c1"># started (excluding a possible lookback buffer).</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env_t_started</span> <span class="o">=</span> <span class="n">env_t_started</span> <span class="ow">or</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env_t</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span> <span class="k">if</span> <span class="n">rewards</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
            <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_len_lookback_buffers</span>
            <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_t_started</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">agent_t_started</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">agent_t_started</span> <span class="ow">or</span> <span class="p">{})</span>

        <span class="c1"># Keeps track of the correspondence between agent steps and environment steps.</span>
        <span class="c1"># Under each AgentID as key is a InfiniteLookbackBuffer with the following</span>
        <span class="c1"># data in it:</span>
        <span class="c1"># The indices of the items in the data represent environment timesteps,</span>
        <span class="c1"># starting from index=0 for the `env.reset()` and with each `env.step()` call</span>
        <span class="c1"># increase by 1.</span>
        <span class="c1"># The values behind these (env timestep) indices represent the agent timesteps</span>
        <span class="c1"># happening at these env timesteps and the special value of</span>
        <span class="c1"># `self.SKIP_ENV_TS_TAG` means that the agent did NOT step at the given env</span>
        <span class="c1"># timestep.</span>
        <span class="c1"># Thus, agents that are part of the reset obs, will start their mapping data</span>
        <span class="c1"># with a [0 ...], all other agents will start their mapping data with:</span>
        <span class="c1"># [self.SKIP_ENV_TS_TAG, ...].</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="p">:</span> <span class="n">DefaultDict</span><span class="p">[</span>
            <span class="n">AgentID</span><span class="p">,</span> <span class="n">InfiniteLookbackBuffer</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="n">InfiniteLookbackBuffer</span><span class="p">)</span>

        <span class="c1"># Create caches for hanging actions/rewards/extra_model_outputs.</span>
        <span class="c1"># When an agent gets an observation (and then sends an action), but does not</span>
        <span class="c1"># receive immediately a next observation, we store the &quot;hanging&quot; action (and</span>
        <span class="c1"># related rewards and extra model outputs) in the caches postfixed w/ `_end`</span>
        <span class="c1"># until the next observation is received.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_actions_end</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_extra_model_outputs_end</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">dict</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_rewards_end</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>

        <span class="c1"># In case of a `cut()` or `slice()`, we also need to store the hanging actions,</span>
        <span class="c1"># rewards, and extra model outputs that were already &quot;hanging&quot; in preceeding</span>
        <span class="c1"># episode slice.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_rewards_begin</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>

        <span class="c1"># If this is an ongoing episode than the last `__all__` should be `False`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_terminated</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">terminateds</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">terminateds</span><span class="p">,</span> <span class="nb">bool</span><span class="p">)</span>
            <span class="k">else</span> <span class="n">terminateds</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;__all__&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># If this is an ongoing episode than the last `__all__` should be `False`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_truncated</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">truncateds</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">truncateds</span><span class="p">,</span> <span class="nb">bool</span><span class="p">)</span>
            <span class="k">else</span> <span class="n">truncateds</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;__all__&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># The individual agent SingleAgentEpisode objects.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">AgentID</span><span class="p">,</span> <span class="n">SingleAgentEpisode</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_single_agent_episodes</span><span class="p">(</span>
            <span class="n">agent_module_ids</span><span class="o">=</span><span class="n">agent_module_ids</span><span class="p">,</span>
            <span class="n">agent_episode_ids</span><span class="o">=</span><span class="n">agent_episode_ids</span><span class="p">,</span>
            <span class="n">observations</span><span class="o">=</span><span class="n">observations</span><span class="p">,</span>
            <span class="n">infos</span><span class="o">=</span><span class="n">infos</span><span class="p">,</span>
            <span class="n">actions</span><span class="o">=</span><span class="n">actions</span><span class="p">,</span>
            <span class="n">rewards</span><span class="o">=</span><span class="n">rewards</span><span class="p">,</span>
            <span class="n">terminateds</span><span class="o">=</span><span class="n">terminateds</span><span class="p">,</span>
            <span class="n">truncateds</span><span class="o">=</span><span class="n">truncateds</span><span class="p">,</span>
            <span class="n">extra_model_outputs</span><span class="o">=</span><span class="n">extra_model_outputs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Caches for temporary per-timestep data. May be used to store custom metrics</span>
        <span class="c1"># from within a callback for the ongoing episode (e.g. render images).</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_temporary_timestep_data</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>

        <span class="c1"># Keep timer stats on deltas between steps.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_start_time</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_step_time</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Validate ourselves.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span></div>


<div class="viewcode-block" id="MultiAgentEpisode.add_env_reset">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.add_env_reset.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.add_env_reset">[docs]</a>
    <span class="k">def</span> <span class="nf">add_env_reset</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">observations</span><span class="p">:</span> <span class="n">MultiAgentDict</span><span class="p">,</span>
        <span class="n">infos</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Stores initial observation.</span>

<span class="sd">        Args:</span>
<span class="sd">            observations: A dictionary mapping agent IDs to initial observations.</span>
<span class="sd">                Note that some agents may not have an initial observation.</span>
<span class="sd">            infos: A dictionary mapping agent IDs to initial info dicts.</span>
<span class="sd">                Note that some agents may not have an initial info dict. If not None,</span>
<span class="sd">                the agent IDs in `infos` must be a subset of those in `observations`</span>
<span class="sd">                meaning it would not be allowed to have an agent with an info dict,</span>
<span class="sd">                but not with an observation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_done</span>
        <span class="c1"># Assume that this episode is completely empty and has not stepped yet.</span>
        <span class="c1"># Leave self.env_t (and self.env_t_started) at 0.</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_t</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_t_started</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="n">infos</span> <span class="o">=</span> <span class="n">infos</span> <span class="ow">or</span> <span class="p">{}</span>

        <span class="c1"># Note, all agents will have an initial observation, some may have an initial</span>
        <span class="c1"># info dict as well.</span>
        <span class="k">for</span> <span class="n">agent_id</span><span class="p">,</span> <span class="n">agent_obs</span> <span class="ow">in</span> <span class="n">observations</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># Update env_t_to_agent_t mapping (all agents that are part of the reset</span>
            <span class="c1"># obs have their first mapping 0 (env_t) -&gt; 0 (agent_t)).</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="c1"># Create SingleAgentEpisode, if necessary.</span>
            <span class="k">if</span> <span class="n">agent_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">SingleAgentEpisode</span><span class="p">(</span>
                    <span class="n">agent_id</span><span class="o">=</span><span class="n">agent_id</span><span class="p">,</span>
                    <span class="n">module_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">module_for</span><span class="p">(</span><span class="n">agent_id</span><span class="p">),</span>
                    <span class="n">multi_agent_episode_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">id_</span><span class="p">,</span>
                    <span class="n">observation_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">),</span>
                    <span class="n">action_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">),</span>
                <span class="p">)</span>
            <span class="c1"># Add initial observations (and infos) to the agent&#39;s episode.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span><span class="o">.</span><span class="n">add_env_reset</span><span class="p">(</span>
                <span class="n">observation</span><span class="o">=</span><span class="n">agent_obs</span><span class="p">,</span>
                <span class="n">infos</span><span class="o">=</span><span class="n">infos</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">),</span>
            <span class="p">)</span>

        <span class="c1"># Validate our data.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span>

        <span class="c1"># Start the timer for this episode.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span></div>


<div class="viewcode-block" id="MultiAgentEpisode.add_env_step">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.add_env_step.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.add_env_step">[docs]</a>
    <span class="k">def</span> <span class="nf">add_env_step</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">observations</span><span class="p">:</span> <span class="n">MultiAgentDict</span><span class="p">,</span>
        <span class="n">actions</span><span class="p">:</span> <span class="n">MultiAgentDict</span><span class="p">,</span>
        <span class="n">rewards</span><span class="p">:</span> <span class="n">MultiAgentDict</span><span class="p">,</span>
        <span class="n">infos</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">terminateds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">truncateds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">extra_model_outputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Adds a timestep to the episode.</span>

<span class="sd">        Args:</span>
<span class="sd">            observations: A dictionary mapping agent IDs to their corresponding</span>
<span class="sd">                next observations. Note that some agents may not have stepped at this</span>
<span class="sd">                timestep.</span>
<span class="sd">            actions: Mandatory. A dictionary mapping agent IDs to their</span>
<span class="sd">                corresponding actions. Note that some agents may not have stepped at</span>
<span class="sd">                this timestep.</span>
<span class="sd">            rewards: Mandatory. A dictionary mapping agent IDs to their</span>
<span class="sd">                corresponding observations. Note that some agents may not have stepped</span>
<span class="sd">                at this timestep.</span>
<span class="sd">            infos: A dictionary mapping agent IDs to their</span>
<span class="sd">                corresponding info. Note that some agents may not have stepped at this</span>
<span class="sd">                timestep.</span>
<span class="sd">            terminateds: A dictionary mapping agent IDs to their `terminated` flags,</span>
<span class="sd">                indicating, whether the environment has been terminated for them.</span>
<span class="sd">                A special `__all__` key indicates that the episode is terminated for</span>
<span class="sd">                all agent IDs.</span>
<span class="sd">            terminateds: A dictionary mapping agent IDs to their `truncated` flags,</span>
<span class="sd">                indicating, whether the environment has been truncated for them.</span>
<span class="sd">                A special `__all__` key indicates that the episode is `truncated` for</span>
<span class="sd">                all agent IDs.</span>
<span class="sd">            extra_model_outputs: A dictionary mapping agent IDs to their</span>
<span class="sd">                corresponding specific model outputs (also in a dictionary; e.g.</span>
<span class="sd">                `vf_preds` for PPO).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Cannot add data to an already done episode.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_done</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MultiAgentEnvError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot call `add_env_step` on a MultiAgentEpisode that is already &quot;</span>
                <span class="s2">&quot;done!&quot;</span>
            <span class="p">)</span>

        <span class="n">infos</span> <span class="o">=</span> <span class="n">infos</span> <span class="ow">or</span> <span class="p">{}</span>
        <span class="n">terminateds</span> <span class="o">=</span> <span class="n">terminateds</span> <span class="ow">or</span> <span class="p">{}</span>
        <span class="n">truncateds</span> <span class="o">=</span> <span class="n">truncateds</span> <span class="ow">or</span> <span class="p">{}</span>
        <span class="n">extra_model_outputs</span> <span class="o">=</span> <span class="n">extra_model_outputs</span> <span class="ow">or</span> <span class="p">{}</span>

        <span class="c1"># Increase (global) env step by one.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env_t</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Find out, whether this episode is terminated/truncated (for all agents).</span>
        <span class="c1"># Case 1: all agents are terminated or all are truncated.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_terminated</span> <span class="o">=</span> <span class="n">terminateds</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;__all__&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_truncated</span> <span class="o">=</span> <span class="n">truncateds</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;__all__&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="c1"># Find all agents that were done at prior timesteps and add the agents that are</span>
        <span class="c1"># done at the present timestep.</span>
        <span class="n">agents_done</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
            <span class="p">[</span><span class="n">aid</span> <span class="k">for</span> <span class="n">aid</span><span class="p">,</span> <span class="n">sa_eps</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">sa_eps</span><span class="o">.</span><span class="n">is_done</span><span class="p">]</span>
            <span class="o">+</span> <span class="p">[</span><span class="n">aid</span> <span class="k">for</span> <span class="n">aid</span> <span class="ow">in</span> <span class="n">terminateds</span> <span class="k">if</span> <span class="n">terminateds</span><span class="p">[</span><span class="n">aid</span><span class="p">]]</span>
            <span class="o">+</span> <span class="p">[</span><span class="n">aid</span> <span class="k">for</span> <span class="n">aid</span> <span class="ow">in</span> <span class="n">truncateds</span> <span class="k">if</span> <span class="n">truncateds</span><span class="p">[</span><span class="n">aid</span><span class="p">]]</span>
        <span class="p">)</span>
        <span class="c1"># Case 2: Some agents are truncated and the others are terminated -&gt; Declare</span>
        <span class="c1"># this episode as terminated.</span>
        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">aid</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">agents_done</span><span class="p">)</span> <span class="k">for</span> <span class="n">aid</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_ids</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_terminated</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># For all agents that are not stepping in this env step, but that are not done</span>
        <span class="c1"># yet -&gt; Add a skip tag to their env- to agent-step mappings.</span>
        <span class="n">stepped_agent_ids</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">observations</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">for</span> <span class="n">agent_id</span><span class="p">,</span> <span class="n">env_t_to_agent_t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">agent_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stepped_agent_ids</span><span class="p">:</span>
                <span class="n">env_t_to_agent_t</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">SKIP_ENV_TS_TAG</span><span class="p">)</span>

        <span class="c1"># Loop through all agent IDs that we received data for in this step:</span>
        <span class="c1"># Those found in observations, actions, and rewards.</span>
        <span class="n">agent_ids_with_data</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">set</span><span class="p">(</span><span class="n">observations</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
            <span class="o">|</span> <span class="nb">set</span><span class="p">(</span><span class="n">actions</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
            <span class="o">|</span> <span class="nb">set</span><span class="p">(</span><span class="n">rewards</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
            <span class="o">|</span> <span class="nb">set</span><span class="p">(</span><span class="n">terminateds</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
            <span class="o">|</span> <span class="nb">set</span><span class="p">(</span><span class="n">truncateds</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
            <span class="o">|</span> <span class="nb">set</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">terminateds</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;__all__&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">truncateds</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;__all__&quot;</span><span class="p">)</span>
                <span class="k">else</span> <span class="nb">set</span><span class="p">()</span>
            <span class="p">)</span>
        <span class="p">)</span> <span class="o">-</span> <span class="p">{</span><span class="s2">&quot;__all__&quot;</span><span class="p">}</span>
        <span class="k">for</span> <span class="n">agent_id</span> <span class="ow">in</span> <span class="n">agent_ids_with_data</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">agent_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="p">:</span>
                <span class="n">sa_episode</span> <span class="o">=</span> <span class="n">SingleAgentEpisode</span><span class="p">(</span>
                    <span class="n">agent_id</span><span class="o">=</span><span class="n">agent_id</span><span class="p">,</span>
                    <span class="n">module_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">module_for</span><span class="p">(</span><span class="n">agent_id</span><span class="p">),</span>
                    <span class="n">multi_agent_episode_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">id_</span><span class="p">,</span>
                    <span class="n">observation_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">),</span>
                    <span class="n">action_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">),</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">sa_episode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span>

            <span class="c1"># Collect value to be passed (at end of for-loop) into `add_env_step()`</span>
            <span class="c1"># call.</span>
            <span class="n">_observation</span> <span class="o">=</span> <span class="n">observations</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">)</span>
            <span class="n">_action</span> <span class="o">=</span> <span class="n">actions</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">)</span>
            <span class="n">_reward</span> <span class="o">=</span> <span class="n">rewards</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">)</span>
            <span class="n">_infos</span> <span class="o">=</span> <span class="n">infos</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">)</span>
            <span class="n">_terminated</span> <span class="o">=</span> <span class="n">terminateds</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_terminated</span>
            <span class="n">_truncated</span> <span class="o">=</span> <span class="n">truncateds</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_truncated</span>
            <span class="n">_extra_model_outputs</span> <span class="o">=</span> <span class="n">extra_model_outputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">)</span>

            <span class="c1"># The value to place into the env- to agent-step map for this agent ID.</span>
            <span class="c1"># _agent_step = self.SKIP_ENV_TS_TAG</span>

            <span class="c1"># Agents, whose SingleAgentEpisode had already been done before this</span>
            <span class="c1"># step should NOT have received any data in this step.</span>
            <span class="k">if</span> <span class="n">sa_episode</span><span class="o">.</span><span class="n">is_done</span> <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span>
                <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span><span class="n">_observation</span><span class="p">,</span> <span class="n">_action</span><span class="p">,</span> <span class="n">_reward</span><span class="p">,</span> <span class="n">_infos</span><span class="p">,</span> <span class="n">_extra_model_outputs</span><span class="p">]</span>
            <span class="p">):</span>

                <span class="k">raise</span> <span class="n">MultiAgentEnvError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Agent </span><span class="si">{</span><span class="n">agent_id</span><span class="si">}</span><span class="s2"> already had its `SingleAgentEpisode.is_done` &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;set to True, but still received data in a following step! &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;obs=</span><span class="si">{</span><span class="n">_observation</span><span class="si">}</span><span class="s2"> act=</span><span class="si">{</span><span class="n">_action</span><span class="si">}</span><span class="s2"> rew=</span><span class="si">{</span><span class="n">_reward</span><span class="si">}</span><span class="s2"> info=</span><span class="si">{</span><span class="n">_infos</span><span class="si">}</span><span class="s2"> &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;extra_model_outputs=</span><span class="si">{</span><span class="n">_extra_model_outputs</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
            <span class="n">_reward</span> <span class="o">=</span> <span class="n">_reward</span> <span class="ow">or</span> <span class="mf">0.0</span>

            <span class="c1"># CASE 1: A complete agent step is available (in one env step).</span>
            <span class="c1"># -------------------------------------------------------------</span>
            <span class="c1"># We have an observation and an action for this agent -&gt;</span>
            <span class="c1"># Add the agent step to the single agent episode.</span>
            <span class="c1"># ... action -&gt; next obs + reward ...</span>
            <span class="k">if</span> <span class="n">_observation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">_action</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">agent_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">rewards</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="n">MultiAgentEnvError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Agent </span><span class="si">{</span><span class="n">agent_id</span><span class="si">}</span><span class="s2"> acted (and received next obs), but did NOT &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;receive any reward from the env!&quot;</span>
                    <span class="p">)</span>

            <span class="c1"># CASE 2: Step gets completed with a hanging action OR first observation.</span>
            <span class="c1"># ------------------------------------------------------------------------</span>
            <span class="c1"># We have an observation, but no action -&gt;</span>
            <span class="c1"># a) Action (and extra model outputs) must be hanging already. Also use</span>
            <span class="c1"># collected hanging rewards and extra_model_outputs.</span>
            <span class="c1"># b) The observation is the first observation for this agent ID.</span>
            <span class="k">elif</span> <span class="n">_observation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">_action</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">_action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_actions_end</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">agent_id</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

                <span class="c1"># We have a hanging action (the agent had acted after the previous</span>
                <span class="c1"># observation, but the env had not responded - until now - with another</span>
                <span class="c1"># observation).</span>
                <span class="c1"># ...[hanging action] ... ... -&gt; next obs + (reward)? ...</span>
                <span class="k">if</span> <span class="n">_action</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="c1"># Get the extra model output if available.</span>
                    <span class="n">_extra_model_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_extra_model_outputs_end</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span>
                        <span class="n">agent_id</span><span class="p">,</span> <span class="kc">None</span>
                    <span class="p">)</span>
                    <span class="n">_reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_rewards_end</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">agent_id</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span> <span class="o">+</span> <span class="n">_reward</span>
                <span class="c1"># First observation for this agent, we have no hanging action.</span>
                <span class="c1"># ... [done]? ... -&gt; [1st obs for agent ID]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># The agent is already done -&gt; The agent thus has never stepped once</span>
                    <span class="c1"># and we do not have to create a SingleAgentEpisode for it.</span>
                    <span class="k">if</span> <span class="n">_terminated</span> <span class="ow">or</span> <span class="n">_truncated</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_del_hanging</span><span class="p">(</span><span class="n">agent_id</span><span class="p">)</span>
                        <span class="k">continue</span>
                    <span class="c1"># This must be the agent&#39;s initial observation.</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># Prepend n skip tags to this agent&#39;s mapping + the initial [0].</span>
                        <span class="k">assert</span> <span class="n">agent_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
                            <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">SKIP_ENV_TS_TAG</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_t</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                        <span class="p">)</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="p">[</span>
                            <span class="n">agent_id</span>
                        <span class="p">]</span><span class="o">.</span><span class="n">lookback</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_len_lookback_buffers</span>
                        <span class="c1"># Make `add_env_reset` call and continue with next agent.</span>
                        <span class="n">sa_episode</span><span class="o">.</span><span class="n">add_env_reset</span><span class="p">(</span><span class="n">observation</span><span class="o">=</span><span class="n">_observation</span><span class="p">,</span> <span class="n">infos</span><span class="o">=</span><span class="n">_infos</span><span class="p">)</span>
                        <span class="c1"># Add possible reward to begin cache.</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_rewards_begin</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">+=</span> <span class="n">_reward</span>
                        <span class="c1"># Now that the SAEps is valid, add it to our dict.</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">sa_episode</span>
                        <span class="k">continue</span>

            <span class="c1"># CASE 3: Step is started (by an action), but not completed (no next obs).</span>
            <span class="c1"># ------------------------------------------------------------------------</span>
            <span class="c1"># We have no observation, but we have a hanging action (used when we receive</span>
            <span class="c1"># the next obs for this agent in the future).</span>
            <span class="k">elif</span> <span class="n">agent_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">observations</span> <span class="ow">and</span> <span class="n">agent_id</span> <span class="ow">in</span> <span class="n">actions</span><span class="p">:</span>
                <span class="c1"># Agent got truncated -&gt; Error b/c we would need a last (truncation)</span>
                <span class="c1"># observation for this (otherwise, e.g. bootstrapping would not work).</span>
                <span class="c1"># [previous obs] [action] (hanging) ... ... [truncated]</span>
                <span class="k">if</span> <span class="n">_truncated</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="n">MultiAgentEnvError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Agent </span><span class="si">{</span><span class="n">agent_id</span><span class="si">}</span><span class="s2"> acted and then got truncated, but did NOT &quot;</span>
                        <span class="s2">&quot;receive a last (truncation) observation, required for e.g. &quot;</span>
                        <span class="s2">&quot;value function bootstrapping!&quot;</span>
                    <span class="p">)</span>
                <span class="c1"># Agent got terminated.</span>
                <span class="c1"># [previous obs] [action] (hanging) ... ... [terminated]</span>
                <span class="k">elif</span> <span class="n">_terminated</span><span class="p">:</span>
                    <span class="c1"># If the agent was terminated and no observation is provided,</span>
                    <span class="c1"># duplicate the previous one (this is a technical &quot;fix&quot; to properly</span>
                    <span class="c1"># complete the single agent episode; this last observation is never</span>
                    <span class="c1"># used for learning anyway).</span>
                    <span class="n">_observation</span> <span class="o">=</span> <span class="n">sa_episode</span><span class="o">.</span><span class="n">_last_added_observation</span>
                    <span class="n">_infos</span> <span class="o">=</span> <span class="n">sa_episode</span><span class="o">.</span><span class="n">_last_added_infos</span>
                <span class="c1"># Agent is still alive.</span>
                <span class="c1"># [previous obs] [action] (hanging) ...</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Hanging action, reward, and extra_model_outputs.</span>
                    <span class="k">assert</span> <span class="n">agent_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_actions_end</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_actions_end</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">_action</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_rewards_end</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">_reward</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_extra_model_outputs_end</span><span class="p">[</span>
                        <span class="n">agent_id</span>
                    <span class="p">]</span> <span class="o">=</span> <span class="n">_extra_model_outputs</span>

            <span class="c1"># CASE 4: Step has started in the past and is still ongoing (no observation,</span>
            <span class="c1"># no action).</span>
            <span class="c1"># --------------------------------------------------------------------------</span>
            <span class="c1"># Record reward and terminated/truncated flags.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">_action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_actions_end</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">)</span>

                <span class="c1"># Agent is done.</span>
                <span class="k">if</span> <span class="n">_terminated</span> <span class="ow">or</span> <span class="n">_truncated</span><span class="p">:</span>
                    <span class="c1"># If the agent has NOT stepped, we treat it as not being</span>
                    <span class="c1"># part of this episode.</span>
                    <span class="c1"># ... ... [other agents doing stuff] ... ... [agent done]</span>
                    <span class="k">if</span> <span class="n">_action</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_del_hanging</span><span class="p">(</span><span class="n">agent_id</span><span class="p">)</span>
                        <span class="k">continue</span>

                    <span class="c1"># Agent got truncated -&gt; Error b/c we would need a last (truncation)</span>
                    <span class="c1"># observation for this (otherwise, e.g. bootstrapping would not</span>
                    <span class="c1"># work).</span>
                    <span class="k">if</span> <span class="n">_truncated</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="n">MultiAgentEnvError</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Agent </span><span class="si">{</span><span class="n">agent_id</span><span class="si">}</span><span class="s2"> acted and then got truncated, but did &quot;</span>
                            <span class="s2">&quot;NOT receive a last (truncation) observation, required &quot;</span>
                            <span class="s2">&quot;for e.g. value function bootstrapping!&quot;</span>
                        <span class="p">)</span>

                    <span class="c1"># [obs] ... ... [hanging action] ... ... [done]</span>
                    <span class="c1"># If the agent was terminated and no observation is provided,</span>
                    <span class="c1"># duplicate the previous one (this is a technical &quot;fix&quot; to properly</span>
                    <span class="c1"># complete the single agent episode; this last observation is never</span>
                    <span class="c1"># used for learning anyway).</span>
                    <span class="n">_observation</span> <span class="o">=</span> <span class="n">sa_episode</span><span class="o">.</span><span class="n">_last_added_observation</span>
                    <span class="n">_infos</span> <span class="o">=</span> <span class="n">sa_episode</span><span class="o">.</span><span class="n">_last_added_infos</span>
                    <span class="c1"># `_action` is already `get` above. We don&#39;t need to pop out from</span>
                    <span class="c1"># the cache as it gets wiped out anyway below b/c the agent is</span>
                    <span class="c1"># done.</span>
                    <span class="n">_extra_model_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_extra_model_outputs_end</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span>
                        <span class="n">agent_id</span><span class="p">,</span> <span class="kc">None</span>
                    <span class="p">)</span>
                    <span class="n">_reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_rewards_end</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">agent_id</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span> <span class="o">+</span> <span class="n">_reward</span>
                <span class="c1"># The agent is still alive, just add current reward to cache.</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># But has never stepped in this episode -&gt; add to begin cache.</span>
                    <span class="k">if</span> <span class="n">agent_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_rewards_begin</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">+=</span> <span class="n">_reward</span>
                    <span class="c1"># Otherwise, add to end cache.</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_rewards_end</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">+=</span> <span class="n">_reward</span>

            <span class="c1"># If agent is stepping, add timestep to `SingleAgentEpisode`.</span>
            <span class="k">if</span> <span class="n">_observation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">sa_episode</span><span class="o">.</span><span class="n">add_env_step</span><span class="p">(</span>
                    <span class="n">observation</span><span class="o">=</span><span class="n">_observation</span><span class="p">,</span>
                    <span class="n">action</span><span class="o">=</span><span class="n">_action</span><span class="p">,</span>
                    <span class="n">reward</span><span class="o">=</span><span class="n">_reward</span><span class="p">,</span>
                    <span class="n">infos</span><span class="o">=</span><span class="n">_infos</span><span class="p">,</span>
                    <span class="n">terminated</span><span class="o">=</span><span class="n">_terminated</span><span class="p">,</span>
                    <span class="n">truncated</span><span class="o">=</span><span class="n">_truncated</span><span class="p">,</span>
                    <span class="n">extra_model_outputs</span><span class="o">=</span><span class="n">_extra_model_outputs</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="c1"># Update the env- to agent-step mapping.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="nb">len</span><span class="p">(</span><span class="n">sa_episode</span><span class="p">)</span> <span class="o">+</span> <span class="n">sa_episode</span><span class="o">.</span><span class="n">observations</span><span class="o">.</span><span class="n">lookback</span>
                <span class="p">)</span>

            <span class="c1"># Agent is also done. -&gt; Erase all hanging values for this agent</span>
            <span class="c1"># (they should be empty at this point anyways).</span>
            <span class="k">if</span> <span class="n">_terminated</span> <span class="ow">or</span> <span class="n">_truncated</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_del_hanging</span><span class="p">(</span><span class="n">agent_id</span><span class="p">)</span>

        <span class="c1"># Validate our data.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span>

        <span class="c1"># Step time stats.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_step_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_start_time</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_start_time</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_last_step_time</span></div>


<div class="viewcode-block" id="MultiAgentEpisode.validate">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.validate.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.validate">[docs]</a>
    <span class="k">def</span> <span class="nf">validate</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Validates the episode&#39;s data.</span>

<span class="sd">        This function ensures that the data stored to a `MultiAgentEpisode` is</span>
<span class="sd">        in order (e.g. that the correct number of observations, actions, rewards</span>
<span class="sd">        are there).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">eps</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">eps</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span></div>


        <span class="c1"># TODO (sven): Validate MultiAgentEpisode specifics, like the timestep mappings,</span>
        <span class="c1">#  action/reward caches, etc..</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">is_reset</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns True if `self.add_env_reset()` has already been called.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">any</span><span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">sa_episode</span><span class="o">.</span><span class="n">observations</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">sa_episode</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">is_numpy</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;True, if the data in this episode is already stored as numpy arrays.&quot;&quot;&quot;</span>
        <span class="n">is_numpy</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span><span class="o">.</span><span class="n">is_numpy</span>
        <span class="c1"># Make sure that all single agent&#39;s episodes&#39; `is_numpy` flags are the same.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">eps</span><span class="o">.</span><span class="n">is_numpy</span> <span class="ow">is</span> <span class="n">is_numpy</span> <span class="k">for</span> <span class="n">eps</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Only some SingleAgentEpisode objects in </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2"> are converted to &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;numpy, others are not!&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">is_numpy</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">is_done</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Whether the episode is actually done (terminated or truncated).</span>

<span class="sd">        A done episode cannot be continued via `self.add_env_step()` or being</span>
<span class="sd">        concatenated on its right-side with another episode chunk or being</span>
<span class="sd">        succeeded via `self.cut()`.</span>

<span class="sd">        Note that in a multi-agent environment this does not necessarily</span>
<span class="sd">        correspond to single agents having terminated or being truncated.</span>

<span class="sd">        `self.is_terminated` should be `True`, if all agents are terminated and</span>
<span class="sd">        `self.is_truncated` should be `True`, if all agents are truncated. If</span>
<span class="sd">        only one or more (but not all!) agents are `terminated/truncated the</span>
<span class="sd">        `MultiAgentEpisode.is_terminated/is_truncated` should be `False`. This</span>
<span class="sd">        information about single agent&#39;s terminated/truncated states can always</span>
<span class="sd">        be retrieved from the `SingleAgentEpisode`s inside the &#39;MultiAgentEpisode`</span>
<span class="sd">        one.</span>

<span class="sd">        If all agents are either terminated or truncated, but in a mixed fashion,</span>
<span class="sd">        i.e. some are terminated and others are truncated: This is currently</span>
<span class="sd">        undefined and could potentially be a problem (if a user really implemented</span>
<span class="sd">        such a multi-agent env that behaves this way).</span>

<span class="sd">        Returns:</span>
<span class="sd">            Boolean defining if an episode has either terminated or truncated.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_terminated</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_truncated</span>

<div class="viewcode-block" id="MultiAgentEpisode.to_numpy">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.to_numpy.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.to_numpy">[docs]</a>
    <span class="k">def</span> <span class="nf">to_numpy</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;MultiAgentEpisode&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Converts this Episode&#39;s list attributes to numpy arrays.</span>

<span class="sd">        This means in particular that this episodes&#39; lists (per single agent) of</span>
<span class="sd">        (possibly complex) data (e.g. an agent having a dict obs space) will be</span>
<span class="sd">        converted to (possibly complex) structs, whose leafs are now numpy arrays.</span>
<span class="sd">        Each of these leaf numpy arrays will have the same length (batch dimension)</span>
<span class="sd">        as the length of the original lists.</span>

<span class="sd">        Note that Columns.INFOS are NEVER numpy&#39;ized and will remain a list</span>
<span class="sd">        (normally, a list of the original, env-returned dicts). This is due to the</span>
<span class="sd">        heterogeneous nature of INFOS returned by envs, which would make it unwieldy to</span>
<span class="sd">        convert this information to numpy arrays.</span>

<span class="sd">        After calling this method, no further data may be added to this episode via</span>
<span class="sd">        the `self.add_env_step()` method.</span>

<span class="sd">        Examples:</span>

<span class="sd">        .. testcode::</span>

<span class="sd">            import numpy as np</span>

<span class="sd">            from ray.rllib.env.multi_agent_episode import MultiAgentEpisode</span>
<span class="sd">            from ray.rllib.env.tests.test_multi_agent_episode import (</span>
<span class="sd">                TestMultiAgentEpisode</span>
<span class="sd">            )</span>

<span class="sd">            # Create some multi-agent episode data.</span>
<span class="sd">            (</span>
<span class="sd">                observations,</span>
<span class="sd">                actions,</span>
<span class="sd">                rewards,</span>
<span class="sd">                terminateds,</span>
<span class="sd">                truncateds,</span>
<span class="sd">                infos,</span>
<span class="sd">            ) = TestMultiAgentEpisode._mock_multi_agent_records()</span>
<span class="sd">            # Define the agent ids.</span>
<span class="sd">            agent_ids = [&quot;agent_1&quot;, &quot;agent_2&quot;, &quot;agent_3&quot;, &quot;agent_4&quot;, &quot;agent_5&quot;]</span>

<span class="sd">            episode = MultiAgentEpisode(</span>
<span class="sd">                observations=observations,</span>
<span class="sd">                infos=infos,</span>
<span class="sd">                actions=actions,</span>
<span class="sd">                rewards=rewards,</span>
<span class="sd">                # Note: terminated/truncated have nothing to do with an episode</span>
<span class="sd">                # being converted `to_numpy` or not (via the `self.to_numpy()` method)!</span>
<span class="sd">                terminateds=terminateds,</span>
<span class="sd">                truncateds=truncateds,</span>
<span class="sd">                len_lookback_buffer=0,  # no lookback; all data is actually &quot;in&quot; episode</span>
<span class="sd">            )</span>

<span class="sd">            # Episode has not been numpy&#39;ized yet.</span>
<span class="sd">            assert not episode.is_numpy</span>
<span class="sd">            # We are still operating on lists.</span>
<span class="sd">            assert (</span>
<span class="sd">                episode.get_observations(</span>
<span class="sd">                    indices=[1],</span>
<span class="sd">                    agent_ids=&quot;agent_1&quot;,</span>
<span class="sd">                ) == {&quot;agent_1&quot;: [1]}</span>
<span class="sd">            )</span>

<span class="sd">            # Numpy&#39;ized the episode.</span>
<span class="sd">            episode.to_numpy()</span>
<span class="sd">            assert episode.is_numpy</span>

<span class="sd">            # Everything is now numpy arrays (with 0-axis of size</span>
<span class="sd">            # B=[len of requested slice]).</span>
<span class="sd">            assert (</span>
<span class="sd">                isinstance(episode.get_observations(</span>
<span class="sd">                    indices=[1],</span>
<span class="sd">                    agent_ids=&quot;agent_1&quot;,</span>
<span class="sd">                )[&quot;agent_1&quot;], np.ndarray)</span>
<span class="sd">            )</span>

<span class="sd">        Returns:</span>
<span class="sd">             This `MultiAgentEpisode` object with the converted numpy data.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">for</span> <span class="n">agent_id</span><span class="p">,</span> <span class="n">agent_eps</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">agent_eps</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

        <span class="k">return</span> <span class="bp">self</span></div>


<div class="viewcode-block" id="MultiAgentEpisode.concat_episode">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.concat_episode.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.concat_episode">[docs]</a>
    <span class="k">def</span> <span class="nf">concat_episode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="s2">&quot;MultiAgentEpisode&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Adds the given `other` MultiAgentEpisode to the right side of self.</span>

<span class="sd">        In order for this to work, both chunks (`self` and `other`) must fit</span>
<span class="sd">        together. This is checked by the IDs (must be identical), the time step counters</span>
<span class="sd">        (`self.env_t` must be the same as `episode_chunk.env_t_started`), as well as the</span>
<span class="sd">        observations/infos of the individual agents at the concatenation boundaries.</span>
<span class="sd">        Also, `self.is_done` must not be True, meaning `self.is_terminated` and</span>
<span class="sd">        `self.is_truncated` are both False.</span>

<span class="sd">        Args:</span>
<span class="sd">            other: The other `MultiAgentEpisode` to be concatenated to this one.</span>

<span class="sd">        Returns: A `MultiAgentEpisode` instance containing the concatenated data</span>
<span class="sd">            from both episodes (`self` and `other`).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Make sure the IDs match.</span>
        <span class="k">assert</span> <span class="n">other</span><span class="o">.</span><span class="n">id_</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">id_</span>
        <span class="c1"># NOTE (sven): This is what we agreed on. As the replay buffers must be</span>
        <span class="c1"># able to concatenate.</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_done</span>
        <span class="c1"># Make sure the timesteps match.</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_t</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">env_t_started</span>
        <span class="c1"># Validate `other`.</span>
        <span class="n">other</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span>

        <span class="c1"># Concatenate the individual SingleAgentEpisodes from both chunks.</span>
        <span class="n">all_agent_ids</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">agent_ids</span><span class="p">)</span> <span class="o">|</span> <span class="nb">set</span><span class="p">(</span><span class="n">other</span><span class="o">.</span><span class="n">agent_ids</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">agent_id</span> <span class="ow">in</span> <span class="n">all_agent_ids</span><span class="p">:</span>
            <span class="n">sa_episode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">)</span>

            <span class="c1"># If agent is only in the new episode chunk -&gt; Store all the data of `other`</span>
            <span class="c1"># wrt agent in `self`.</span>
            <span class="k">if</span> <span class="n">sa_episode</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">agent_episodes</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">agent_t_started</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">agent_t_started</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_copy_hanging</span><span class="p">(</span><span class="n">agent_id</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

            <span class="c1"># If the agent was done in `self`, ignore and continue. There should not be</span>
            <span class="c1"># any data of that agent in `other`.</span>
            <span class="k">elif</span> <span class="n">sa_episode</span><span class="o">.</span><span class="n">is_done</span><span class="p">:</span>
                <span class="k">continue</span>

            <span class="c1"># If the agent has data in both chunks, concatenate on the single-agent</span>
            <span class="c1"># level, thereby making sure the hanging values (begin and end) match.</span>
            <span class="k">elif</span> <span class="n">agent_id</span> <span class="ow">in</span> <span class="n">other</span><span class="o">.</span><span class="n">agent_episodes</span><span class="p">:</span>
                <span class="c1"># If `other` has hanging (end) values -&gt; Add these to `self`&#39;s agent</span>
                <span class="c1"># SingleAgentEpisode (as a new timestep) and only then concatenate.</span>
                <span class="c1"># Otherwise, the concatentaion would fail b/c of missing data.</span>
                <span class="k">if</span> <span class="n">agent_id</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_actions_end</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="n">agent_id</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_extra_model_outputs_end</span>
                    <span class="n">sa_episode</span><span class="o">.</span><span class="n">add_env_step</span><span class="p">(</span>
                        <span class="n">observation</span><span class="o">=</span><span class="n">other</span><span class="o">.</span><span class="n">agent_episodes</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span><span class="o">.</span><span class="n">get_observations</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
                        <span class="n">infos</span><span class="o">=</span><span class="n">other</span><span class="o">.</span><span class="n">agent_episodes</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span><span class="o">.</span><span class="n">get_infos</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
                        <span class="n">action</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_hanging_actions_end</span><span class="p">[</span><span class="n">agent_id</span><span class="p">],</span>
                        <span class="n">reward</span><span class="o">=</span><span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_rewards_end</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span>
                            <span class="o">+</span> <span class="n">other</span><span class="o">.</span><span class="n">_hanging_rewards_begin</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span>
                        <span class="p">),</span>
                        <span class="n">extra_model_outputs</span><span class="o">=</span><span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_extra_model_outputs_end</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span>
                        <span class="p">),</span>
                    <span class="p">)</span>
                <span class="n">sa_episode</span><span class="o">.</span><span class="n">concat_episode</span><span class="p">(</span><span class="n">other</span><span class="o">.</span><span class="n">agent_episodes</span><span class="p">[</span><span class="n">agent_id</span><span class="p">])</span>
                <span class="c1"># Override `self`&#39;s hanging (end) values with `other`&#39;s hanging (end).</span>
                <span class="k">if</span> <span class="n">agent_id</span> <span class="ow">in</span> <span class="n">other</span><span class="o">.</span><span class="n">_hanging_actions_end</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_actions_end</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span>
                        <span class="n">other</span><span class="o">.</span><span class="n">_hanging_actions_end</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span>
                    <span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_rewards_end</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">_hanging_rewards_end</span><span class="p">[</span>
                        <span class="n">agent_id</span>
                    <span class="p">]</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_extra_model_outputs_end</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span>
                        <span class="n">other</span><span class="o">.</span><span class="n">_hanging_extra_model_outputs_end</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span>
                    <span class="p">)</span>

                <span class="c1"># Concatenate the env- to agent-timestep mappings.</span>
                <span class="n">j</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_t</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">other</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="p">[</span><span class="n">agent_id</span><span class="p">][</span><span class="mi">1</span><span class="p">:]):</span>
                    <span class="k">if</span> <span class="n">val</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">SKIP_ENV_TS_TAG</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">SKIP_ENV_TS_TAG</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">j</span><span class="p">)</span>

            <span class="c1"># Otherwise, the agent is only in `self` and not done. All data is stored</span>
            <span class="c1"># already -&gt; skip</span>
            <span class="c1"># else: pass</span>

        <span class="c1"># Update all timestep counters.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env_t</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">env_t</span>
        <span class="c1"># Check, if the episode is terminated or truncated.</span>
        <span class="k">if</span> <span class="n">other</span><span class="o">.</span><span class="n">is_terminated</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_terminated</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">elif</span> <span class="n">other</span><span class="o">.</span><span class="n">is_truncated</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_truncated</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># Erase all temporary timestep data caches.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_temporary_timestep_data</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>

        <span class="c1"># Validate.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span></div>


<div class="viewcode-block" id="MultiAgentEpisode.cut">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.cut.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.cut">[docs]</a>
    <span class="k">def</span> <span class="nf">cut</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">len_lookback_buffer</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;MultiAgentEpisode&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns a successor episode chunk (of len=0) continuing from this Episode.</span>

<span class="sd">        The successor will have the same ID as `self`.</span>
<span class="sd">        If no lookback buffer is requested (len_lookback_buffer=0), the successor&#39;s</span>
<span class="sd">        observations will be the last observation(s) of `self` and its length will</span>
<span class="sd">        therefore be 0 (no further steps taken yet). If `len_lookback_buffer` &gt; 0,</span>
<span class="sd">        the returned successor will have `len_lookback_buffer` observations (and</span>
<span class="sd">        actions, rewards, etc..) taken from the right side (end) of `self`. For example</span>
<span class="sd">        if `len_lookback_buffer=2`, the returned successor&#39;s lookback buffer actions</span>
<span class="sd">        will be identical to teh results of `self.get_actions([-2, -1])`.</span>

<span class="sd">        This method is useful if you would like to discontinue building an episode</span>
<span class="sd">        chunk (b/c you have to return it from somewhere), but would like to have a new</span>
<span class="sd">        episode instance to continue building the actual gym.Env episode at a later</span>
<span class="sd">        time. Vie the `len_lookback_buffer` argument, the continuing chunk (successor)</span>
<span class="sd">        will still be able to &quot;look back&quot; into this predecessor episode&#39;s data (at</span>
<span class="sd">        least to some extend, depending on the value of `len_lookback_buffer`).</span>

<span class="sd">        Args:</span>
<span class="sd">            len_lookback_buffer: The number of environment timesteps to take along into</span>
<span class="sd">                the new chunk as &quot;lookback buffer&quot;. A lookback buffer is additional data</span>
<span class="sd">                on the left side of the actual episode data for visibility purposes</span>
<span class="sd">                (but without actually being part of the new chunk). For example, if</span>
<span class="sd">                `self` ends in actions: agent_1=5,6,7 and agent_2=6,7, and we call</span>
<span class="sd">                `self.cut(len_lookback_buffer=2)`, the returned chunk will have</span>
<span class="sd">                actions 6 and 7 for both agents already in it, but still</span>
<span class="sd">                `t_started`==t==8 (not 7!) and a length of 0. If there is not enough</span>
<span class="sd">                data in `self` yet to fulfil the `len_lookback_buffer` request, the</span>
<span class="sd">                value of `len_lookback_buffer` is automatically adjusted (lowered).</span>

<span class="sd">        Returns:</span>
<span class="sd">            The successor Episode chunk of this one with the same ID and state and the</span>
<span class="sd">            only observation being the last observation in self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">len_lookback_buffer</span> <span class="o">&gt;=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_done</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Can&#39;t call `MultiAgentEpisode.cut()` when the episode is already done!&quot;</span>
            <span class="p">)</span>

        <span class="c1"># If there is hanging data (e.g. actions) in the agents&#39; caches, we might have</span>
        <span class="c1"># to re-adjust the lookback len further into the past to make sure that these</span>
        <span class="c1"># agents have at least one observation to look back to. Otherwise, the timestep</span>
        <span class="c1"># that got cut into will be &quot;lost&quot; for learning from it.</span>
        <span class="n">orig_len_lb</span> <span class="o">=</span> <span class="n">len_lookback_buffer</span>
        <span class="k">for</span> <span class="n">agent_id</span><span class="p">,</span> <span class="n">agent_actions</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_actions_end</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">SKIP_ENV_TS_TAG</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">orig_len_lb</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="o">-</span><span class="n">i</span><span class="p">)</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">SKIP_ENV_TS_TAG</span><span class="p">:</span>
                    <span class="n">len_lookback_buffer</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">len_lookback_buffer</span><span class="p">,</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
                    <span class="k">break</span>

        <span class="c1"># Initialize this episode chunk with the most recent observations</span>
        <span class="c1"># and infos (even if lookback is zero). Similar to an initial `env.reset()`</span>
        <span class="n">indices_obs_and_infos</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="o">-</span><span class="n">len_lookback_buffer</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">indices_rest</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">slice</span><span class="p">(</span><span class="o">-</span><span class="n">len_lookback_buffer</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">len_lookback_buffer</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="k">else</span> <span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># -&gt; empty slice</span>
        <span class="p">)</span>

        <span class="n">observations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_observations</span><span class="p">(</span>
            <span class="n">indices</span><span class="o">=</span><span class="n">indices_obs_and_infos</span><span class="p">,</span> <span class="n">return_list</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="n">infos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_infos</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">indices_obs_and_infos</span><span class="p">,</span> <span class="n">return_list</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_actions</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">indices_rest</span><span class="p">,</span> <span class="n">return_list</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">rewards</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_rewards</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">indices_rest</span><span class="p">,</span> <span class="n">return_list</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">extra_model_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_extra_model_outputs</span><span class="p">(</span>
            <span class="n">key</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># all keys</span>
            <span class="n">indices</span><span class="o">=</span><span class="n">indices_rest</span><span class="p">,</span>
            <span class="n">return_list</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">successor</span> <span class="o">=</span> <span class="n">MultiAgentEpisode</span><span class="p">(</span>
            <span class="c1"># Same ID.</span>
            <span class="n">id_</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">id_</span><span class="p">,</span>
            <span class="n">observations</span><span class="o">=</span><span class="n">observations</span><span class="p">,</span>
            <span class="n">observation_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span>
            <span class="n">infos</span><span class="o">=</span><span class="n">infos</span><span class="p">,</span>
            <span class="n">actions</span><span class="o">=</span><span class="n">actions</span><span class="p">,</span>
            <span class="n">action_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span>
            <span class="n">rewards</span><span class="o">=</span><span class="n">rewards</span><span class="p">,</span>
            <span class="c1"># List of MADicts, mapping agent IDs to their respective extra model output</span>
            <span class="c1"># dicts.</span>
            <span class="n">extra_model_outputs</span><span class="o">=</span><span class="n">extra_model_outputs</span><span class="p">,</span>
            <span class="n">terminateds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">get_terminateds</span><span class="p">(),</span>
            <span class="n">truncateds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">get_truncateds</span><span class="p">(),</span>
            <span class="c1"># Continue with `self`&#39;s current timesteps.</span>
            <span class="n">env_t_started</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_t</span><span class="p">,</span>
            <span class="n">agent_t_started</span><span class="o">=</span><span class="p">{</span>
                <span class="n">aid</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="p">[</span><span class="n">aid</span><span class="p">]</span><span class="o">.</span><span class="n">t</span>
                <span class="k">for</span> <span class="n">aid</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_ids</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="p">[</span><span class="n">aid</span><span class="p">]</span><span class="o">.</span><span class="n">is_done</span>
            <span class="p">},</span>
            <span class="c1"># Same AgentIDs and SingleAgentEpisode IDs.</span>
            <span class="n">agent_episode_ids</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">agent_episode_ids</span><span class="p">,</span>
            <span class="n">agent_module_ids</span><span class="o">=</span><span class="p">{</span>
                <span class="n">aid</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="p">[</span><span class="n">aid</span><span class="p">]</span><span class="o">.</span><span class="n">module_id</span> <span class="k">for</span> <span class="n">aid</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_ids</span>
            <span class="p">},</span>
            <span class="n">agent_to_module_mapping_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">agent_to_module_mapping_fn</span><span class="p">,</span>
            <span class="c1"># All data we provided to the c&#39;tor goes into the lookback buffer.</span>
            <span class="n">len_lookback_buffer</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Copy over the hanging (end) values into the hanging (begin) chaches of the</span>
        <span class="c1"># successor.</span>
        <span class="n">successor</span><span class="o">.</span><span class="n">_hanging_rewards_begin</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_rewards_end</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">successor</span></div>


    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">agent_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Set</span><span class="p">[</span><span class="n">AgentID</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the agent ids.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">agent_episode_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MultiAgentDict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns ids from each agent&#39;s `SingleAgentEpisode`.&quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="n">agent_id</span><span class="p">:</span> <span class="n">agent_eps</span><span class="o">.</span><span class="n">id_</span>
            <span class="k">for</span> <span class="n">agent_id</span><span class="p">,</span> <span class="n">agent_eps</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">}</span>

<div class="viewcode-block" id="MultiAgentEpisode.module_for">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.module_for.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.module_for">[docs]</a>
    <span class="k">def</span> <span class="nf">module_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">agent_id</span><span class="p">:</span> <span class="n">AgentID</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ModuleID</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the ModuleID for a given AgentID.</span>

<span class="sd">        Forces the agent-to-module mapping to be performed (via</span>
<span class="sd">        `self.agent_to_module_mapping_fn`), if this has not been done yet.</span>
<span class="sd">        Note that all such mappings are stored in the `self._agent_to_module_mapping`</span>
<span class="sd">        property.</span>

<span class="sd">        Args:</span>
<span class="sd">            agent_id: The AgentID to get a mapped ModuleID for.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The ModuleID mapped to from the given `agent_id`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">agent_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_agent_to_module_mapping</span><span class="p">:</span>
            <span class="n">module_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_agent_to_module_mapping</span><span class="p">[</span>
                <span class="n">agent_id</span>
            <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_to_module_mapping_fn</span><span class="p">(</span><span class="n">agent_id</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">module_id</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_agent_to_module_mapping</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span></div>


<div class="viewcode-block" id="MultiAgentEpisode.get_observations">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_observations.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_observations">[docs]</a>
    <span class="k">def</span> <span class="nf">get_observations</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">slice</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">agent_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Collection</span><span class="p">[</span><span class="n">AgentID</span><span class="p">],</span> <span class="n">AgentID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">env_steps</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="c1"># global_indices: bool = False,</span>
        <span class="n">neg_index_as_lookback</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">fill</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">one_hot_discrete</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_list</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns agents&#39; observations or batched ranges thereof from this episode.</span>

<span class="sd">        Args:</span>
<span class="sd">            indices: A single int is interpreted as an index, from which to return the</span>
<span class="sd">                individual observation stored at this index.</span>
<span class="sd">                A list of ints is interpreted as a list of indices from which to gather</span>
<span class="sd">                individual observations in a batch of size len(indices).</span>
<span class="sd">                A slice object is interpreted as a range of observations to be returned.</span>
<span class="sd">                Thereby, negative indices by default are interpreted as &quot;before the end&quot;</span>
<span class="sd">                unless the `neg_index_as_lookback=True` option is used, in which case</span>
<span class="sd">                negative indices are interpreted as &quot;before ts=0&quot;, meaning going back</span>
<span class="sd">                into the lookback buffer.</span>
<span class="sd">                If None, will return all observations (from ts=0 to the end).</span>
<span class="sd">            agent_ids: An optional collection of AgentIDs or a single AgentID to get</span>
<span class="sd">                observations for. If None, will return observations for all agents in</span>
<span class="sd">                this episode.</span>
<span class="sd">            env_steps: Whether `indices` should be interpreted as environment time steps</span>
<span class="sd">                (True) or per-agent timesteps (False).</span>
<span class="sd">            neg_index_as_lookback: If True, negative values in `indices` are</span>
<span class="sd">                interpreted as &quot;before ts=0&quot;, meaning going back into the lookback</span>
<span class="sd">                buffer. For example, an episode with agent A&#39;s observations</span>
<span class="sd">                [4, 5, 6,  7, 8, 9], where [4, 5, 6] is the lookback buffer range</span>
<span class="sd">                (ts=0 item is 7), will respond to `get_observations(-1, agent_ids=[A],</span>
<span class="sd">                neg_index_as_lookback=True)` with {A: `6`} and to</span>
<span class="sd">                `get_observations(slice(-2, 1), agent_ids=[A],</span>
<span class="sd">                neg_index_as_lookback=True)` with {A: `[5, 6,  7]`}.</span>
<span class="sd">            fill: An optional value to use for filling up the returned results at</span>
<span class="sd">                the boundaries. This filling only happens if the requested index range&#39;s</span>
<span class="sd">                start/stop boundaries exceed the episode&#39;s boundaries (including the</span>
<span class="sd">                lookback buffer on the left side). This comes in very handy, if users</span>
<span class="sd">                don&#39;t want to worry about reaching such boundaries and want to zero-pad.</span>
<span class="sd">                For example, an episode with agent A&#39; observations [10, 11,  12, 13, 14]</span>
<span class="sd">                and lookback buffer size of 2 (meaning observations `10` and `11` are</span>
<span class="sd">                part of the lookback buffer) will respond to</span>
<span class="sd">                `get_observations(slice(-7, -2), agent_ids=[A], fill=0.0)` with</span>
<span class="sd">                `{A: [0.0, 0.0, 10, 11, 12]}`.</span>
<span class="sd">            one_hot_discrete: If True, will return one-hot vectors (instead of</span>
<span class="sd">                int-values) for those sub-components of a (possibly complex) observation</span>
<span class="sd">                space that are Discrete or MultiDiscrete.  Note that if `fill=0` and the</span>
<span class="sd">                requested `indices` are out of the range of our data, the returned</span>
<span class="sd">                one-hot vectors will actually be zero-hot (all slots zero).</span>
<span class="sd">            return_list: Whether to return a list of multi-agent dicts (instead of</span>
<span class="sd">                a single multi-agent dict of lists/structs). False by default. This</span>
<span class="sd">                option can only be used when `env_steps` is True due to the fact the</span>
<span class="sd">                such a list can only be interpreted as one env step per list item</span>
<span class="sd">                (would not work with agent steps).</span>

<span class="sd">        Returns:</span>
<span class="sd">            A dictionary mapping agent IDs to observations (at the given</span>
<span class="sd">            `indices`). If `env_steps` is True, only agents that have stepped</span>
<span class="sd">            (were ready) at the given env step `indices` are returned (i.e. not all</span>
<span class="sd">            agent IDs are necessarily in the keys).</span>
<span class="sd">            If `return_list` is True, returns a list of MultiAgentDicts (mapping agent</span>
<span class="sd">            IDs to observations) instead.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get</span><span class="p">(</span>
            <span class="n">what</span><span class="o">=</span><span class="s2">&quot;observations&quot;</span><span class="p">,</span>
            <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span>
            <span class="n">agent_ids</span><span class="o">=</span><span class="n">agent_ids</span><span class="p">,</span>
            <span class="n">env_steps</span><span class="o">=</span><span class="n">env_steps</span><span class="p">,</span>
            <span class="n">neg_index_as_lookback</span><span class="o">=</span><span class="n">neg_index_as_lookback</span><span class="p">,</span>
            <span class="n">fill</span><span class="o">=</span><span class="n">fill</span><span class="p">,</span>
            <span class="n">one_hot_discrete</span><span class="o">=</span><span class="n">one_hot_discrete</span><span class="p">,</span>
            <span class="n">return_list</span><span class="o">=</span><span class="n">return_list</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="MultiAgentEpisode.get_infos">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_infos.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_infos">[docs]</a>
    <span class="k">def</span> <span class="nf">get_infos</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">slice</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">agent_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Collection</span><span class="p">[</span><span class="n">AgentID</span><span class="p">],</span> <span class="n">AgentID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">env_steps</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">neg_index_as_lookback</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">fill</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_list</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns agents&#39; info dicts or list (ranges) thereof from this episode.</span>

<span class="sd">        Args:</span>
<span class="sd">            indices: A single int is interpreted as an index, from which to return the</span>
<span class="sd">                individual info dict stored at this index.</span>
<span class="sd">                A list of ints is interpreted as a list of indices from which to gather</span>
<span class="sd">                individual info dicts in a list of size len(indices).</span>
<span class="sd">                A slice object is interpreted as a range of info dicts to be returned.</span>
<span class="sd">                Thereby, negative indices by default are interpreted as &quot;before the end&quot;</span>
<span class="sd">                unless the `neg_index_as_lookback=True` option is used, in which case</span>
<span class="sd">                negative indices are interpreted as &quot;before ts=0&quot;, meaning going back</span>
<span class="sd">                into the lookback buffer.</span>
<span class="sd">                If None, will return all infos (from ts=0 to the end).</span>
<span class="sd">            agent_ids: An optional collection of AgentIDs or a single AgentID to get</span>
<span class="sd">                info dicts for. If None, will return info dicts for all agents in</span>
<span class="sd">                this episode.</span>
<span class="sd">            env_steps: Whether `indices` should be interpreted as environment time steps</span>
<span class="sd">                (True) or per-agent timesteps (False).</span>
<span class="sd">            neg_index_as_lookback: If True, negative values in `indices` are</span>
<span class="sd">                interpreted as &quot;before ts=0&quot;, meaning going back into the lookback</span>
<span class="sd">                buffer. For example, an episode with agent A&#39;s info dicts</span>
<span class="sd">                [{&quot;l&quot;:4}, {&quot;l&quot;:5}, {&quot;l&quot;:6},  {&quot;a&quot;:7}, {&quot;b&quot;:8}, {&quot;c&quot;:9}], where the</span>
<span class="sd">                first 3 items are the lookback buffer (ts=0 item is {&quot;a&quot;: 7}), will</span>
<span class="sd">                respond to `get_infos(-1, agent_ids=A, neg_index_as_lookback=True)`</span>
<span class="sd">                with `{A: {&quot;l&quot;:6}}` and to</span>
<span class="sd">                `get_infos(slice(-2, 1), agent_ids=A, neg_index_as_lookback=True)`</span>
<span class="sd">                with `{A: [{&quot;l&quot;:5}, {&quot;l&quot;:6},  {&quot;a&quot;:7}]}`.</span>
<span class="sd">            fill: An optional value to use for filling up the returned results at</span>
<span class="sd">                the boundaries. This filling only happens if the requested index range&#39;s</span>
<span class="sd">                start/stop boundaries exceed the episode&#39;s boundaries (including the</span>
<span class="sd">                lookback buffer on the left side). This comes in very handy, if users</span>
<span class="sd">                don&#39;t want to worry about reaching such boundaries and want to</span>
<span class="sd">                auto-fill. For example, an episode with agent A&#39;s infos being</span>
<span class="sd">                [{&quot;l&quot;:10}, {&quot;l&quot;:11},  {&quot;a&quot;:12}, {&quot;b&quot;:13}, {&quot;c&quot;:14}] and lookback buffer</span>
<span class="sd">                size of 2 (meaning infos {&quot;l&quot;:10}, {&quot;l&quot;:11} are part of the lookback</span>
<span class="sd">                buffer) will respond to `get_infos(slice(-7, -2), agent_ids=A,</span>
<span class="sd">                fill={&quot;o&quot;: 0.0})` with</span>
<span class="sd">                `{A: [{&quot;o&quot;:0.0}, {&quot;o&quot;:0.0}, {&quot;l&quot;:10}, {&quot;l&quot;:11}, {&quot;a&quot;:12}]}`.</span>
<span class="sd">            return_list: Whether to return a list of multi-agent dicts (instead of</span>
<span class="sd">                a single multi-agent dict of lists/structs). False by default. This</span>
<span class="sd">                option can only be used when `env_steps` is True due to the fact the</span>
<span class="sd">                such a list can only be interpreted as one env step per list item</span>
<span class="sd">                (would not work with agent steps).</span>

<span class="sd">        Returns:</span>
<span class="sd">            A dictionary mapping agent IDs to observations (at the given</span>
<span class="sd">            `indices`). If `env_steps` is True, only agents that have stepped</span>
<span class="sd">            (were ready) at the given env step `indices` are returned (i.e. not all</span>
<span class="sd">            agent IDs are necessarily in the keys).</span>
<span class="sd">            If `return_list` is True, returns a list of MultiAgentDicts (mapping agent</span>
<span class="sd">            IDs to infos) instead.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get</span><span class="p">(</span>
            <span class="n">what</span><span class="o">=</span><span class="s2">&quot;infos&quot;</span><span class="p">,</span>
            <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span>
            <span class="n">agent_ids</span><span class="o">=</span><span class="n">agent_ids</span><span class="p">,</span>
            <span class="n">env_steps</span><span class="o">=</span><span class="n">env_steps</span><span class="p">,</span>
            <span class="n">neg_index_as_lookback</span><span class="o">=</span><span class="n">neg_index_as_lookback</span><span class="p">,</span>
            <span class="n">fill</span><span class="o">=</span><span class="n">fill</span><span class="p">,</span>
            <span class="n">return_list</span><span class="o">=</span><span class="n">return_list</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="MultiAgentEpisode.get_actions">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_actions.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_actions">[docs]</a>
    <span class="k">def</span> <span class="nf">get_actions</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">slice</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">agent_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Collection</span><span class="p">[</span><span class="n">AgentID</span><span class="p">],</span> <span class="n">AgentID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">env_steps</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">neg_index_as_lookback</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">fill</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">one_hot_discrete</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_list</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns agents&#39; actions or batched ranges thereof from this episode.</span>

<span class="sd">        Args:</span>
<span class="sd">            indices: A single int is interpreted as an index, from which to return the</span>
<span class="sd">                individual actions stored at this index.</span>
<span class="sd">                A list of ints is interpreted as a list of indices from which to gather</span>
<span class="sd">                individual actions in a batch of size len(indices).</span>
<span class="sd">                A slice object is interpreted as a range of actions to be returned.</span>
<span class="sd">                Thereby, negative indices by default are interpreted as &quot;before the end&quot;</span>
<span class="sd">                unless the `neg_index_as_lookback=True` option is used, in which case</span>
<span class="sd">                negative indices are interpreted as &quot;before ts=0&quot;, meaning going back</span>
<span class="sd">                into the lookback buffer.</span>
<span class="sd">                If None, will return all actions (from ts=0 to the end).</span>
<span class="sd">            agent_ids: An optional collection of AgentIDs or a single AgentID to get</span>
<span class="sd">                actions for. If None, will return actions for all agents in</span>
<span class="sd">                this episode.</span>
<span class="sd">            env_steps: Whether `indices` should be interpreted as environment time steps</span>
<span class="sd">                (True) or per-agent timesteps (False).</span>
<span class="sd">            neg_index_as_lookback: If True, negative values in `indices` are</span>
<span class="sd">                interpreted as &quot;before ts=0&quot;, meaning going back into the lookback</span>
<span class="sd">                buffer. For example, an episode with agent A&#39;s actions</span>
<span class="sd">                [4, 5, 6,  7, 8, 9], where [4, 5, 6] is the lookback buffer range</span>
<span class="sd">                (ts=0 item is 7), will respond to `get_actions(-1, agent_ids=[A],</span>
<span class="sd">                neg_index_as_lookback=True)` with {A: `6`} and to</span>
<span class="sd">                `get_actions(slice(-2, 1), agent_ids=[A],</span>
<span class="sd">                neg_index_as_lookback=True)` with {A: `[5, 6,  7]`}.</span>
<span class="sd">            fill: An optional value to use for filling up the returned results at</span>
<span class="sd">                the boundaries. This filling only happens if the requested index range&#39;s</span>
<span class="sd">                start/stop boundaries exceed the episode&#39;s boundaries (including the</span>
<span class="sd">                lookback buffer on the left side). This comes in very handy, if users</span>
<span class="sd">                don&#39;t want to worry about reaching such boundaries and want to zero-pad.</span>
<span class="sd">                For example, an episode with agent A&#39; actions [10, 11,  12, 13, 14]</span>
<span class="sd">                and lookback buffer size of 2 (meaning actions `10` and `11` are</span>
<span class="sd">                part of the lookback buffer) will respond to</span>
<span class="sd">                `get_actions(slice(-7, -2), agent_ids=[A], fill=0.0)` with</span>
<span class="sd">                `{A: [0.0, 0.0, 10, 11, 12]}`.</span>
<span class="sd">            one_hot_discrete: If True, will return one-hot vectors (instead of</span>
<span class="sd">                int-values) for those sub-components of a (possibly complex) observation</span>
<span class="sd">                space that are Discrete or MultiDiscrete.  Note that if `fill=0` and the</span>
<span class="sd">                requested `indices` are out of the range of our data, the returned</span>
<span class="sd">                one-hot vectors will actually be zero-hot (all slots zero).</span>
<span class="sd">            return_list: Whether to return a list of multi-agent dicts (instead of</span>
<span class="sd">                a single multi-agent dict of lists/structs). False by default. This</span>
<span class="sd">                option can only be used when `env_steps` is True due to the fact the</span>
<span class="sd">                such a list can only be interpreted as one env step per list item</span>
<span class="sd">                (would not work with agent steps).</span>

<span class="sd">        Returns:</span>
<span class="sd">            A dictionary mapping agent IDs to actions (at the given</span>
<span class="sd">            `indices`). If `env_steps` is True, only agents that have stepped</span>
<span class="sd">            (were ready) at the given env step `indices` are returned (i.e. not all</span>
<span class="sd">            agent IDs are necessarily in the keys).</span>
<span class="sd">            If `return_list` is True, returns a list of MultiAgentDicts (mapping agent</span>
<span class="sd">            IDs to actions) instead.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get</span><span class="p">(</span>
            <span class="n">what</span><span class="o">=</span><span class="s2">&quot;actions&quot;</span><span class="p">,</span>
            <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span>
            <span class="n">agent_ids</span><span class="o">=</span><span class="n">agent_ids</span><span class="p">,</span>
            <span class="n">env_steps</span><span class="o">=</span><span class="n">env_steps</span><span class="p">,</span>
            <span class="n">neg_index_as_lookback</span><span class="o">=</span><span class="n">neg_index_as_lookback</span><span class="p">,</span>
            <span class="n">fill</span><span class="o">=</span><span class="n">fill</span><span class="p">,</span>
            <span class="n">one_hot_discrete</span><span class="o">=</span><span class="n">one_hot_discrete</span><span class="p">,</span>
            <span class="n">return_list</span><span class="o">=</span><span class="n">return_list</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="MultiAgentEpisode.get_rewards">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_rewards.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_rewards">[docs]</a>
    <span class="k">def</span> <span class="nf">get_rewards</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">slice</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">agent_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Collection</span><span class="p">[</span><span class="n">AgentID</span><span class="p">],</span> <span class="n">AgentID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">env_steps</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">neg_index_as_lookback</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">fill</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_list</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns agents&#39; rewards or batched ranges thereof from this episode.</span>

<span class="sd">        Args:</span>
<span class="sd">            indices: A single int is interpreted as an index, from which to return the</span>
<span class="sd">                individual rewards stored at this index.</span>
<span class="sd">                A list of ints is interpreted as a list of indices from which to gather</span>
<span class="sd">                individual rewards in a batch of size len(indices).</span>
<span class="sd">                A slice object is interpreted as a range of rewards to be returned.</span>
<span class="sd">                Thereby, negative indices by default are interpreted as &quot;before the end&quot;</span>
<span class="sd">                unless the `neg_index_as_lookback=True` option is used, in which case</span>
<span class="sd">                negative indices are interpreted as &quot;before ts=0&quot;, meaning going back</span>
<span class="sd">                into the lookback buffer.</span>
<span class="sd">                If None, will return all rewards (from ts=0 to the end).</span>
<span class="sd">            agent_ids: An optional collection of AgentIDs or a single AgentID to get</span>
<span class="sd">                rewards for. If None, will return rewards for all agents in</span>
<span class="sd">                this episode.</span>
<span class="sd">            env_steps: Whether `indices` should be interpreted as environment time steps</span>
<span class="sd">                (True) or per-agent timesteps (False).</span>
<span class="sd">            neg_index_as_lookback: If True, negative values in `indices` are</span>
<span class="sd">                interpreted as &quot;before ts=0&quot;, meaning going back into the lookback</span>
<span class="sd">                buffer. For example, an episode with agent A&#39;s rewards</span>
<span class="sd">                [4, 5, 6,  7, 8, 9], where [4, 5, 6] is the lookback buffer range</span>
<span class="sd">                (ts=0 item is 7), will respond to `get_rewards(-1, agent_ids=[A],</span>
<span class="sd">                neg_index_as_lookback=True)` with {A: `6`} and to</span>
<span class="sd">                `get_rewards(slice(-2, 1), agent_ids=[A],</span>
<span class="sd">                neg_index_as_lookback=True)` with {A: `[5, 6,  7]`}.</span>
<span class="sd">            fill: An optional float value to use for filling up the returned results at</span>
<span class="sd">                the boundaries. This filling only happens if the requested index range&#39;s</span>
<span class="sd">                start/stop boundaries exceed the episode&#39;s boundaries (including the</span>
<span class="sd">                lookback buffer on the left side). This comes in very handy, if users</span>
<span class="sd">                don&#39;t want to worry about reaching such boundaries and want to zero-pad.</span>
<span class="sd">                For example, an episode with agent A&#39; rewards [10, 11,  12, 13, 14]</span>
<span class="sd">                and lookback buffer size of 2 (meaning rewards `10` and `11` are</span>
<span class="sd">                part of the lookback buffer) will respond to</span>
<span class="sd">                `get_rewards(slice(-7, -2), agent_ids=[A], fill=0.0)` with</span>
<span class="sd">                `{A: [0.0, 0.0, 10, 11, 12]}`.</span>
<span class="sd">            return_list: Whether to return a list of multi-agent dicts (instead of</span>
<span class="sd">                a single multi-agent dict of lists/structs). False by default. This</span>
<span class="sd">                option can only be used when `env_steps` is True due to the fact the</span>
<span class="sd">                such a list can only be interpreted as one env step per list item</span>
<span class="sd">                (would not work with agent steps).</span>

<span class="sd">        Returns:</span>
<span class="sd">            A dictionary mapping agent IDs to rewards (at the given</span>
<span class="sd">            `indices`). If `env_steps` is True, only agents that have stepped</span>
<span class="sd">            (were ready) at the given env step `indices` are returned (i.e. not all</span>
<span class="sd">            agent IDs are necessarily in the keys).</span>
<span class="sd">            If `return_list` is True, returns a list of MultiAgentDicts (mapping agent</span>
<span class="sd">            IDs to rewards) instead.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get</span><span class="p">(</span>
            <span class="n">what</span><span class="o">=</span><span class="s2">&quot;rewards&quot;</span><span class="p">,</span>
            <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span>
            <span class="n">agent_ids</span><span class="o">=</span><span class="n">agent_ids</span><span class="p">,</span>
            <span class="n">env_steps</span><span class="o">=</span><span class="n">env_steps</span><span class="p">,</span>
            <span class="n">neg_index_as_lookback</span><span class="o">=</span><span class="n">neg_index_as_lookback</span><span class="p">,</span>
            <span class="n">fill</span><span class="o">=</span><span class="n">fill</span><span class="p">,</span>
            <span class="n">return_list</span><span class="o">=</span><span class="n">return_list</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="MultiAgentEpisode.get_extra_model_outputs">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_extra_model_outputs.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_extra_model_outputs">[docs]</a>
    <span class="k">def</span> <span class="nf">get_extra_model_outputs</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">key</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">slice</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">agent_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Collection</span><span class="p">[</span><span class="n">AgentID</span><span class="p">],</span> <span class="n">AgentID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">env_steps</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">neg_index_as_lookback</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">fill</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_list</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns agents&#39; actions or batched ranges thereof from this episode.</span>

<span class="sd">        Args:</span>
<span class="sd">            key: The `key` within each agents&#39; extra_model_outputs dict to extract</span>
<span class="sd">                data for. If None, return data of all extra model output keys.</span>
<span class="sd">            indices: A single int is interpreted as an index, from which to return the</span>
<span class="sd">                individual extra model outputs stored at this index.</span>
<span class="sd">                A list of ints is interpreted as a list of indices from which to gather</span>
<span class="sd">                individual extra model outputs in a batch of size len(indices).</span>
<span class="sd">                A slice object is interpreted as a range of extra model outputs to be</span>
<span class="sd">                returned.</span>
<span class="sd">                Thereby, negative indices by default are interpreted as &quot;before the end&quot;</span>
<span class="sd">                unless the `neg_index_as_lookback=True` option is used, in which case</span>
<span class="sd">                negative indices are interpreted as &quot;before ts=0&quot;, meaning going back</span>
<span class="sd">                into the lookback buffer.</span>
<span class="sd">                If None, will return all extra model outputs (from ts=0 to the end).</span>
<span class="sd">            agent_ids: An optional collection of AgentIDs or a single AgentID to get</span>
<span class="sd">                extra model outputs for. If None, will return extra model outputs for</span>
<span class="sd">                all agents in this episode.</span>
<span class="sd">            env_steps: Whether `indices` should be interpreted as environment time steps</span>
<span class="sd">                (True) or per-agent timesteps (False).</span>
<span class="sd">            neg_index_as_lookback: If True, negative values in `indices` are</span>
<span class="sd">                interpreted as &quot;before ts=0&quot;, meaning going back into the lookback</span>
<span class="sd">                buffer. For example, an episode with agent A&#39;s actions</span>
<span class="sd">                [4, 5, 6,  7, 8, 9], where [4, 5, 6] is the lookback buffer range</span>
<span class="sd">                (ts=0 item is 7), will respond to `get_actions(-1, agent_ids=[A],</span>
<span class="sd">                neg_index_as_lookback=True)` with {A: `6`} and to</span>
<span class="sd">                `get_actions(slice(-2, 1), agent_ids=[A],</span>
<span class="sd">                neg_index_as_lookback=True)` with {A: `[5, 6,  7]`}.</span>
<span class="sd">            fill: An optional value to use for filling up the returned results at</span>
<span class="sd">                the boundaries. This filling only happens if the requested index range&#39;s</span>
<span class="sd">                start/stop boundaries exceed the episode&#39;s boundaries (including the</span>
<span class="sd">                lookback buffer on the left side). This comes in very handy, if users</span>
<span class="sd">                don&#39;t want to worry about reaching such boundaries and want to zero-pad.</span>
<span class="sd">                For example, an episode with agent A&#39; actions [10, 11,  12, 13, 14]</span>
<span class="sd">                and lookback buffer size of 2 (meaning actions `10` and `11` are</span>
<span class="sd">                part of the lookback buffer) will respond to</span>
<span class="sd">                `get_actions(slice(-7, -2), agent_ids=[A], fill=0.0)` with</span>
<span class="sd">                `{A: [0.0, 0.0, 10, 11, 12]}`.</span>
<span class="sd">            one_hot_discrete: If True, will return one-hot vectors (instead of</span>
<span class="sd">                int-values) for those sub-components of a (possibly complex) observation</span>
<span class="sd">                space that are Discrete or MultiDiscrete.  Note that if `fill=0` and the</span>
<span class="sd">                requested `indices` are out of the range of our data, the returned</span>
<span class="sd">                one-hot vectors will actually be zero-hot (all slots zero).</span>
<span class="sd">            return_list: Whether to return a list of multi-agent dicts (instead of</span>
<span class="sd">                a single multi-agent dict of lists/structs). False by default. This</span>
<span class="sd">                option can only be used when `env_steps` is True due to the fact the</span>
<span class="sd">                such a list can only be interpreted as one env step per list item</span>
<span class="sd">                (would not work with agent steps).</span>

<span class="sd">        Returns:</span>
<span class="sd">            A dictionary mapping agent IDs to actions (at the given</span>
<span class="sd">            `indices`). If `env_steps` is True, only agents that have stepped</span>
<span class="sd">            (were ready) at the given env step `indices` are returned (i.e. not all</span>
<span class="sd">            agent IDs are necessarily in the keys).</span>
<span class="sd">            If `return_list` is True, returns a list of MultiAgentDicts (mapping agent</span>
<span class="sd">            IDs to extra_model_outputs) instead.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get</span><span class="p">(</span>
            <span class="n">what</span><span class="o">=</span><span class="s2">&quot;extra_model_outputs&quot;</span><span class="p">,</span>
            <span class="n">extra_model_outputs_key</span><span class="o">=</span><span class="n">key</span><span class="p">,</span>
            <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span>
            <span class="n">agent_ids</span><span class="o">=</span><span class="n">agent_ids</span><span class="p">,</span>
            <span class="n">env_steps</span><span class="o">=</span><span class="n">env_steps</span><span class="p">,</span>
            <span class="n">neg_index_as_lookback</span><span class="o">=</span><span class="n">neg_index_as_lookback</span><span class="p">,</span>
            <span class="n">fill</span><span class="o">=</span><span class="n">fill</span><span class="p">,</span>
            <span class="n">return_list</span><span class="o">=</span><span class="n">return_list</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="MultiAgentEpisode.get_terminateds">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_terminateds.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_terminateds">[docs]</a>
    <span class="k">def</span> <span class="nf">get_terminateds</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MultiAgentDict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Gets the terminateds at given indices.&quot;&quot;&quot;</span>
        <span class="n">terminateds</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">agent_id</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span><span class="o">.</span><span class="n">is_terminated</span>
            <span class="k">for</span> <span class="n">agent_id</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_ids</span>
        <span class="p">}</span>
        <span class="n">terminateds</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;__all__&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_terminated</span><span class="p">})</span>
        <span class="k">return</span> <span class="n">terminateds</span></div>


<div class="viewcode-block" id="MultiAgentEpisode.get_truncateds">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_truncateds.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_truncateds">[docs]</a>
    <span class="k">def</span> <span class="nf">get_truncateds</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MultiAgentDict</span><span class="p">:</span>
        <span class="n">truncateds</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">agent_id</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span><span class="o">.</span><span class="n">is_truncated</span>
            <span class="k">for</span> <span class="n">agent_id</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_ids</span>
        <span class="p">}</span>
        <span class="n">truncateds</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;__all__&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_terminated</span><span class="p">})</span>
        <span class="k">return</span> <span class="n">truncateds</span></div>


<div class="viewcode-block" id="MultiAgentEpisode.add_temporary_timestep_data">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.add_temporary_timestep_data.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.add_temporary_timestep_data">[docs]</a>
    <span class="k">def</span> <span class="nf">add_temporary_timestep_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Temporarily adds (until `to_numpy()` called) per-timestep data to self.</span>

<span class="sd">        The given `data` is appended to a list (`self._temporary_timestep_data`), which</span>
<span class="sd">        is cleared upon calling `self.to_numpy()`. To get the thus-far accumulated</span>
<span class="sd">        temporary timestep data for a certain key, use the `get_temporary_timestep_data`</span>
<span class="sd">        API.</span>
<span class="sd">        Note that the size of the per timestep list is NOT checked or validated against</span>
<span class="sd">        the other, non-temporary data in this episode (like observations).</span>

<span class="sd">        Args:</span>
<span class="sd">            key: The key under which to find the list to append `data` to. If `data` is</span>
<span class="sd">                the first data to be added for this key, start a new list.</span>
<span class="sd">            data: The data item (representing a single timestep) to be stored.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_numpy</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot use the `add_temporary_timestep_data` API on an already &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;numpy&#39;ized </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">!&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_temporary_timestep_data</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">)</span></div>


<div class="viewcode-block" id="MultiAgentEpisode.get_temporary_timestep_data">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_temporary_timestep_data.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_temporary_timestep_data">[docs]</a>
    <span class="k">def</span> <span class="nf">get_temporary_timestep_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns all temporarily stored data items (list) under the given key.</span>

<span class="sd">        Note that all temporary timestep data is erased/cleared when calling</span>
<span class="sd">        `self.to_numpy()`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The current list storing temporary timestep data under `key`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_numpy</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot use the `get_temporary_timestep_data` API on an already &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;numpy&#39;ized </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">! All temporary data has been erased &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;upon `</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.to_numpy()`.&quot;</span>
            <span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_temporary_timestep_data</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Key </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> not found in temporary timestep data!&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="MultiAgentEpisode.slice">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.slice.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.slice">[docs]</a>
    <span class="k">def</span> <span class="nf">slice</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">slice_</span><span class="p">:</span> <span class="nb">slice</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">len_lookback_buffer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;MultiAgentEpisode&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns a slice of this episode with the given slice object.</span>

<span class="sd">        Works analogous to</span>
<span class="sd">        :py:meth:`~ray.rllib.env.single_agent_episode.SingleAgentEpisode.slice`</span>

<span class="sd">        However, the important differences are:</span>
<span class="sd">        - `slice_` is provided in (global) env steps, not agent steps.</span>
<span class="sd">        - In case `slice_` ends - for a certain agent - in an env step, where that</span>
<span class="sd">        particular agent does not have an observation, the previous observation will</span>
<span class="sd">        be included, but the next action and sum of rewards until this point will</span>
<span class="sd">        be stored in the agent&#39;s hanging values caches for the returned</span>
<span class="sd">        MultiAgentEpisode slice.</span>

<span class="sd">        .. testcode::</span>

<span class="sd">            from ray.rllib.env.multi_agent_episode import MultiAgentEpisode</span>
<span class="sd">            from ray.rllib.utils.test_utils import check</span>

<span class="sd">            # Generate a simple multi-agent episode.</span>
<span class="sd">            observations = [</span>
<span class="sd">                {&quot;a0&quot;: 0, &quot;a1&quot;: 0},  # 0</span>
<span class="sd">                {         &quot;a1&quot;: 1},  # 1</span>
<span class="sd">                {         &quot;a1&quot;: 2},  # 2</span>
<span class="sd">                {&quot;a0&quot;: 3, &quot;a1&quot;: 3},  # 3</span>
<span class="sd">                {&quot;a0&quot;: 4},           # 4</span>
<span class="sd">            ]</span>
<span class="sd">            # Actions are the same as observations (except for last obs, which doesn&#39;t</span>
<span class="sd">            # have an action).</span>
<span class="sd">            actions = observations[:-1]</span>
<span class="sd">            # Make up a reward for each action.</span>
<span class="sd">            rewards = [</span>
<span class="sd">                {aid: r / 10 + 0.1 for aid, r in o.items()}</span>
<span class="sd">                for o in observations</span>
<span class="sd">            ]</span>
<span class="sd">            episode = MultiAgentEpisode(</span>
<span class="sd">                observations=observations,</span>
<span class="sd">                actions=actions,</span>
<span class="sd">                rewards=rewards,</span>
<span class="sd">                len_lookback_buffer=0,</span>
<span class="sd">            )</span>

<span class="sd">            # Slice the episode and check results.</span>
<span class="sd">            slice = episode[1:3]</span>
<span class="sd">            a0 = slice.agent_episodes[&quot;a0&quot;]</span>
<span class="sd">            a1 = slice.agent_episodes[&quot;a1&quot;]</span>
<span class="sd">            check((a0.observations, a1.observations), ([3], [1, 2, 3]))</span>
<span class="sd">            check((a0.actions, a1.actions), ([], [1, 2]))</span>
<span class="sd">            check((a0.rewards, a1.rewards), ([], [0.2, 0.3]))</span>
<span class="sd">            check((a0.is_done, a1.is_done), (False, False))</span>

<span class="sd">            # If a slice ends in a &quot;gap&quot; for an agent, expect actions and rewards to be</span>
<span class="sd">            # cached for this agent.</span>
<span class="sd">            slice = episode[:2]</span>
<span class="sd">            a0 = slice.agent_episodes[&quot;a0&quot;]</span>
<span class="sd">            check(a0.observations, [0])</span>
<span class="sd">            check(a0.actions, [])</span>
<span class="sd">            check(a0.rewards, [])</span>
<span class="sd">            check(slice._hanging_actions_end[&quot;a0&quot;], 0)</span>
<span class="sd">            check(slice._hanging_rewards_end[&quot;a0&quot;], 0.1)</span>

<span class="sd">        Args:</span>
<span class="sd">            slice_: The slice object to use for slicing. This should exclude the</span>
<span class="sd">                lookback buffer, which will be prepended automatically to the returned</span>
<span class="sd">                slice.</span>
<span class="sd">            len_lookback_buffer: If not None, forces the returned slice to try to have</span>
<span class="sd">                this number of timesteps in its lookback buffer (if available). If None</span>
<span class="sd">                (default), tries to make the returned slice&#39;s lookback as large as the</span>
<span class="sd">                current lookback buffer of this episode (`self`).</span>

<span class="sd">        Returns:</span>
<span class="sd">            The new MultiAgentEpisode representing the requested slice.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">slice_</span><span class="o">.</span><span class="n">step</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;Slicing MultiAgentEnv with a step other than 1 (you used&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">slice_</span><span class="o">.</span><span class="n">step</span><span class="si">}</span><span class="s2">) is not supported!&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Translate `slice_` into one that only contains 0-or-positive ints and will</span>
        <span class="c1"># NOT contain any None.</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">slice_</span><span class="o">.</span><span class="n">start</span>
        <span class="n">stop</span> <span class="o">=</span> <span class="n">slice_</span><span class="o">.</span><span class="n">stop</span>

        <span class="c1"># Start is None -&gt; 0.</span>
        <span class="k">if</span> <span class="n">start</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># Start is negative -&gt; Interpret index as counting &quot;from end&quot;.</span>
        <span class="k">elif</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">+</span> <span class="n">start</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="c1"># Start is larger than len(self) -&gt; Clip to len(self).</span>
        <span class="k">elif</span> <span class="n">start</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="n">start</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="c1"># Stop is None -&gt; Set stop to our len (one ts past last valid index).</span>
        <span class="k">if</span> <span class="n">stop</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">stop</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="c1"># Stop is negative -&gt; Interpret index as counting &quot;from end&quot;.</span>
        <span class="k">elif</span> <span class="n">stop</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">stop</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">+</span> <span class="n">stop</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="c1"># Stop is larger than len(self) -&gt; Clip to len(self).</span>
        <span class="k">elif</span> <span class="n">stop</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="n">stop</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="n">ref_lookback</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">aid</span><span class="p">,</span> <span class="n">sa_episode</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">ref_lookback</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">ref_lookback</span> <span class="o">=</span> <span class="n">sa_episode</span><span class="o">.</span><span class="n">observations</span><span class="o">.</span><span class="n">lookback</span>
                <span class="k">assert</span> <span class="n">sa_episode</span><span class="o">.</span><span class="n">observations</span><span class="o">.</span><span class="n">lookback</span> <span class="o">==</span> <span class="n">ref_lookback</span>
                <span class="k">assert</span> <span class="n">sa_episode</span><span class="o">.</span><span class="n">actions</span><span class="o">.</span><span class="n">lookback</span> <span class="o">==</span> <span class="n">ref_lookback</span>
                <span class="k">assert</span> <span class="n">sa_episode</span><span class="o">.</span><span class="n">rewards</span><span class="o">.</span><span class="n">lookback</span> <span class="o">==</span> <span class="n">ref_lookback</span>
                <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
                    <span class="n">ilb</span><span class="o">.</span><span class="n">lookback</span> <span class="o">==</span> <span class="n">ref_lookback</span>
                    <span class="k">for</span> <span class="n">ilb</span> <span class="ow">in</span> <span class="n">sa_episode</span><span class="o">.</span><span class="n">extra_model_outputs</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
                <span class="p">)</span>
        <span class="k">except</span> <span class="ne">AssertionError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Can only slice a MultiAgentEpisode if all lookback buffers in this &quot;</span>
                <span class="s2">&quot;episode have the exact same size!&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Determine terminateds/truncateds and when (in agent timesteps) the</span>
        <span class="c1"># single-agent episode slices start.</span>
        <span class="n">terminateds</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">truncateds</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">agent_t_started</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">aid</span><span class="p">,</span> <span class="n">sa_episode</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">mapping</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="p">[</span><span class="n">aid</span><span class="p">]</span>
            <span class="c1"># If the (agent) timestep directly at the slice stop boundary is equal to</span>
            <span class="c1"># the length of the single-agent episode of this agent -&gt; Use the</span>
            <span class="c1"># single-agent episode&#39;s terminated/truncated flags.</span>
            <span class="c1"># If `stop` is already beyond this agent&#39;s single-agent episode, then we</span>
            <span class="c1"># don&#39;t have to keep track of this: The MultiAgentEpisode initializer will</span>
            <span class="c1"># automatically determine that this agent must be done (b/c it has no action</span>
            <span class="c1"># following its final observation).</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">stop</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">mapping</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">mapping</span><span class="p">[</span><span class="n">stop</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">SKIP_ENV_TS_TAG</span>
                <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">sa_episode</span><span class="p">)</span> <span class="o">==</span> <span class="n">mapping</span><span class="p">[</span><span class="n">stop</span><span class="p">]</span>
            <span class="p">):</span>
                <span class="n">terminateds</span><span class="p">[</span><span class="n">aid</span><span class="p">]</span> <span class="o">=</span> <span class="n">sa_episode</span><span class="o">.</span><span class="n">is_terminated</span>
                <span class="n">truncateds</span><span class="p">[</span><span class="n">aid</span><span class="p">]</span> <span class="o">=</span> <span class="n">sa_episode</span><span class="o">.</span><span class="n">is_truncated</span>
            <span class="c1"># Determine this agent&#39;s t_started.</span>
            <span class="k">if</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">mapping</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">mapping</span><span class="p">)):</span>
                    <span class="k">if</span> <span class="n">mapping</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">SKIP_ENV_TS_TAG</span><span class="p">:</span>
                        <span class="n">agent_t_started</span><span class="p">[</span><span class="n">aid</span><span class="p">]</span> <span class="o">=</span> <span class="n">sa_episode</span><span class="o">.</span><span class="n">t_started</span> <span class="o">+</span> <span class="n">mapping</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                        <span class="k">break</span>
        <span class="n">terminateds</span><span class="p">[</span><span class="s2">&quot;__all__&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span>
            <span class="n">terminateds</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">aid</span><span class="p">)</span> <span class="k">for</span> <span class="n">aid</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span>
        <span class="p">)</span>
        <span class="n">truncateds</span><span class="p">[</span><span class="s2">&quot;__all__&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="n">truncateds</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">aid</span><span class="p">)</span> <span class="k">for</span> <span class="n">aid</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="p">)</span>

        <span class="c1"># Determine all other slice contents.</span>
        <span class="n">_lb</span> <span class="o">=</span> <span class="n">len_lookback_buffer</span> <span class="k">if</span> <span class="n">len_lookback_buffer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">ref_lookback</span>
        <span class="k">if</span> <span class="n">start</span> <span class="o">-</span> <span class="n">_lb</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">ref_lookback</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">_lb</span> <span class="o">-</span> <span class="n">start</span><span class="p">):</span>
            <span class="n">_lb</span> <span class="o">=</span> <span class="n">ref_lookback</span> <span class="o">+</span> <span class="n">start</span>
        <span class="n">observations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_observations</span><span class="p">(</span>
            <span class="nb">slice</span><span class="p">(</span><span class="n">start</span> <span class="o">-</span> <span class="n">_lb</span><span class="p">,</span> <span class="n">stop</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">neg_index_as_lookback</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">return_list</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_actions</span><span class="p">(</span>
            <span class="nb">slice</span><span class="p">(</span><span class="n">start</span> <span class="o">-</span> <span class="n">_lb</span><span class="p">,</span> <span class="n">stop</span><span class="p">),</span>
            <span class="n">neg_index_as_lookback</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">return_list</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">rewards</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_rewards</span><span class="p">(</span>
            <span class="nb">slice</span><span class="p">(</span><span class="n">start</span> <span class="o">-</span> <span class="n">_lb</span><span class="p">,</span> <span class="n">stop</span><span class="p">),</span>
            <span class="n">neg_index_as_lookback</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">return_list</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">extra_model_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_extra_model_outputs</span><span class="p">(</span>
            <span class="n">indices</span><span class="o">=</span><span class="nb">slice</span><span class="p">(</span><span class="n">start</span> <span class="o">-</span> <span class="n">_lb</span><span class="p">,</span> <span class="n">stop</span><span class="p">),</span>
            <span class="n">neg_index_as_lookback</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">return_list</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Create the actual slice to be returned.</span>
        <span class="n">ma_episode</span> <span class="o">=</span> <span class="n">MultiAgentEpisode</span><span class="p">(</span>
            <span class="n">id_</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">id_</span><span class="p">,</span>
            <span class="c1"># In the following, offset `start`s automatically by lookbacks.</span>
            <span class="n">observations</span><span class="o">=</span><span class="n">observations</span><span class="p">,</span>
            <span class="n">observation_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span>
            <span class="n">actions</span><span class="o">=</span><span class="n">actions</span><span class="p">,</span>
            <span class="n">action_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span>
            <span class="n">rewards</span><span class="o">=</span><span class="n">rewards</span><span class="p">,</span>
            <span class="n">extra_model_outputs</span><span class="o">=</span><span class="n">extra_model_outputs</span><span class="p">,</span>
            <span class="n">terminateds</span><span class="o">=</span><span class="n">terminateds</span><span class="p">,</span>
            <span class="n">truncateds</span><span class="o">=</span><span class="n">truncateds</span><span class="p">,</span>
            <span class="n">len_lookback_buffer</span><span class="o">=</span><span class="n">_lb</span><span class="p">,</span>
            <span class="n">env_t_started</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_t_started</span> <span class="o">+</span> <span class="n">start</span><span class="p">,</span>
            <span class="n">agent_episode_ids</span><span class="o">=</span><span class="p">{</span>
                <span class="n">aid</span><span class="p">:</span> <span class="n">eid</span><span class="o">.</span><span class="n">id_</span> <span class="k">for</span> <span class="n">aid</span><span class="p">,</span> <span class="n">eid</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="p">},</span>
            <span class="n">agent_t_started</span><span class="o">=</span><span class="n">agent_t_started</span><span class="p">,</span>
            <span class="n">agent_module_ids</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_agent_to_module_mapping</span><span class="p">,</span>
            <span class="n">agent_to_module_mapping_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">agent_to_module_mapping_fn</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Numpy&#39;ize slice if `self` is also finalized.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_numpy</span><span class="p">:</span>
            <span class="n">ma_episode</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">ma_episode</span></div>


<div class="viewcode-block" id="MultiAgentEpisode.__len__">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.__len__.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.__len__">[docs]</a>
    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the length of an `MultiAgentEpisode`.</span>

<span class="sd">        Note that the length of an episode is defined by the difference</span>
<span class="sd">        between its actual timestep and the starting point.</span>

<span class="sd">        Returns: An integer defining the length of the episode or an</span>
<span class="sd">            error if the episode has not yet started.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_t</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_t_started</span></div>


    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">sa_eps_returns</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">aid</span><span class="p">:</span> <span class="n">sa_eps</span><span class="o">.</span><span class="n">get_return</span><span class="p">()</span> <span class="k">for</span> <span class="n">aid</span><span class="p">,</span> <span class="n">sa_eps</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;MAEps(len=</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="si">}</span><span class="s2"> done=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">is_done</span><span class="si">}</span><span class="s2"> &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Rs=</span><span class="si">{</span><span class="n">sa_eps_returns</span><span class="si">}</span><span class="s2"> id_=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">id_</span><span class="si">}</span><span class="s2">)&quot;</span>
        <span class="p">)</span>

<div class="viewcode-block" id="MultiAgentEpisode.print">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.print.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.print">[docs]</a>
    <span class="k">def</span> <span class="nf">print</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Prints this MultiAgentEpisode as a table of observations for the agents.&quot;&quot;&quot;</span>

        <span class="c1"># Find the maximum timestep across all agents to determine the grid width.</span>
        <span class="n">max_ts</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">ts</span><span class="o">.</span><span class="n">len_incl_lookback</span><span class="p">()</span> <span class="k">for</span> <span class="n">ts</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="n">lookback</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span><span class="o">.</span><span class="n">lookback</span>
        <span class="n">longest_agent</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">aid</span><span class="p">)</span> <span class="k">for</span> <span class="n">aid</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_ids</span><span class="p">)</span>
        <span class="c1"># Construct the header.</span>
        <span class="n">header</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;ts&quot;</span>
            <span class="o">+</span> <span class="p">(</span><span class="s2">&quot; &quot;</span> <span class="o">*</span> <span class="n">longest_agent</span><span class="p">)</span>
            <span class="o">+</span> <span class="s2">&quot;   &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="n">lookback</span><span class="p">,</span> <span class="n">max_ts</span> <span class="o">-</span> <span class="n">lookback</span><span class="p">))</span>
            <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="c1"># Construct each agent&#39;s row.</span>
        <span class="n">rows</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">agent</span><span class="p">,</span> <span class="n">inf_buffer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">row</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">agent</span><span class="si">}</span><span class="s2">  &quot;</span> <span class="o">+</span> <span class="p">(</span><span class="s2">&quot; &quot;</span> <span class="o">*</span> <span class="p">(</span><span class="n">longest_agent</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">agent</span><span class="p">)))</span>
            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">inf_buffer</span><span class="o">.</span><span class="n">data</span><span class="p">:</span>
                <span class="c1"># Two spaces for alignment.</span>
                <span class="k">if</span> <span class="n">t</span> <span class="o">==</span> <span class="s2">&quot;S&quot;</span><span class="p">:</span>
                    <span class="n">row</span> <span class="o">+=</span> <span class="s2">&quot;    &quot;</span>
                <span class="c1"># Mark the step with an x.</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">row</span> <span class="o">+=</span> <span class="s2">&quot; x  &quot;</span>
            <span class="c1"># Remove trailing space for alignment.</span>
            <span class="n">rows</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">row</span><span class="o">.</span><span class="n">rstrip</span><span class="p">())</span>

        <span class="c1"># Join all components into a final string</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">header</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">rows</span><span class="p">))</span></div>


<div class="viewcode-block" id="MultiAgentEpisode.get_state">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_state.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_state">[docs]</a>
    <span class="k">def</span> <span class="nf">get_state</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the state of a multi-agent episode.</span>

<span class="sd">        Note that from an episode&#39;s state the episode itself can</span>
<span class="sd">        be recreated.</span>

<span class="sd">        Returns: A dicitonary containing pickable data for a</span>
<span class="sd">            `MultiAgentEpisode`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;id_&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">id_</span><span class="p">,</span>
            <span class="s2">&quot;agent_to_module_mapping_fn&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_to_module_mapping_fn</span><span class="p">,</span>
            <span class="s2">&quot;_agent_to_module_mapping&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_agent_to_module_mapping</span><span class="p">,</span>
            <span class="s2">&quot;observation_space&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span>
            <span class="s2">&quot;action_space&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span>
            <span class="s2">&quot;env_t_started&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_t_started</span><span class="p">,</span>
            <span class="s2">&quot;env_t&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_t</span><span class="p">,</span>
            <span class="s2">&quot;agent_t_started&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_t_started</span><span class="p">,</span>
            <span class="c1"># TODO (simon): Check, if we can store the `InfiniteLookbackBuffer`</span>
            <span class="s2">&quot;env_t_to_agent_t&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="p">,</span>
            <span class="s2">&quot;_hanging_actions_end&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_actions_end</span><span class="p">,</span>
            <span class="s2">&quot;_hanging_extra_model_outputs_end&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_extra_model_outputs_end</span><span class="p">,</span>
            <span class="s2">&quot;_hanging_rewards_end&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_rewards_end</span><span class="p">,</span>
            <span class="s2">&quot;_hanging_rewards_begin&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_rewards_begin</span><span class="p">,</span>
            <span class="s2">&quot;is_terminated&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_terminated</span><span class="p">,</span>
            <span class="s2">&quot;is_truncated&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_truncated</span><span class="p">,</span>
            <span class="s2">&quot;agent_episodes&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="n">agent_id</span><span class="p">:</span> <span class="n">agent_eps</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>
                    <span class="k">for</span> <span class="n">agent_id</span><span class="p">,</span> <span class="n">agent_eps</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                <span class="p">}</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="p">),</span>
            <span class="s2">&quot;_start_time&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_start_time</span><span class="p">,</span>
            <span class="s2">&quot;_last_step_time&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_last_step_time</span><span class="p">,</span>
        <span class="p">}</span></div>


<div class="viewcode-block" id="MultiAgentEpisode.from_state">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.from_state.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.from_state">[docs]</a>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">from_state</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;MultiAgentEpisode&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Creates a multi-agent episode from a state dictionary.</span>

<span class="sd">        See `MultiAgentEpisode.get_state()` for creating a state for</span>
<span class="sd">        a `MultiAgentEpisode` pickable state. For recreating a</span>
<span class="sd">        `MultiAgentEpisode` from a state, this state has to be complete,</span>
<span class="sd">        i.e. all data must have been stored in the state.</span>

<span class="sd">        Args:</span>
<span class="sd">            state: A dict containing all data required to recreate a MultiAgentEpisode`.</span>
<span class="sd">                See `MultiAgentEpisode.get_state()`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `MultiAgentEpisode` instance created from the state data.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Create an empty `MultiAgentEpisode` instance.</span>
        <span class="n">episode</span> <span class="o">=</span> <span class="n">MultiAgentEpisode</span><span class="p">(</span><span class="n">id_</span><span class="o">=</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;id_&quot;</span><span class="p">])</span>
        <span class="c1"># Fill the instance with the state data.</span>
        <span class="n">episode</span><span class="o">.</span><span class="n">agent_to_module_mapping_fn</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;agent_to_module_mapping_fn&quot;</span><span class="p">]</span>
        <span class="n">episode</span><span class="o">.</span><span class="n">_agent_to_module_mapping</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;_agent_to_module_mapping&quot;</span><span class="p">]</span>
        <span class="n">episode</span><span class="o">.</span><span class="n">observation_space</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;observation_space&quot;</span><span class="p">]</span>
        <span class="n">episode</span><span class="o">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;action_space&quot;</span><span class="p">]</span>
        <span class="n">episode</span><span class="o">.</span><span class="n">env_t_started</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;env_t_started&quot;</span><span class="p">]</span>
        <span class="n">episode</span><span class="o">.</span><span class="n">env_t</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;env_t&quot;</span><span class="p">]</span>
        <span class="n">episode</span><span class="o">.</span><span class="n">agent_t_started</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;agent_t_started&quot;</span><span class="p">]</span>
        <span class="n">episode</span><span class="o">.</span><span class="n">env_t_to_agent_t</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;env_t_to_agent_t&quot;</span><span class="p">]</span>
        <span class="n">episode</span><span class="o">.</span><span class="n">_hanging_actions_end</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;_hanging_actions_end&quot;</span><span class="p">]</span>
        <span class="n">episode</span><span class="o">.</span><span class="n">_hanging_extra_model_outputs_end</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span>
            <span class="s2">&quot;_hanging_extra_model_outputs_end&quot;</span>
        <span class="p">]</span>
        <span class="n">episode</span><span class="o">.</span><span class="n">_hanging_rewards_end</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;_hanging_rewards_end&quot;</span><span class="p">]</span>
        <span class="n">episode</span><span class="o">.</span><span class="n">_hanging_rewards_begin</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;_hanging_rewards_begin&quot;</span><span class="p">]</span>
        <span class="n">episode</span><span class="o">.</span><span class="n">is_terminated</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;is_terminated&quot;</span><span class="p">]</span>
        <span class="n">episode</span><span class="o">.</span><span class="n">is_truncated</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;is_truncated&quot;</span><span class="p">]</span>
        <span class="n">episode</span><span class="o">.</span><span class="n">agent_episodes</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">agent_id</span><span class="p">:</span> <span class="n">SingleAgentEpisode</span><span class="o">.</span><span class="n">from_state</span><span class="p">(</span><span class="n">agent_state</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">agent_id</span><span class="p">,</span> <span class="n">agent_state</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;agent_episodes&quot;</span><span class="p">]</span>
        <span class="p">}</span>
        <span class="n">episode</span><span class="o">.</span><span class="n">_start_time</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;_start_time&quot;</span><span class="p">]</span>
        <span class="n">episode</span><span class="o">.</span><span class="n">_last_step_time</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;_last_step_time&quot;</span><span class="p">]</span>

        <span class="c1"># Validate the episode.</span>
        <span class="n">episode</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">episode</span></div>


<div class="viewcode-block" id="MultiAgentEpisode.get_sample_batch">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_sample_batch.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_sample_batch">[docs]</a>
    <span class="k">def</span> <span class="nf">get_sample_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MultiAgentBatch</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Converts this `MultiAgentEpisode` into a `MultiAgentBatch`.</span>

<span class="sd">        Each `SingleAgentEpisode` instances in `MultiAgentEpisode.agent_epiosdes`</span>
<span class="sd">        will be converted into a `SampleBatch` and the environment timestep will be</span>
<span class="sd">        passed as the returned MultiAgentBatch&#39;s `env_steps`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A MultiAgentBatch containing all of this episode&#39;s data.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># TODO (simon): Check, if timesteps should be converted into global</span>
        <span class="c1"># timesteps instead of agent steps.</span>
        <span class="c1"># Note, only agents that have stepped are included into the batch.</span>
        <span class="k">return</span> <span class="n">MultiAgentBatch</span><span class="p">(</span>
            <span class="n">policy_batches</span><span class="o">=</span><span class="p">{</span>
                <span class="n">agent_id</span><span class="p">:</span> <span class="n">agent_eps</span><span class="o">.</span><span class="n">get_sample_batch</span><span class="p">()</span>
                <span class="k">for</span> <span class="n">agent_id</span><span class="p">,</span> <span class="n">agent_eps</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">agent_eps</span><span class="o">.</span><span class="n">t</span> <span class="o">-</span> <span class="n">agent_eps</span><span class="o">.</span><span class="n">t_started</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="p">},</span>
            <span class="n">env_steps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_t</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_t_started</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="MultiAgentEpisode.get_return">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_return.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_return">[docs]</a>
    <span class="k">def</span> <span class="nf">get_return</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">include_hanging_rewards</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns all-agent return.</span>

<span class="sd">        Args:</span>
<span class="sd">            include_hanging_rewards: Whether we should also consider</span>
<span class="sd">                hanging rewards wehn calculating the overall return. Agents might</span>
<span class="sd">                have received partial rewards, i.e. rewards without an</span>
<span class="sd">                observation. These are stored in the &quot;hanging&quot; caches (begin and end)</span>
<span class="sd">                for each agent and added up until the next observation is received by</span>
<span class="sd">                that agent.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The sum of all single-agents&#39; returns (maybe including the hanging</span>
<span class="sd">            rewards per agent).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">env_return</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span>
            <span class="n">agent_eps</span><span class="o">.</span><span class="n">get_return</span><span class="p">()</span> <span class="k">for</span> <span class="n">agent_eps</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">include_hanging_rewards</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">hanging_r</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_rewards_begin</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                <span class="n">env_return</span> <span class="o">+=</span> <span class="n">hanging_r</span>
            <span class="k">for</span> <span class="n">hanging_r</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_rewards_end</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                <span class="n">env_return</span> <span class="o">+=</span> <span class="n">hanging_r</span>

        <span class="k">return</span> <span class="n">env_return</span></div>


<div class="viewcode-block" id="MultiAgentEpisode.get_agents_to_act">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_agents_to_act.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_agents_to_act">[docs]</a>
    <span class="k">def</span> <span class="nf">get_agents_to_act</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Set</span><span class="p">[</span><span class="n">AgentID</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns a set of agent IDs required to send an action to `env.step()` next.</span>

<span class="sd">        Those are generally the agents that received an observation in the most recent</span>
<span class="sd">        `env.step()` call.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A set of AgentIDs that are supposed to send actions to the next `env.step()`</span>
<span class="sd">            call.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="n">aid</span>
            <span class="k">for</span> <span class="n">aid</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_observations</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="p">[</span><span class="n">aid</span><span class="p">]</span><span class="o">.</span><span class="n">is_done</span>
        <span class="p">}</span></div>


<div class="viewcode-block" id="MultiAgentEpisode.get_agents_that_stepped">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_agents_that_stepped.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_agents_that_stepped">[docs]</a>
    <span class="k">def</span> <span class="nf">get_agents_that_stepped</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Set</span><span class="p">[</span><span class="n">AgentID</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns a set of agent IDs of those agents that just finished stepping.</span>

<span class="sd">        These are all the agents that have an observation logged at the last env</span>
<span class="sd">        timestep, which may include agents, whose single agent episode just terminated</span>
<span class="sd">        or truncated.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A set of AgentIDs of those agents that just finished stepping (that have a</span>
<span class="sd">            most recent observation on the env timestep scale), regardless of whether</span>
<span class="sd">            their single agent episodes are done or not.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_observations</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span></div>


<div class="viewcode-block" id="MultiAgentEpisode.get_duration_s">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_duration_s.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.get_duration_s">[docs]</a>
    <span class="k">def</span> <span class="nf">get_duration_s</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the duration of this Episode (chunk) in seconds.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_last_step_time</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="mf">0.0</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_last_step_time</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_start_time</span></div>


<div class="viewcode-block" id="MultiAgentEpisode.env_steps">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.env_steps.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.env_steps">[docs]</a>
    <span class="k">def</span> <span class="nf">env_steps</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the number of environment steps.</span>

<span class="sd">        Note, this episode instance could be a chunk of an actual episode.</span>

<span class="sd">        Returns:</span>
<span class="sd">            An integer that counts the number of environment steps this episode instance</span>
<span class="sd">            has seen.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>


<div class="viewcode-block" id="MultiAgentEpisode.agent_steps">
<a class="viewcode-back" href="../../../../rllib/package_ref/env/env/ray.rllib.env.multi_agent_episode.MultiAgentEpisode.agent_steps.html#ray.rllib.env.multi_agent_episode.MultiAgentEpisode.agent_steps">[docs]</a>
    <span class="k">def</span> <span class="nf">agent_steps</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Number of agent steps.</span>

<span class="sd">        Note, there are &gt;= 1 agent steps per environment step.</span>

<span class="sd">        Returns:</span>
<span class="sd">            An integer counting the number of agent steps executed during the time this</span>
<span class="sd">            episode instance records.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">eps</span><span class="p">)</span> <span class="k">for</span> <span class="n">eps</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="o">.</span><span class="n">values</span><span class="p">())</span></div>


    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">:</span> <span class="nb">slice</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;MultiAgentEpisode&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Enable squared bracket indexing- and slicing syntax, e.g. episode[-4:].&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="nb">slice</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">slice_</span><span class="o">=</span><span class="n">item</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;MultiAgentEpisode does not support getting item &#39;</span><span class="si">{</span><span class="n">item</span><span class="si">}</span><span class="s2">&#39;! &quot;</span>
                <span class="s2">&quot;Only slice objects allowed with the syntax: `episode[a:b]`.&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_init_single_agent_episodes</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">agent_module_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="n">AgentID</span><span class="p">,</span> <span class="n">ModuleID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">agent_episode_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="n">AgentID</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">observations</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">actions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">rewards</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">infos</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">terminateds</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">truncateds</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">extra_model_outputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">observations</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="n">actions</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="ow">not</span> <span class="n">rewards</span>
            <span class="k">assert</span> <span class="ow">not</span> <span class="n">extra_model_outputs</span>
            <span class="n">actions</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">extra_model_outputs</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Infos and `extra_model_outputs` are allowed to be None -&gt; Fill them with</span>
        <span class="c1"># proper dummy values, if so.</span>
        <span class="k">if</span> <span class="n">infos</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">infos</span> <span class="o">=</span> <span class="p">[{}</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">observations</span><span class="p">))]</span>
        <span class="k">if</span> <span class="n">extra_model_outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">extra_model_outputs</span> <span class="o">=</span> <span class="p">[{}</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">actions</span><span class="p">))]</span>

        <span class="n">observations_per_agent</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
        <span class="n">infos_per_agent</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
        <span class="n">actions_per_agent</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
        <span class="n">rewards_per_agent</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
        <span class="n">extra_model_outputs_per_agent</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
        <span class="n">done_per_agent</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">bool</span><span class="p">)</span>
        <span class="n">len_lookback_buffer_per_agent</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_len_lookback_buffers</span><span class="p">)</span>

        <span class="n">all_agent_ids</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
            <span class="n">agent_episode_ids</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">if</span> <span class="n">agent_episode_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">[]</span>
        <span class="p">)</span>
        <span class="n">agent_module_ids</span> <span class="o">=</span> <span class="n">agent_module_ids</span> <span class="ow">or</span> <span class="p">{}</span>

        <span class="c1"># Step through all observations and interpret these as the (global) env steps.</span>
        <span class="k">for</span> <span class="n">data_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">inf</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">observations</span><span class="p">,</span> <span class="n">infos</span><span class="p">)):</span>
            <span class="c1"># If we do have actions/extra outs/rewards for this timestep, use the data.</span>
            <span class="c1"># It may be that these lists have the same length as the observations list,</span>
            <span class="c1"># in which case the data will be cached (agent did step/send an action,</span>
            <span class="c1"># but the step has not been concluded yet by the env).</span>
            <span class="n">act</span> <span class="o">=</span> <span class="n">actions</span><span class="p">[</span><span class="n">data_idx</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">data_idx</span> <span class="k">else</span> <span class="p">{}</span>
            <span class="n">extra_outs</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">extra_model_outputs</span><span class="p">[</span><span class="n">data_idx</span><span class="p">]</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">extra_model_outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">data_idx</span>
                <span class="k">else</span> <span class="p">{}</span>
            <span class="p">)</span>
            <span class="n">rew</span> <span class="o">=</span> <span class="n">rewards</span><span class="p">[</span><span class="n">data_idx</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">data_idx</span> <span class="k">else</span> <span class="p">{}</span>

            <span class="k">for</span> <span class="n">agent_id</span><span class="p">,</span> <span class="n">agent_obs</span> <span class="ow">in</span> <span class="n">obs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">all_agent_ids</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">agent_id</span><span class="p">)</span>

                <span class="n">observations_per_agent</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">agent_obs</span><span class="p">)</span>
                <span class="n">infos_per_agent</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">,</span> <span class="p">{}))</span>

                <span class="c1"># Pull out hanging action (if not first obs for this agent) and</span>
                <span class="c1"># complete step for agent.</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">observations_per_agent</span><span class="p">[</span><span class="n">agent_id</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">actions_per_agent</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_actions_end</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">agent_id</span><span class="p">)</span>
                    <span class="p">)</span>
                    <span class="n">extra_model_outputs_per_agent</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_extra_model_outputs_end</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">agent_id</span><span class="p">)</span>
                    <span class="p">)</span>
                    <span class="n">rewards_per_agent</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_rewards_end</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">agent_id</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="c1"># First obs for this agent. Make sure the agent&#39;s mapping is</span>
                <span class="c1"># appropriately prepended with self.SKIP_ENV_TS_TAG tags.</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">agent_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
                            <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">SKIP_ENV_TS_TAG</span><span class="p">]</span> <span class="o">*</span> <span class="n">data_idx</span>
                        <span class="p">)</span>
                        <span class="n">len_lookback_buffer_per_agent</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">-=</span> <span class="n">data_idx</span>

                <span class="c1"># Agent is still continuing (has an action for the next step).</span>
                <span class="k">if</span> <span class="n">agent_id</span> <span class="ow">in</span> <span class="n">act</span><span class="p">:</span>
                    <span class="c1"># Always push actions/extra outputs into cache, then remove them</span>
                    <span class="c1"># from there, once the next observation comes in. Same for rewards.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_actions_end</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">act</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_extra_model_outputs_end</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">extra_outs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                        <span class="n">agent_id</span><span class="p">,</span> <span class="p">{}</span>
                    <span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_rewards_end</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">+=</span> <span class="n">rew</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
                <span class="c1"># Agent is done (has no action for the next step).</span>
                <span class="k">elif</span> <span class="n">terminateds</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">)</span> <span class="ow">or</span> <span class="n">truncateds</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">):</span>
                    <span class="n">done_per_agent</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="c1"># There is more (global) action/reward data. This agent must therefore</span>
                <span class="c1"># be done. Automatically add it to `done_per_agent` and `terminateds`.</span>
                <span class="k">elif</span> <span class="n">data_idx</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">observations</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">done_per_agent</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">terminateds</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

                <span class="c1"># Update env_t_to_agent_t mapping.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="nb">len</span><span class="p">(</span><span class="n">observations_per_agent</span><span class="p">[</span><span class="n">agent_id</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span>
                <span class="p">)</span>

            <span class="c1"># Those agents that did NOT step:</span>
            <span class="c1"># - Get self.SKIP_ENV_TS_TAG added to their env_t_to_agent_t mapping.</span>
            <span class="c1"># - Get their reward (if any) added up.</span>
            <span class="k">for</span> <span class="n">agent_id</span> <span class="ow">in</span> <span class="n">all_agent_ids</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">agent_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">obs</span> <span class="ow">and</span> <span class="n">agent_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">done_per_agent</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">SKIP_ENV_TS_TAG</span><span class="p">)</span>
                    <span class="c1"># If we are still in the global lookback buffer segment, deduct 1</span>
                    <span class="c1"># from this agents&#39; lookback buffer, b/c we don&#39;t want the agent</span>
                    <span class="c1"># to use this (missing) obs/data in its single-agent lookback.</span>
                    <span class="k">if</span> <span class="p">(</span>
                        <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="p">[</span><span class="n">agent_id</span><span class="p">])</span>
                        <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_len_lookback_buffers</span>
                        <span class="o">&lt;=</span> <span class="mi">0</span>
                    <span class="p">):</span>
                        <span class="n">len_lookback_buffer_per_agent</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">1</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_rewards_end</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">+=</span> <span class="n">rew</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>

        <span class="c1"># - Validate per-agent data.</span>
        <span class="c1"># - Fix lookback buffers of env_t_to_agent_t mappings.</span>
        <span class="k">for</span> <span class="n">agent_id</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="c1"># Skip agent if it doesn&#39;t seem to have any data.</span>
            <span class="k">if</span> <span class="n">agent_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">observations_per_agent</span><span class="p">:</span>
                <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span>
                <span class="k">continue</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">observations_per_agent</span><span class="p">[</span><span class="n">agent_id</span><span class="p">])</span>
                <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">infos_per_agent</span><span class="p">[</span><span class="n">agent_id</span><span class="p">])</span>
                <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">actions_per_agent</span><span class="p">[</span><span class="n">agent_id</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span>
                <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">extra_model_outputs_per_agent</span><span class="p">[</span><span class="n">agent_id</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span>
                <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">rewards_per_agent</span><span class="p">[</span><span class="n">agent_id</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span><span class="o">.</span><span class="n">lookback</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_len_lookback_buffers</span>

        <span class="c1"># Now create the individual episodes from the collected per-agent data.</span>
        <span class="k">for</span> <span class="n">agent_id</span><span class="p">,</span> <span class="n">agent_obs</span> <span class="ow">in</span> <span class="n">observations_per_agent</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># If agent only has a single obs AND is already done, remove all its traces</span>
            <span class="c1"># from this MultiAgentEpisode.</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">agent_obs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">done_per_agent</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_del_agent</span><span class="p">(</span><span class="n">agent_id</span><span class="p">)</span>
                <span class="k">continue</span>

            <span class="c1"># Try to figure out the module ID for this agent.</span>
            <span class="c1"># If not provided explicitly by the user that initializes this episode</span>
            <span class="c1"># object, try our mapping function.</span>
            <span class="n">module_id</span> <span class="o">=</span> <span class="n">agent_module_ids</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                <span class="n">agent_id</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_to_module_mapping_fn</span><span class="p">(</span><span class="n">agent_id</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="c1"># Create this agent&#39;s SingleAgentEpisode.</span>
            <span class="n">sa_episode</span> <span class="o">=</span> <span class="n">SingleAgentEpisode</span><span class="p">(</span>
                <span class="n">id_</span><span class="o">=</span><span class="p">(</span>
                    <span class="n">agent_episode_ids</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">agent_episode_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="k">else</span> <span class="kc">None</span>
                <span class="p">),</span>
                <span class="n">agent_id</span><span class="o">=</span><span class="n">agent_id</span><span class="p">,</span>
                <span class="n">module_id</span><span class="o">=</span><span class="n">module_id</span><span class="p">,</span>
                <span class="n">multi_agent_episode_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">id_</span><span class="p">,</span>
                <span class="n">observations</span><span class="o">=</span><span class="n">agent_obs</span><span class="p">,</span>
                <span class="n">observation_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">),</span>
                <span class="n">infos</span><span class="o">=</span><span class="n">infos_per_agent</span><span class="p">[</span><span class="n">agent_id</span><span class="p">],</span>
                <span class="n">actions</span><span class="o">=</span><span class="n">actions_per_agent</span><span class="p">[</span><span class="n">agent_id</span><span class="p">],</span>
                <span class="n">action_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">),</span>
                <span class="n">rewards</span><span class="o">=</span><span class="n">rewards_per_agent</span><span class="p">[</span><span class="n">agent_id</span><span class="p">],</span>
                <span class="n">extra_model_outputs</span><span class="o">=</span><span class="p">(</span>
                    <span class="p">{</span>
                        <span class="n">k</span><span class="p">:</span> <span class="p">[</span><span class="n">i</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">extra_model_outputs_per_agent</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]]</span>
                        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">extra_model_outputs_per_agent</span><span class="p">[</span><span class="n">agent_id</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
                    <span class="p">}</span>
                    <span class="k">if</span> <span class="n">extra_model_outputs_per_agent</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span>
                    <span class="k">else</span> <span class="kc">None</span>
                <span class="p">),</span>
                <span class="n">terminated</span><span class="o">=</span><span class="n">terminateds</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
                <span class="n">truncated</span><span class="o">=</span><span class="n">truncateds</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
                <span class="n">t_started</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">agent_t_started</span><span class="p">[</span><span class="n">agent_id</span><span class="p">],</span>
                <span class="n">len_lookback_buffer</span><span class="o">=</span><span class="nb">max</span><span class="p">(</span><span class="n">len_lookback_buffer_per_agent</span><span class="p">[</span><span class="n">agent_id</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="c1"># .. and store it.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">sa_episode</span>

    <span class="k">def</span> <span class="nf">_get</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">what</span><span class="p">,</span>
        <span class="n">indices</span><span class="p">,</span>
        <span class="n">agent_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">env_steps</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">neg_index_as_lookback</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">fill</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">one_hot_discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">return_list</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">extra_model_outputs_key</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">agent_ids</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">force_list</span><span class="p">(</span><span class="n">agent_ids</span><span class="p">))</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_ids</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="n">what</span><span class="o">=</span><span class="n">what</span><span class="p">,</span>
            <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span>
            <span class="n">agent_ids</span><span class="o">=</span><span class="n">agent_ids</span><span class="p">,</span>
            <span class="n">neg_index_as_lookback</span><span class="o">=</span><span class="n">neg_index_as_lookback</span><span class="p">,</span>
            <span class="n">fill</span><span class="o">=</span><span class="n">fill</span><span class="p">,</span>
            <span class="c1"># Rewards and infos do not support one_hot_discrete option.</span>
            <span class="n">one_hot_discrete</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
                <span class="p">{}</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">one_hot_discrete</span> <span class="k">else</span> <span class="p">{</span><span class="s2">&quot;one_hot_discrete&quot;</span><span class="p">:</span> <span class="n">one_hot_discrete</span><span class="p">}</span>
            <span class="p">),</span>
            <span class="n">extra_model_outputs_key</span><span class="o">=</span><span class="n">extra_model_outputs_key</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># User specified agent timesteps (indices) -&gt; Simply delegate everything</span>
        <span class="c1"># to the individual agents&#39; SingleAgentEpisodes.</span>
        <span class="k">if</span> <span class="n">env_steps</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">return_list</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;`MultiAgentEpisode.get_</span><span class="si">{</span><span class="n">what</span><span class="si">}</span><span class="s2">()` can&#39;t be called with both &quot;</span>
                    <span class="s2">&quot;`env_steps=False` and `return_list=True`!&quot;</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_data_by_agent_steps</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="c1"># User specified env timesteps (indices) -&gt; We need to translate them for each</span>
        <span class="c1"># agent into agent-timesteps.</span>
        <span class="c1"># Return a list of individual per-env-timestep multi-agent dicts.</span>
        <span class="k">elif</span> <span class="n">return_list</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_data_by_env_steps_as_list</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="c1"># Return a single multi-agent dict with lists/arrays as leafs.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_data_by_env_steps</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_data_by_agent_steps</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">what</span><span class="p">,</span>
        <span class="n">indices</span><span class="p">,</span>
        <span class="n">agent_ids</span><span class="p">,</span>
        <span class="n">neg_index_as_lookback</span><span class="p">,</span>
        <span class="n">fill</span><span class="p">,</span>
        <span class="n">one_hot_discrete</span><span class="p">,</span>
        <span class="n">extra_model_outputs_key</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># Return requested data by agent-steps.</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="c1"># For each agent, we retrieve the data through passing the given indices into</span>
        <span class="c1"># the SingleAgentEpisode of that agent.</span>
        <span class="k">for</span> <span class="n">agent_id</span><span class="p">,</span> <span class="n">sa_episode</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">agent_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">agent_ids</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">inf_lookback_buffer</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">sa_episode</span><span class="p">,</span> <span class="n">what</span><span class="p">)</span>
            <span class="n">hanging_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_hanging_value</span><span class="p">(</span><span class="n">what</span><span class="p">,</span> <span class="n">agent_id</span><span class="p">)</span>
            <span class="c1"># User wants a specific `extra_model_outputs` key.</span>
            <span class="k">if</span> <span class="n">extra_model_outputs_key</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">inf_lookback_buffer</span> <span class="o">=</span> <span class="n">inf_lookback_buffer</span><span class="p">[</span><span class="n">extra_model_outputs_key</span><span class="p">]</span>
                <span class="n">hanging_val</span> <span class="o">=</span> <span class="n">hanging_val</span><span class="p">[</span><span class="n">extra_model_outputs_key</span><span class="p">]</span>
            <span class="n">agent_value</span> <span class="o">=</span> <span class="n">inf_lookback_buffer</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span>
                <span class="n">neg_index_as_lookback</span><span class="o">=</span><span class="n">neg_index_as_lookback</span><span class="p">,</span>
                <span class="n">fill</span><span class="o">=</span><span class="n">fill</span><span class="p">,</span>
                <span class="n">_add_last_ts_value</span><span class="o">=</span><span class="n">hanging_val</span><span class="p">,</span>
                <span class="o">**</span><span class="n">one_hot_discrete</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">agent_value</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">agent_value</span> <span class="o">==</span> <span class="p">[]:</span>
                <span class="k">continue</span>
            <span class="n">ret</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">agent_value</span>
        <span class="k">return</span> <span class="n">ret</span>

    <span class="k">def</span> <span class="nf">_get_data_by_env_steps_as_list</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">what</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">indices</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">slice</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
        <span class="n">agent_ids</span><span class="p">:</span> <span class="n">Collection</span><span class="p">[</span><span class="n">AgentID</span><span class="p">],</span>
        <span class="n">neg_index_as_lookback</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">fill</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">one_hot_discrete</span><span class="p">,</span>
        <span class="n">extra_model_outputs_key</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">MultiAgentDict</span><span class="p">]:</span>
        <span class="c1"># Collect indices for each agent first, so we can construct the list in</span>
        <span class="c1"># the next step.</span>
        <span class="n">agent_indices</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">agent_id</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">agent_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">agent_ids</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">agent_indices</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                <span class="n">indices</span><span class="p">,</span>
                <span class="n">neg_index_as_lookback</span><span class="o">=</span><span class="n">neg_index_as_lookback</span><span class="p">,</span>
                <span class="n">fill</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">SKIP_ENV_TS_TAG</span><span class="p">,</span>
                <span class="c1"># For those records where there is no &quot;hanging&quot; last timestep (all</span>
                <span class="c1"># other than obs and infos), we have to ignore the last entry in</span>
                <span class="c1"># the env_t_to_agent_t mappings.</span>
                <span class="n">_ignore_last_ts</span><span class="o">=</span><span class="n">what</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;observations&quot;</span><span class="p">,</span> <span class="s2">&quot;infos&quot;</span><span class="p">],</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">agent_indices</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[]</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">agent_indices</span><span class="o">.</span><span class="n">values</span><span class="p">())))):</span>
            <span class="n">ret2</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">agent_id</span><span class="p">,</span> <span class="n">idxes</span> <span class="ow">in</span> <span class="n">agent_indices</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">hanging_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_hanging_value</span><span class="p">(</span><span class="n">what</span><span class="p">,</span> <span class="n">agent_id</span><span class="p">)</span>
                <span class="p">(</span>
                    <span class="n">inf_lookback_buffer</span><span class="p">,</span>
                    <span class="n">indices_to_use</span><span class="p">,</span>
                <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_inf_lookback_buffer_or_dict</span><span class="p">(</span>
                    <span class="n">agent_id</span><span class="p">,</span>
                    <span class="n">what</span><span class="p">,</span>
                    <span class="n">extra_model_outputs_key</span><span class="p">,</span>
                    <span class="n">hanging_val</span><span class="p">,</span>
                    <span class="n">filter_for_skip_indices</span><span class="o">=</span><span class="n">idxes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="n">what</span> <span class="o">==</span> <span class="s2">&quot;extra_model_outputs&quot;</span>
                    <span class="ow">and</span> <span class="ow">not</span> <span class="n">inf_lookback_buffer</span>
                    <span class="ow">and</span> <span class="ow">not</span> <span class="n">hanging_val</span>
                <span class="p">):</span>
                    <span class="k">continue</span>
                <span class="n">agent_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_single_agent_data_by_index</span><span class="p">(</span>
                    <span class="n">what</span><span class="o">=</span><span class="n">what</span><span class="p">,</span>
                    <span class="n">inf_lookback_buffer</span><span class="o">=</span><span class="n">inf_lookback_buffer</span><span class="p">,</span>
                    <span class="n">agent_id</span><span class="o">=</span><span class="n">agent_id</span><span class="p">,</span>
                    <span class="n">index_incl_lookback</span><span class="o">=</span><span class="n">indices_to_use</span><span class="p">,</span>
                    <span class="n">fill</span><span class="o">=</span><span class="n">fill</span><span class="p">,</span>
                    <span class="n">one_hot_discrete</span><span class="o">=</span><span class="n">one_hot_discrete</span><span class="p">,</span>
                    <span class="n">extra_model_outputs_key</span><span class="o">=</span><span class="n">extra_model_outputs_key</span><span class="p">,</span>
                    <span class="n">hanging_val</span><span class="o">=</span><span class="n">hanging_val</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">agent_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">ret2</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">agent_value</span>
            <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ret2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ret</span>

    <span class="k">def</span> <span class="nf">_get_data_by_env_steps</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">what</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">indices</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">slice</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
        <span class="n">agent_ids</span><span class="p">:</span> <span class="n">Collection</span><span class="p">[</span><span class="n">AgentID</span><span class="p">],</span>
        <span class="n">neg_index_as_lookback</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">fill</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">one_hot_discrete</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">extra_model_outputs_key</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MultiAgentDict</span><span class="p">:</span>
        <span class="n">ignore_last_ts</span> <span class="o">=</span> <span class="n">what</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;observations&quot;</span><span class="p">,</span> <span class="s2">&quot;infos&quot;</span><span class="p">]</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">agent_id</span><span class="p">,</span> <span class="n">sa_episode</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">agent_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">agent_ids</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">hanging_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_hanging_value</span><span class="p">(</span><span class="n">what</span><span class="p">,</span> <span class="n">agent_id</span><span class="p">)</span>
            <span class="n">agent_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                <span class="n">indices</span><span class="p">,</span>
                <span class="n">neg_index_as_lookback</span><span class="o">=</span><span class="n">neg_index_as_lookback</span><span class="p">,</span>
                <span class="n">fill</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">SKIP_ENV_TS_TAG</span> <span class="k">if</span> <span class="n">fill</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                <span class="c1"># For those records where there is no &quot;hanging&quot; last timestep (all</span>
                <span class="c1"># other than obs and infos), we have to ignore the last entry in</span>
                <span class="c1"># the env_t_to_agent_t mappings.</span>
                <span class="n">_ignore_last_ts</span><span class="o">=</span><span class="n">ignore_last_ts</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">inf_lookback_buffer</span><span class="p">,</span> <span class="n">agent_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_inf_lookback_buffer_or_dict</span><span class="p">(</span>
                <span class="n">agent_id</span><span class="p">,</span>
                <span class="n">what</span><span class="p">,</span>
                <span class="n">extra_model_outputs_key</span><span class="p">,</span>
                <span class="n">hanging_val</span><span class="p">,</span>
                <span class="n">filter_for_skip_indices</span><span class="o">=</span><span class="n">agent_indices</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">agent_indices</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="n">agent_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_single_agent_data_by_env_step_indices</span><span class="p">(</span>
                    <span class="n">what</span><span class="o">=</span><span class="n">what</span><span class="p">,</span>
                    <span class="n">agent_id</span><span class="o">=</span><span class="n">agent_id</span><span class="p">,</span>
                    <span class="n">indices_incl_lookback</span><span class="o">=</span><span class="n">agent_indices</span><span class="p">,</span>
                    <span class="n">fill</span><span class="o">=</span><span class="n">fill</span><span class="p">,</span>
                    <span class="n">one_hot_discrete</span><span class="o">=</span><span class="n">one_hot_discrete</span><span class="p">,</span>
                    <span class="n">hanging_val</span><span class="o">=</span><span class="n">hanging_val</span><span class="p">,</span>
                    <span class="n">extra_model_outputs_key</span><span class="o">=</span><span class="n">extra_model_outputs_key</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">agent_values</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">ret</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">agent_values</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">agent_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_single_agent_data_by_index</span><span class="p">(</span>
                    <span class="n">what</span><span class="o">=</span><span class="n">what</span><span class="p">,</span>
                    <span class="n">inf_lookback_buffer</span><span class="o">=</span><span class="n">inf_lookback_buffer</span><span class="p">,</span>
                    <span class="n">agent_id</span><span class="o">=</span><span class="n">agent_id</span><span class="p">,</span>
                    <span class="n">index_incl_lookback</span><span class="o">=</span><span class="n">agent_indices</span><span class="p">,</span>
                    <span class="n">fill</span><span class="o">=</span><span class="n">fill</span><span class="p">,</span>
                    <span class="n">one_hot_discrete</span><span class="o">=</span><span class="n">one_hot_discrete</span><span class="p">,</span>
                    <span class="n">extra_model_outputs_key</span><span class="o">=</span><span class="n">extra_model_outputs_key</span><span class="p">,</span>
                    <span class="n">hanging_val</span><span class="o">=</span><span class="n">hanging_val</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">agent_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">ret</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">agent_values</span>
        <span class="k">return</span> <span class="n">ret</span>

    <span class="k">def</span> <span class="nf">_get_single_agent_data_by_index</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">what</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">inf_lookback_buffer</span><span class="p">:</span> <span class="n">InfiniteLookbackBuffer</span><span class="p">,</span>
        <span class="n">agent_id</span><span class="p">:</span> <span class="n">AgentID</span><span class="p">,</span>
        <span class="n">index_incl_lookback</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span>
        <span class="n">fill</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">one_hot_discrete</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
        <span class="n">extra_model_outputs_key</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">hanging_val</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="n">sa_episode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">index_incl_lookback</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">SKIP_ENV_TS_TAG</span><span class="p">:</span>
            <span class="c1"># We don&#39;t want to fill -&gt; Skip this agent.</span>
            <span class="k">if</span> <span class="n">fill</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span>
            <span class="c1"># Provide filled value for this agent.</span>
            <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">sa_episode</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;get_</span><span class="si">{</span><span class="n">what</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)(</span>
                <span class="n">indices</span><span class="o">=</span><span class="mi">1000000000000</span><span class="p">,</span>
                <span class="n">neg_index_as_lookback</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">fill</span><span class="o">=</span><span class="n">fill</span><span class="p">,</span>
                <span class="o">**</span><span class="nb">dict</span><span class="p">(</span>
                    <span class="p">{}</span>
                    <span class="k">if</span> <span class="n">extra_model_outputs_key</span> <span class="ow">is</span> <span class="kc">None</span>
                    <span class="k">else</span> <span class="p">{</span><span class="s2">&quot;key&quot;</span><span class="p">:</span> <span class="n">extra_model_outputs_key</span><span class="p">}</span>
                <span class="p">),</span>
                <span class="o">**</span><span class="n">one_hot_discrete</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># No skip timestep -&gt; Provide value at given index for this agent.</span>

        <span class="c1"># Special case: extra_model_outputs and key=None (return all keys as</span>
        <span class="c1"># a dict). Note that `inf_lookback_buffer` is NOT an infinite lookback</span>
        <span class="c1"># buffer, but a dict mapping keys to individual infinite lookback</span>
        <span class="c1"># buffers.</span>
        <span class="k">elif</span> <span class="n">what</span> <span class="o">==</span> <span class="s2">&quot;extra_model_outputs&quot;</span> <span class="ow">and</span> <span class="n">extra_model_outputs_key</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">hanging_val</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">hanging_val</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">if</span> <span class="n">inf_lookback_buffer</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">sub_buffer</span> <span class="ow">in</span> <span class="n">inf_lookback_buffer</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="n">ret</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">sub_buffer</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                        <span class="n">indices</span><span class="o">=</span><span class="n">index_incl_lookback</span> <span class="o">-</span> <span class="n">sub_buffer</span><span class="o">.</span><span class="n">lookback</span><span class="p">,</span>
                        <span class="n">neg_index_as_lookback</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">fill</span><span class="o">=</span><span class="n">fill</span><span class="p">,</span>
                        <span class="n">_add_last_ts_value</span><span class="o">=</span><span class="p">(</span>
                            <span class="kc">None</span> <span class="k">if</span> <span class="n">hanging_val</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">hanging_val</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
                        <span class="p">),</span>
                        <span class="o">**</span><span class="n">one_hot_discrete</span><span class="p">,</span>
                    <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">hanging_val</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                    <span class="n">ret</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">InfiniteLookbackBuffer</span><span class="p">()</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                        <span class="n">indices</span><span class="o">=</span><span class="n">index_incl_lookback</span><span class="p">,</span>
                        <span class="n">neg_index_as_lookback</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">fill</span><span class="o">=</span><span class="n">fill</span><span class="p">,</span>
                        <span class="n">_add_last_ts_value</span><span class="o">=</span><span class="n">hanging_val</span><span class="p">[</span><span class="n">key</span><span class="p">],</span>
                        <span class="o">**</span><span class="n">one_hot_discrete</span><span class="p">,</span>
                    <span class="p">)</span>
            <span class="k">return</span> <span class="n">ret</span>

        <span class="c1"># Extract data directly from the infinite lookback buffer object.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">inf_lookback_buffer</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                <span class="n">indices</span><span class="o">=</span><span class="n">index_incl_lookback</span> <span class="o">-</span> <span class="n">inf_lookback_buffer</span><span class="o">.</span><span class="n">lookback</span><span class="p">,</span>
                <span class="n">neg_index_as_lookback</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">fill</span><span class="o">=</span><span class="n">fill</span><span class="p">,</span>
                <span class="n">_add_last_ts_value</span><span class="o">=</span><span class="n">hanging_val</span><span class="p">,</span>
                <span class="o">**</span><span class="n">one_hot_discrete</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_single_agent_data_by_env_step_indices</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">what</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">agent_id</span><span class="p">:</span> <span class="n">AgentID</span><span class="p">,</span>
        <span class="n">indices_incl_lookback</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span>
        <span class="n">fill</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">one_hot_discrete</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">extra_model_outputs_key</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">hanging_val</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns single data item from the episode based on given (env step) indices.</span>

<span class="sd">        The returned data item will have a batch size that matches the env timesteps</span>
<span class="sd">        defined via `indices_incl_lookback`.</span>

<span class="sd">        Args:</span>
<span class="sd">            what: A (str) descriptor of what data to collect. Must be one of</span>
<span class="sd">                &quot;observations&quot;, &quot;infos&quot;, &quot;actions&quot;, &quot;rewards&quot;, or &quot;extra_model_outputs&quot;.</span>
<span class="sd">            indices_incl_lookback: A list of ints specifying, which indices</span>
<span class="sd">                to pull from the InfiniteLookbackBuffer defined by `agent_id` and `what`</span>
<span class="sd">                (and maybe `extra_model_outputs_key`). Note that these indices</span>
<span class="sd">                disregard the special logic of the lookback buffer. Meaning if one</span>
<span class="sd">                index in `indices_incl_lookback` is 0, then the first value in the</span>
<span class="sd">                lookback buffer should be returned, not the first value after the</span>
<span class="sd">                lookback buffer (which would be normal behavior for pulling items from</span>
<span class="sd">                an `InfiniteLookbackBuffer` object).</span>
<span class="sd">            agent_id: The individual agent ID to pull data for. Used to lookup the</span>
<span class="sd">                `SingleAgentEpisode` object for this agent in `self`.</span>
<span class="sd">            fill: An optional float value to use for filling up the returned results at</span>
<span class="sd">                the boundaries. This filling only happens if the requested index range&#39;s</span>
<span class="sd">                start/stop boundaries exceed the buffer&#39;s boundaries (including the</span>
<span class="sd">                lookback buffer on the left side). This comes in very handy, if users</span>
<span class="sd">                don&#39;t want to worry about reaching such boundaries and want to zero-pad.</span>
<span class="sd">                For example, a buffer with data [10, 11,  12, 13, 14] and lookback</span>
<span class="sd">                buffer size of 2 (meaning `10` and `11` are part of the lookback buffer)</span>
<span class="sd">                will respond to `indices_incl_lookback=[-1, -2, 0]` and `fill=0.0`</span>
<span class="sd">                with `[0.0, 0.0, 10]`.</span>
<span class="sd">            one_hot_discrete: If True, will return one-hot vectors (instead of</span>
<span class="sd">                int-values) for those sub-components of a (possibly complex) space</span>
<span class="sd">                that are Discrete or MultiDiscrete. Note that if `fill=0` and the</span>
<span class="sd">                requested `indices_incl_lookback` are out of the range of our data, the</span>
<span class="sd">                returned one-hot vectors will actually be zero-hot (all slots zero).</span>
<span class="sd">            extra_model_outputs_key: Only if what is &quot;extra_model_outputs&quot;, this</span>
<span class="sd">                specifies the sub-key (str) inside the extra_model_outputs dict, e.g.</span>
<span class="sd">                STATE_OUT or ACTION_DIST_INPUTS.</span>
<span class="sd">            hanging_val: In case we are pulling actions, rewards, or extra_model_outputs</span>
<span class="sd">                data, there might be information &quot;hanging&quot; (cached). For example,</span>
<span class="sd">                if an agent receives an observation o0 and then immediately sends an</span>
<span class="sd">                action a0 back, but then does NOT immediately reveive a next</span>
<span class="sd">                observation, a0 is now cached (not fully logged yet with this</span>
<span class="sd">                episode). The currently cached value must be provided here to be able</span>
<span class="sd">                to return it in case the index is -1 (most recent timestep).</span>

<span class="sd">        Returns:</span>
<span class="sd">            A data item corresponding to the provided args.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sa_episode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span>

        <span class="n">inf_lookback_buffer</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">sa_episode</span><span class="p">,</span> <span class="n">what</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">extra_model_outputs_key</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">inf_lookback_buffer</span> <span class="o">=</span> <span class="n">inf_lookback_buffer</span><span class="p">[</span><span class="n">extra_model_outputs_key</span><span class="p">]</span>

        <span class="c1"># If there are self.SKIP_ENV_TS_TAG items in `indices_incl_lookback` and user</span>
        <span class="c1"># wants to fill these (together with outside-episode-bounds indices) -&gt;</span>
        <span class="c1"># Provide these skipped timesteps as filled values.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">SKIP_ENV_TS_TAG</span> <span class="ow">in</span> <span class="n">indices_incl_lookback</span> <span class="ow">and</span> <span class="n">fill</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">single_fill_value</span> <span class="o">=</span> <span class="n">inf_lookback_buffer</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                <span class="n">indices</span><span class="o">=</span><span class="mi">1000000000000</span><span class="p">,</span>
                <span class="n">neg_index_as_lookback</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">fill</span><span class="o">=</span><span class="n">fill</span><span class="p">,</span>
                <span class="o">**</span><span class="n">one_hot_discrete</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices_incl_lookback</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">SKIP_ENV_TS_TAG</span><span class="p">:</span>
                    <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">single_fill_value</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">inf_lookback_buffer</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                            <span class="n">indices</span><span class="o">=</span><span class="n">i</span> <span class="o">-</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">sa_episode</span><span class="p">,</span> <span class="n">what</span><span class="p">)</span><span class="o">.</span><span class="n">lookback</span><span class="p">,</span>
                            <span class="n">neg_index_as_lookback</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                            <span class="n">fill</span><span class="o">=</span><span class="n">fill</span><span class="p">,</span>
                            <span class="n">_add_last_ts_value</span><span class="o">=</span><span class="n">hanging_val</span><span class="p">,</span>
                            <span class="o">**</span><span class="n">one_hot_discrete</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_numpy</span><span class="p">:</span>
                <span class="n">ret</span> <span class="o">=</span> <span class="n">batch</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Filter these indices out up front.</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">i</span> <span class="o">-</span> <span class="n">inf_lookback_buffer</span><span class="o">.</span><span class="n">lookback</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices_incl_lookback</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">SKIP_ENV_TS_TAG</span>
            <span class="p">]</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">inf_lookback_buffer</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span>
                <span class="n">neg_index_as_lookback</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">fill</span><span class="o">=</span><span class="n">fill</span><span class="p">,</span>
                <span class="n">_add_last_ts_value</span><span class="o">=</span><span class="n">hanging_val</span><span class="p">,</span>
                <span class="o">**</span><span class="n">one_hot_discrete</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">ret</span>

    <span class="k">def</span> <span class="nf">_get_hanging_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">what</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">agent_id</span><span class="p">:</span> <span class="n">AgentID</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the hanging action/reward/extra_model_outputs for given agent.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">what</span> <span class="o">==</span> <span class="s2">&quot;actions&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_actions_end</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">what</span> <span class="o">==</span> <span class="s2">&quot;extra_model_outputs&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_extra_model_outputs_end</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">what</span> <span class="o">==</span> <span class="s2">&quot;rewards&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_rewards_end</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">agent_id</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_copy_hanging</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">agent_id</span><span class="p">:</span> <span class="n">AgentID</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="s2">&quot;MultiAgentEpisode&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Copies hanging action, reward, extra_model_outputs from `other` to `self.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">agent_id</span> <span class="ow">in</span> <span class="n">other</span><span class="o">.</span><span class="n">_hanging_rewards_begin</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_rewards_begin</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">_hanging_rewards_begin</span><span class="p">[</span>
                <span class="n">agent_id</span>
            <span class="p">]</span>
        <span class="k">if</span> <span class="n">agent_id</span> <span class="ow">in</span> <span class="n">other</span><span class="o">.</span><span class="n">_hanging_rewards_end</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_actions_end</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span>
                <span class="n">other</span><span class="o">.</span><span class="n">_hanging_actions_end</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_rewards_end</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">_hanging_rewards_end</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_extra_model_outputs_end</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span>
                <span class="n">other</span><span class="o">.</span><span class="n">_hanging_extra_model_outputs_end</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_del_hanging</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">agent_id</span><span class="p">:</span> <span class="n">AgentID</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Deletes all hanging action, reward, extra_model_outputs of given agent.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_rewards_begin</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">agent_id</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_actions_end</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">agent_id</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_extra_model_outputs_end</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">agent_id</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hanging_rewards_end</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">agent_id</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_del_agent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">agent_id</span><span class="p">:</span> <span class="n">AgentID</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Deletes all data of given agent from this episode.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_del_hanging</span><span class="p">(</span><span class="n">agent_id</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">agent_id</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">agent_ids</span><span class="o">.</span><span class="n">discard</span><span class="p">(</span><span class="n">agent_id</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env_t_to_agent_t</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">agent_id</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_agent_to_module_mapping</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">agent_id</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">agent_t_started</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">agent_id</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_inf_lookback_buffer_or_dict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">agent_id</span><span class="p">:</span> <span class="n">AgentID</span><span class="p">,</span>
        <span class="n">what</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">extra_model_outputs_key</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">hanging_val</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">filter_for_skip_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns a single InfiniteLookbackBuffer or a dict of such.</span>

<span class="sd">        In case `what` is &quot;extra_model_outputs&quot; AND `extra_model_outputs_key` is None,</span>
<span class="sd">        a dict is returned. In all other cases, a single InfiniteLookbackBuffer is</span>
<span class="sd">        returned.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">inf_lookback_buffer_or_dict</span> <span class="o">=</span> <span class="n">inf_lookback_buffer</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">agent_episodes</span><span class="p">[</span><span class="n">agent_id</span><span class="p">],</span> <span class="n">what</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">what</span> <span class="o">==</span> <span class="s2">&quot;extra_model_outputs&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">extra_model_outputs_key</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">inf_lookback_buffer</span> <span class="o">=</span> <span class="n">inf_lookback_buffer_or_dict</span><span class="p">[</span>
                    <span class="n">extra_model_outputs_key</span>
                <span class="p">]</span>
            <span class="k">elif</span> <span class="n">inf_lookback_buffer_or_dict</span><span class="p">:</span>
                <span class="n">inf_lookback_buffer</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">inf_lookback_buffer_or_dict</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>
            <span class="k">elif</span> <span class="n">filter_for_skip_indices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">inf_lookback_buffer_or_dict</span><span class="p">,</span> <span class="n">filter_for_skip_indices</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">inf_lookback_buffer_or_dict</span>

        <span class="k">if</span> <span class="n">filter_for_skip_indices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">inf_lookback_buffer_len</span> <span class="o">=</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">inf_lookback_buffer</span><span class="p">)</span>
                <span class="o">+</span> <span class="n">inf_lookback_buffer</span><span class="o">.</span><span class="n">lookback</span>
                <span class="o">+</span> <span class="p">(</span><span class="n">hanging_val</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">ignore_last_ts</span> <span class="o">=</span> <span class="n">what</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;observations&quot;</span><span class="p">,</span> <span class="s2">&quot;infos&quot;</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">filter_for_skip_indices</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="n">filter_for_skip_indices</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="s2">&quot;S&quot;</span> <span class="k">if</span> <span class="n">ignore_last_ts</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">==</span> <span class="n">inf_lookback_buffer_len</span> <span class="k">else</span> <span class="n">i</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">filter_for_skip_indices</span>
                <span class="p">]</span>
            <span class="k">elif</span> <span class="n">ignore_last_ts</span> <span class="ow">and</span> <span class="n">filter_for_skip_indices</span> <span class="o">==</span> <span class="n">inf_lookback_buffer_len</span><span class="p">:</span>
                <span class="n">filter_for_skip_indices</span> <span class="o">=</span> <span class="s2">&quot;S&quot;</span>
            <span class="k">return</span> <span class="n">inf_lookback_buffer_or_dict</span><span class="p">,</span> <span class="n">filter_for_skip_indices</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">inf_lookback_buffer_or_dict</span>

    <span class="nd">@Deprecated</span><span class="p">(</span><span class="n">new</span><span class="o">=</span><span class="s2">&quot;MultiAgentEpisode.is_numpy()&quot;</span><span class="p">,</span> <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">is_finalized</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="nd">@Deprecated</span><span class="p">(</span><span class="n">new</span><span class="o">=</span><span class="s2">&quot;MultiAgentEpisode.to_numpy()&quot;</span><span class="p">,</span> <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">finalize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span></div>

</pre></div>

                </article>
              
              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="footer-content-items footer-content__inner">
  
    <div class="footer-content-item"><div id="csat">
  <div id="csat-feedback-received" class="csat-hidden">
    <span>Thanks for the feedback!</span>
  </div>
  <div id="csat-inputs">
    <span>Was this helpful?</span>
    <div id="csat-yes" class="csat-button">
      <svg id="csat-yes-icon" class="csat-hidden csat-icon" width="18" height="13" viewBox="0 0 18 13" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 10.172L16.1922 0.979004L17.6072 2.393L7.00023 13L0.63623 6.636L2.05023 5.222L7.00023 10.172Z" />
      </svg>
      <span>Yes<span>
    </div>
    <div id="csat-no" class="csat-button">
      <svg id="csat-no-icon" class="csat-hidden csat-icon" width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 5.58599L11.9502 0.635986L13.3642 2.04999L8.41423 6.99999L13.3642 11.95L11.9502 13.364L7.00023 8.41399L2.05023 13.364L0.63623 11.95L5.58623 6.99999L0.63623 2.04999L2.05023 0.635986L7.00023 5.58599Z" />
      </svg>
      <span>No<span>
    </div>
  </div>
  <div id="csat-textarea-group" class="csat-hidden">
    <span id="csat-feedback-label">Feedback</span>
    <textarea id="csat-textarea"></textarea>
    <div id="csat-submit">Submit</div>
  </div>
</div></div>
  
</div>

          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794"></script>
<script src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
  <p class="copyright">
    
      Â© Copyright 2025, The Ray Team.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">
  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.3.7.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.14.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>