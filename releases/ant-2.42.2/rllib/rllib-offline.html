<!-- prettier-ignore -->

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Working with offline data &#8212; Ray 2.42.2</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=ac02cc09edc035673794" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/termynal.css?v=2fc3cb5e" />
    <link rel="stylesheet" type="text/css" href="../_static/css/csat.css?v=c8f39c76" />
    <link rel="stylesheet" type="text/css" href="../_static/css/assistant.css?v=d345c55e" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=9b1aa6b9" />
    <link rel="stylesheet" type="text/css" href="../_static/docsearch.css?v=45fb5152" />
    <link rel="stylesheet" type="text/css" href="../_static/pydata-docsearch-custom.css?v=7fb92720" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=ac02cc09edc035673794"></script>

    <script src="../_static/documentation_options.js?v=725a9ddb"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=9a29e97e"></script>
    <script defer="defer" src="../_static/js/termynal.js?v=67cfcf08"></script>
    <script defer="defer" src="../_static/js/custom.js?v=9e3b357f"></script>
    <script src="../_static/js/csat.js?v=8e649b1c"></script>
    <script defer="defer" src="../_static/js/assistant.js?v=73fdc522"></script>
    <script defer="defer" src="../_static/docsearch.js?v=77274085"></script>
    <script defer="defer" src="../_static/docsearch_config.js?v=d25523ed"></script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'rllib/rllib-offline';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.14.1';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://antgroup.github.io/ant-ray/versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'releases/ant-2.42.2';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = false;
        </script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/rllib/rllib-offline.html" />
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="next" title="RL Modules" href="rllib-rlmodule.html" />
    <link rel="prev" title="Replay Buffers" href="rllib-replay-buffers.html" /><!-- Extra header to include at the top of each template.
Kept separately so that it can easily be included in any templates
that need to be overridden for individual pages; e.g. included
both in the usual template (layout.html) as well as (index.html). -->

<link rel="preconnect" href="https://fonts.googleapis.com" />
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
<link
  href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;900&family=Roboto:wght@400;500;700&display=swap"
  rel="stylesheet"
/>
<link
  rel="stylesheet"
  title="light"
  href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css"
  disabled="disabled"
/>
<link
  rel="stylesheet"
  title="dark"
  href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css"
  disabled="disabled"
/>
<link
  href="https://cdn.jsdelivr.net/npm/remixicon@4.1.0/fonts/remixicon.css"
  rel="stylesheet"
/>

<!-- Used for text embedded in html on the index page  -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- Parser used to call hljs on responses from Ray Assistant -->
<script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

<!-- Sanitizer for Ray Assistant AI -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/dompurify/2.3.3/purify.min.js"></script>

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<!-- Google Tag Manager -->
<script>
  (function (w, d, s, l, i) {
    w[l] = w[l] || [];
    w[l].push({
      'gtm.start': new Date().getTime(),
      event: 'gtm.js',
    });
    var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s),
      dl = l != 'dataLayer' ? '&l=' + l : '';
    j.async = true;
    j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
    f.parentNode.insertBefore(j, f);
  })(window, document, 'script', 'dataLayer', 'GTM-P8H6KQG');
</script>
<!-- End Google Tag Manager -->
<!-- Data to be shared with JS on every page -->
<script>
  window.data = {
      copyIconSrc: "../_static/copy-button.svg"
  };
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>


  <div class="bd-header-announcement container-fluid bd-header-announcement">
    <div class="bd-header-announcement__content">Influence the future of Ray with our <a target="_blank" href="https://www.surveymonkey.com/r/RayPulse2025?utm_source=ray_docs&utm_medium=website&utm_campaign=banner">Ray Community Pulse survey</a>. Complete it by Monday, January 27th, 2025 to get exclusive swag for eligible participants.</div>
  </div>

  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  

<a class="navbar-brand logo" href="../index.html">
  <svg
   id="svg"
   version="1.1"
   viewBox="0, 0, 400,200.13540961408262"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg">
  <g id="g1">
    <path id="ray-text" d="m 102.875,36.782 c 0.16,0.041 0.423,0.041 0.583,0 0.161,-0.042 0.029,-0.077 -0.291,-0.077 -0.321,0 -0.452,0.035 -0.292,0.077 m 89.958,63.206 v 30.345 h 3.584 3.583 v -9.487 -9.487 l 9.958,-0.059 c 10.822,-0.064 10.056,-0.121 10.664,0.804 0.113,0.172 0.394,0.597 0.625,0.945 0.231,0.348 0.42,0.693 0.42,0.766 0,0.073 0.15,0.269 0.333,0.435 0.183,0.166 0.333,0.358 0.333,0.427 0,0.069 0.366,0.676 0.814,1.349 0.925,1.393 2.687,4.088 2.936,4.493 0.092,0.149 0.312,0.486 0.489,0.751 1.348,2.013 2.261,3.416 2.261,3.474 0,0.038 0.338,0.561 0.75,1.164 0.413,0.602 0.755,1.151 0.761,1.219 0.006,0.067 0.267,0.46 0.58,0.873 0.313,0.412 0.692,0.975 0.843,1.25 0.622,1.138 0.378,1.083 4.846,1.083 h 4.062 l -0.254,-0.458 c -0.139,-0.252 -0.677,-1.058 -1.196,-1.792 -1.001,-1.416 -1.194,-1.694 -5.308,-7.666 -1.452,-2.109 -2.684,-3.871 -2.737,-3.917 -0.053,-0.046 -0.172,-0.213 -0.263,-0.372 -0.326,-0.563 -2.842,-4.226 -3.541,-5.154 -0.969,-1.287 -1.025,-1.162 0.832,-1.838 0.726,-0.263 2.976,-1.446 3.305,-1.737 0.145,-0.128 0.319,-0.232 0.388,-0.232 0.287,0 2.659,-2.095 3.515,-3.105 3.686,-4.345 5.22,-10.506 4.352,-17.484 -0.442,-3.559 -1.381,-5.956 -3.555,-9.078 -0.491,-0.706 -3.235,-3.333 -3.481,-3.333 -0.128,0 -0.232,-0.062 -0.232,-0.136 0,-0.075 -0.319,-0.313 -0.708,-0.53 -0.39,-0.216 -0.746,-0.439 -0.792,-0.496 -0.077,-0.096 -1.012,-0.546 -2.532,-1.218 -1.038,-0.459 -3.173,-1.141 -4.051,-1.293 -0.459,-0.08 -1.359,-0.273 -2,-0.429 -1.04,-0.253 -2.711,-0.292 -15.375,-0.353 l -14.209,-0.07 v 30.346 m 80.935,-28.446 c -0.448,1.031 -1.004,2.287 -1.235,2.791 -0.396,0.861 -0.579,1.27 -1.363,3.038 -0.403,0.911 -1.14,2.528 -1.903,4.177 -0.33,0.714 -0.6,1.373 -0.6,1.464 0,0.092 -0.107,0.333 -0.238,0.536 -0.13,0.203 -0.661,1.344 -1.179,2.535 -0.519,1.192 -1.085,2.467 -1.258,2.834 -0.464,0.979 -1.328,2.9 -2.324,5.166 -1.025,2.334 -1.477,3.34 -1.759,3.917 -0.113,0.229 -1.423,3.154 -2.913,6.5 -1.49,3.346 -2.951,6.608 -3.248,7.25 -1.302,2.818 -8.248,18.378 -8.248,18.475 0,0.06 1.714,0.108 3.808,0.108 h 3.809 l 1.04,-2.375 c 0.573,-1.306 1.191,-2.712 1.374,-3.125 0.183,-0.412 0.483,-1.087 0.665,-1.5 0.183,-0.412 0.467,-1.05 0.63,-1.416 0.164,-0.367 0.656,-1.492 1.093,-2.5 0.437,-1.009 0.93,-2.134 1.097,-2.5 0.166,-0.367 0.399,-0.911 0.517,-1.209 l 0.215,-0.541 h 16.448 16.447 l 0.369,0.791 c 0.203,0.436 0.464,1.017 0.581,1.292 0.209,0.494 1.042,2.363 1.485,3.333 0.21,0.46 2.5,5.679 2.998,6.834 0.118,0.275 0.406,0.912 0.638,1.416 0.232,0.504 0.465,1.048 0.516,1.209 0.085,0.267 0.426,0.291 4.003,0.291 h 3.91 l -0.137,-0.458 c -0.122,-0.41 -1.026,-2.462 -2.331,-5.292 -0.254,-0.55 -1.531,-3.4 -2.838,-6.333 -1.307,-2.933 -2.618,-5.858 -2.913,-6.5 -0.412,-0.894 -5.399,-12.071 -8.292,-18.583 -0.183,-0.413 -0.473,-1.05 -0.644,-1.417 -0.324,-0.694 -1.063,-2.353 -1.56,-3.5 -0.159,-0.367 -0.427,-0.967 -0.595,-1.333 -0.867,-1.885 -2.251,-4.97 -2.916,-6.5 -0.195,-0.45 -0.423,-0.958 -1.421,-3.167 -0.166,-0.367 -0.502,-1.117 -0.747,-1.667 -0.63,-1.413 -1.07,-2.395 -1.419,-3.166 -0.652,-1.443 -0.97,-2.18 -1.059,-2.459 -0.086,-0.266 -0.421,-0.291 -3.891,-0.291 h -3.798 l -0.814,1.875 M 306.5,69.745 c 0,0.096 0.45,0.894 1.132,2.005 0.281,0.458 0.75,1.283 1.042,1.833 0.293,0.55 0.625,1.113 0.738,1.25 0.114,0.138 0.649,1.029 1.189,1.982 0.54,0.952 1.147,2.002 1.348,2.333 0.201,0.331 0.669,1.127 1.04,1.769 0.372,0.641 0.874,1.504 1.117,1.916 0.452,0.769 0.799,1.371 1.408,2.446 0.19,0.337 0.586,0.993 0.878,1.458 0.293,0.466 0.625,1.034 0.738,1.263 0.113,0.229 0.393,0.717 0.622,1.083 0.229,0.367 0.735,1.218 1.124,1.891 0.389,0.673 0.857,1.477 1.041,1.787 0.183,0.31 0.775,1.334 1.316,2.276 0.541,0.942 1.066,1.817 1.167,1.944 0.101,0.127 0.446,0.704 0.767,1.282 0.32,0.579 0.948,1.675 1.393,2.436 0.446,0.762 0.963,1.647 1.149,1.968 0.186,0.32 0.476,0.808 0.644,1.083 0.168,0.275 0.497,0.837 0.73,1.25 0.234,0.412 0.563,0.975 0.731,1.25 0.168,0.275 0.456,0.763 0.64,1.083 0.183,0.321 0.607,1.046 0.94,1.612 l 0.606,1.028 v 10.18 10.18 h 3.583 3.584 v -10.55 c 0,-6.074 0.064,-10.617 0.151,-10.708 0.083,-0.087 0.32,-0.458 0.528,-0.825 0.869,-1.535 1.109,-1.954 1.881,-3.277 1.098,-1.882 1.383,-2.373 1.773,-3.056 0.183,-0.321 0.473,-0.827 0.644,-1.125 0.171,-0.298 0.587,-1.029 0.926,-1.625 0.338,-0.596 0.683,-1.196 0.765,-1.334 0.428,-0.714 1.259,-2.124 1.424,-2.416 0.186,-0.329 0.612,-1.07 1.22,-2.125 0.172,-0.298 0.634,-1.104 1.027,-1.792 0.393,-0.687 0.815,-1.368 0.938,-1.513 0.122,-0.145 0.223,-0.389 0.223,-0.542 0,-0.153 0.061,-0.278 0.137,-0.278 0.075,0 0.302,-0.319 0.503,-0.709 0.202,-0.389 0.551,-1.008 0.775,-1.375 0.225,-0.366 0.551,-0.929 0.724,-1.25 0.63,-1.167 2.234,-3.927 2.52,-4.336 0.161,-0.23 0.547,-0.889 0.858,-1.462 0.312,-0.574 0.732,-1.323 0.935,-1.664 0.203,-0.342 0.561,-0.959 0.796,-1.371 0.234,-0.413 0.585,-1.013 0.779,-1.333 0.325,-0.538 0.934,-1.61 1.374,-2.417 0.1,-0.183 0.267,-0.446 0.371,-0.583 0.103,-0.138 0.425,-0.7 0.715,-1.25 0.29,-0.55 0.647,-1.171 0.793,-1.379 l 0.265,-0.378 -3.801,0.045 -3.801,0.045 -0.3,0.5 c -0.165,0.275 -0.455,0.762 -0.644,1.083 -0.189,0.321 -0.565,0.921 -0.835,1.334 -0.27,0.412 -0.569,0.9 -0.664,1.083 -0.096,0.183 -0.395,0.671 -0.665,1.083 -0.27,0.413 -0.645,1.013 -0.834,1.334 -0.189,0.32 -0.49,0.808 -0.668,1.083 -0.178,0.275 -0.479,0.762 -0.669,1.083 -0.19,0.321 -0.712,1.184 -1.159,1.917 -0.448,0.733 -1.086,1.783 -1.417,2.333 -0.332,0.55 -0.824,1.338 -1.093,1.75 -0.27,0.413 -0.542,0.863 -0.605,1 -0.064,0.138 -0.427,0.738 -0.807,1.334 -0.38,0.596 -0.834,1.308 -1.009,1.583 -0.175,0.275 -0.402,0.65 -0.503,0.833 -0.102,0.184 -0.696,1.159 -1.321,2.167 -0.625,1.008 -1.316,2.133 -1.536,2.5 -0.219,0.367 -0.584,0.967 -0.81,1.333 -0.226,0.367 -0.461,0.777 -0.523,0.911 -0.061,0.134 -0.42,0.696 -0.797,1.25 -0.378,0.553 -0.737,1.118 -0.799,1.256 -0.061,0.138 -0.3,0.55 -0.531,0.917 -0.23,0.366 -0.507,0.837 -0.615,1.046 -0.292,0.563 -0.932,-0.03 -1.58,-1.463 -0.063,-0.138 -0.27,-0.473 -0.461,-0.747 -0.39,-0.556 -0.504,-0.738 -1.098,-1.753 -0.559,-0.954 -0.693,-1.171 -1.084,-1.754 -0.183,-0.274 -0.422,-0.647 -0.531,-0.83 -0.109,-0.183 -0.695,-1.12 -1.302,-2.083 -0.607,-0.962 -1.193,-1.897 -1.302,-2.077 -0.534,-0.884 -1.693,-2.723 -1.97,-3.124 -0.171,-0.249 -0.311,-0.495 -0.311,-0.549 0,-0.053 -0.141,-0.3 -0.312,-0.548 C 323.21,84.6 322,82.674 322,82.582 c 0,-0.053 -0.139,-0.299 -0.309,-0.547 -0.761,-1.11 -1.663,-2.54 -1.81,-2.868 -0.061,-0.138 -0.435,-0.724 -0.83,-1.303 -0.395,-0.578 -0.718,-1.088 -0.718,-1.133 0,-0.044 -0.3,-0.532 -0.666,-1.084 C 317.3,75.095 317,74.611 317,74.571 c 0,-0.039 -0.356,-0.628 -0.792,-1.308 -0.435,-0.681 -1.12,-1.768 -1.522,-2.417 l -0.731,-1.179 h -3.727 c -2.051,0 -3.728,0.035 -3.728,0.078 m -86.75,7.023 c 3.486,0.433 8.055,2.022 8.389,2.917 0.03,0.081 0.138,0.148 0.239,0.148 0.278,0 2.413,2.27 2.792,2.969 0.184,0.338 0.399,0.727 0.48,0.865 2.258,3.865 1.774,11.495 -0.942,14.836 -0.206,0.254 -0.375,0.504 -0.375,0.556 0,0.13 -1.451,1.504 -2.196,2.079 -1.372,1.059 -4.274,2.252 -6.804,2.797 -1.265,0.272 -2.482,0.311 -11.375,0.359 L 200,104.348 V 90.424 76.5 l 8.792,-0.001 c 7.089,0 9.211,0.052 10.958,0.269 m 58.745,1.979 c 0.003,0.09 0.235,0.653 0.516,1.25 0.281,0.598 0.662,1.424 0.846,1.836 0.185,0.413 0.48,1.05 0.656,1.417 0.177,0.367 0.321,0.719 0.321,0.783 -10e-4,0.101 0.437,1.075 1.82,4.05 0.17,0.367 1.189,2.654 2.263,5.084 1.075,2.429 2.18,4.904 2.456,5.5 0.276,0.596 0.76,1.683 1.075,2.416 0.581,1.349 0.792,1.826 1.478,3.334 0.209,0.458 0.542,1.208 0.74,1.666 0.198,0.459 0.498,1.115 0.666,1.459 l 0.307,0.625 h -13.32 c -8.524,0 -13.319,-0.059 -13.319,-0.163 0,-0.089 0.164,-0.52 0.365,-0.958 0.622,-1.356 1.332,-2.938 1.572,-3.504 0.126,-0.298 0.337,-0.786 0.469,-1.084 0.131,-0.298 0.814,-1.854 1.517,-3.458 0.704,-1.604 1.411,-3.217 1.573,-3.583 0.161,-0.367 0.406,-0.93 0.544,-1.25 0.137,-0.321 0.794,-1.821 1.459,-3.334 1.615,-3.672 1.781,-4.049 2.154,-4.875 0.175,-0.389 0.674,-1.533 1.108,-2.541 0.937,-2.177 1.685,-3.833 2.006,-4.443 0.128,-0.243 0.233,-0.576 0.233,-0.74 0,-0.333 0.484,0.168 0.495,0.513" />
    <path id="ray-logo" d="m 101.476,36.871 c -1.303,0.145 -3.55,0.666 -4.332,1.004 -2.044,0.884 -2.472,1.092 -3.144,1.529 -7.177,4.661 -9.961,13.497 -6.746,21.409 0.465,1.144 0.582,1.352 1.707,3.057 0.986,1.494 3.928,4.463 4.424,4.463 0.054,0 0.429,0.223 0.832,0.496 4.639,3.13 12.64,3.283 17.395,0.332 0.457,-0.284 22.892,21.99 22.486,22.327 -0.218,0.181 -0.803,1.491 -1.437,3.22 l -0.352,0.959 h -6.238 c -5.795,0 -6.238,-0.021 -6.238,-0.288 0,-0.977 -1.759,-4.418 -3.151,-6.165 -4.027,-5.051 -11.482,-7.68 -17.226,-6.075 -0.711,0.198 -1.405,0.361 -1.542,0.361 -0.745,0 -4.681,2.22 -5.914,3.336 -0.321,0.291 -0.792,0.706 -1.046,0.923 -0.855,0.728 -2.606,3.23 -3.29,4.701 -0.372,0.801 -0.749,1.587 -0.837,1.746 -0.088,0.159 -0.16,0.434 -0.16,0.611 0,0.888 0.287,0.85 -6.415,0.85 H 74.04 l -0.198,-0.542 c -0.108,-0.298 -0.347,-0.917 -0.531,-1.375 -0.183,-0.458 -0.358,-0.946 -0.389,-1.083 -0.087,-0.39 -1.399,-2.503 -2.064,-3.324 -2.725,-3.366 -5.769,-5.225 -10.441,-6.377 -3.309,-0.816 -10.96,0.695 -12.419,2.453 -0.113,0.136 -0.278,0.248 -0.367,0.248 -0.392,0 -4.298,3.657 -4.298,4.024 0,0.086 -0.131,0.297 -0.291,0.469 -2.793,2.998 -3.936,11.297 -2.141,15.549 0.844,1.999 1.072,2.472 1.583,3.291 0.314,0.504 0.634,0.942 0.711,0.972 0.076,0.031 0.138,0.154 0.138,0.274 0,0.246 2.689,3.043 3.334,3.468 0.229,0.151 0.716,0.486 1.083,0.744 6.843,4.811 18.162,3.018 23.001,-3.643 0.093,-0.128 0.412,-0.542 0.709,-0.922 0.297,-0.38 0.54,-0.747 0.54,-0.815 0,-0.069 0.224,-0.489 0.498,-0.935 0.275,-0.445 0.461,-0.81 0.414,-0.81 -0.046,0 0.104,-0.372 0.335,-0.827 0.231,-0.456 0.42,-0.94 0.42,-1.076 0,-0.136 0.087,-0.439 0.193,-0.672 l 0.194,-0.425 h 6.22 6.22 l 0.101,0.459 c 1.063,4.828 6.623,10.565 11.322,11.682 0.366,0.087 1.154,0.288 1.75,0.446 7.975,2.118 18.533,-4.211 20.101,-12.05 l 0.107,-0.537 h 6.217 6.217 l 0.352,0.959 c 0.642,1.751 1.22,3.04 1.447,3.228 0.178,0.148 -2.094,2.503 -10.941,11.344 l -11.165,11.158 -0.793,-0.404 c -1.29,-0.658 -1.561,-0.769 -2.959,-1.21 -5.239,-1.651 -12.178,-0.296 -16.032,3.131 -1.412,1.256 -2.907,2.908 -3.508,3.878 -4.539,7.323 -3.445,16.326 2.69,22.138 0.992,0.939 3.39,2.578 4.35,2.972 1.875,0.771 2.482,1.005 2.729,1.053 0.149,0.03 0.849,0.174 1.556,0.32 10.388,2.155 20.37,-6.164 20.38,-16.983 0.002,-3.118 -0.379,-4.767 -1.788,-7.728 l -0.505,-1.061 11.107,-11.106 11.107,-11.105 0.873,0.447 c 1.444,0.739 3.341,1.372 5.068,1.691 1.606,0.297 5.042,0.225 6.39,-0.134 0.595,-0.159 1.37,-0.358 1.72,-0.442 1.198,-0.288 4.129,-1.822 5.32,-2.786 9.711,-7.857 8.716,-22.425 -1.957,-28.665 -3.504,-2.048 -8.663,-2.872 -12.167,-1.942 -0.55,0.146 -1.262,0.327 -1.583,0.403 -0.614,0.144 -1.774,0.627 -2.958,1.232 l -0.709,0.362 -11.105,-11.105 -11.105,-11.106 0.289,-0.561 c 3.009,-5.83 2.687,-12.491 -0.861,-17.825 -0.257,-0.386 -0.543,-0.821 -0.634,-0.965 -1.398,-2.203 -6.344,-5.524 -9.084,-6.099 -3.096,-0.651 -4.424,-0.766 -6.357,-0.551 m 4.095,8.912 c 6.101,1.712 8.316,9.521 3.993,14.078 -6.028,6.355 -16.205,1.325 -14.858,-7.345 0.559,-3.597 3.168,-6.047 7.544,-7.084 0.546,-0.13 2.249,0.05 3.321,0.351 M 59.946,91.812 c 2.314,0.806 4.061,2.37 5.152,4.615 l 0.644,1.323 v 2.25 2.25 l -0.648,1.332 c -3.748,7.711 -14.726,6.286 -16.362,-2.124 -1.223,-6.284 5.142,-11.76 11.214,-9.646 M 105,91.512 c 7.468,1.695 9.27,11.549 2.87,15.694 -7.325,4.746 -16.184,-3.082 -12.434,-10.986 0.56,-1.179 2.473,-3.387 2.935,-3.387 0.102,0 0.312,-0.113 0.465,-0.252 1.043,-0.944 4.259,-1.502 6.164,-1.069 m 46.441,0.171 c 4.417,1.35 7.025,5.54 6.148,9.877 -0.306,1.514 -0.863,2.879 -1.336,3.271 -0.139,0.116 -0.253,0.306 -0.253,0.424 0,0.307 -1.541,1.65 -2.554,2.224 -7.406,4.199 -15.743,-3.631 -12.051,-11.318 0.752,-1.565 3.286,-3.994 4.167,-3.994 0.112,0 0.276,-0.069 0.363,-0.153 0.713,-0.687 3.754,-0.87 5.516,-0.331 m -44.774,46.358 c 8.432,4.162 5.806,16.556 -3.5,16.523 C 100.595,154.555 96,152.103 96,150.74 c 0,-0.11 -0.106,-0.287 -0.236,-0.395 -1.065,-0.884 -1.517,-5.232 -0.754,-7.262 1.796,-4.781 7.334,-7.176 11.657,-5.042" />
  </g>
</svg>

</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <div class="navbar-content docutils container">
<ul class="navbar-toplevel">
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../ray-overview/getting-started.html" title="Get Started"><span class="navbar-link-title">Get Started</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../ray-overview/use-cases.html" title="Use Cases"><span class="navbar-link-title">Use Cases</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../ray-overview/examples.html" title="Example Gallery"><span class="navbar-link-title">Example Gallery</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../ray-overview/installation.html" title="Library"><span class="navbar-link-title">Library</span></a></p>
<i class="fa-solid fa-chevron-down"></i></div>
<div class="navbar-dropdown docutils container">
<ul class="navbar-sublevel">
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../ray-core/walkthrough.html" title="Ray Core"><span class="navbar-link-title">Ray Core</span>Scale general Python applications</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../data/data.html" title="Ray Data"><span class="navbar-link-title">Ray Data</span>Scale data ingest and preprocessing</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../train/train.html" title="Ray Train"><span class="navbar-link-title">Ray Train</span>Scale machine learning training</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../tune/index.html" title="Ray Tune"><span class="navbar-link-title">Ray Tune</span>Scale hyperparameter tuning</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../serve/index.html" title="Ray Serve"><span class="navbar-link-title">Ray Serve</span>Scale model serving</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="index.html" title="Ray RLlib"><span class="navbar-link-title">Ray RLlib</span>Scale reinforcement learning</a></p>
</div>
</li>
</ul>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../index.html" title="Docs"><span class="navbar-link-title">Docs</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://discuss.ray.io" title="Resources"><span class="navbar-link-title">Resources</span></a></p>
<i class="fa-solid fa-chevron-down"></i></div>
<div class="navbar-dropdown docutils container">
<ul class="navbar-sublevel">
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://discuss.ray.io" title="Discussion Forum"><span class="navbar-link-title">Discussion Forum</span>Get your Ray questions answered</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://github.com/ray-project/ray-educational-materials" title="Training"><span class="navbar-link-title">Training</span>Hands-on learning</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/blog" title="Blog"><span class="navbar-link-title">Blog</span>Updates, best practices, user-stories</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/events" title="Events"><span class="navbar-link-title">Events</span>Webinars, meetups, office hours</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/blog/how-ray-and-anyscale-make-it-easy-to-do-massive-scale-machine-learning-on" title="Success Stories"><span class="navbar-link-title">Success Stories</span>Real-world workload examples</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../ray-overview/ray-libraries.html" title="Ecosystem"><span class="navbar-link-title">Ecosystem</span>Libraries integrated with Ray</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.ray.io/community" title="Community"><span class="navbar-link-title">Community</span>Connect with us</a></p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>

</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          <div id="docsearch"></div>
        </div>
      
      
        <div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
      
        <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="versionswitcherbutton" type="button" role="button" class="version-switcher__button btn btn-sm navbar-btn dropdown-toggle" data-bs-toggle="dropdown" aria-haspopup="listbox" aria-controls="versionswitcherlist" aria-label="Version switcher list">
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="versionswitcherlist" class="version-switcher__menu dropdown-menu list-group-flush py-0" role="listbox" aria-labelledby="versionswitcherbutton">
    <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
      
        <div class="navbar-item"><a
  id="try-anyscale-href"
  href="https://www.anyscale.com/ray-on-anyscale/?utm_source=ray_docs&utm_medium=docs&utm_campaign=navbar"
>
  <div id="try-anyscale-text">
    <span>Try Ray on Anyscale</span>
  </div>
</a></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile"><div id="docsearch"></div>
    </div>
  

  
    <label class="sidebar-toggle secondary-toggle" for="__secondary" tabindex="0">
      <span class="fa-solid fa-outdent"></span>
    </label>
  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <div class="navbar-content docutils container">
<ul class="navbar-toplevel">
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../ray-overview/getting-started.html" title="Get Started"><span class="navbar-link-title">Get Started</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../ray-overview/use-cases.html" title="Use Cases"><span class="navbar-link-title">Use Cases</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../ray-overview/examples.html" title="Example Gallery"><span class="navbar-link-title">Example Gallery</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../ray-overview/installation.html" title="Library"><span class="navbar-link-title">Library</span></a></p>
<i class="fa-solid fa-chevron-down"></i></div>
<div class="navbar-dropdown docutils container">
<ul class="navbar-sublevel">
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../ray-core/walkthrough.html" title="Ray Core"><span class="navbar-link-title">Ray Core</span>Scale general Python applications</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../data/data.html" title="Ray Data"><span class="navbar-link-title">Ray Data</span>Scale data ingest and preprocessing</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../train/train.html" title="Ray Train"><span class="navbar-link-title">Ray Train</span>Scale machine learning training</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../tune/index.html" title="Ray Tune"><span class="navbar-link-title">Ray Tune</span>Scale hyperparameter tuning</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../serve/index.html" title="Ray Serve"><span class="navbar-link-title">Ray Serve</span>Scale model serving</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="index.html" title="Ray RLlib"><span class="navbar-link-title">Ray RLlib</span>Scale reinforcement learning</a></p>
</div>
</li>
</ul>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../index.html" title="Docs"><span class="navbar-link-title">Docs</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://discuss.ray.io" title="Resources"><span class="navbar-link-title">Resources</span></a></p>
<i class="fa-solid fa-chevron-down"></i></div>
<div class="navbar-dropdown docutils container">
<ul class="navbar-sublevel">
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://discuss.ray.io" title="Discussion Forum"><span class="navbar-link-title">Discussion Forum</span>Get your Ray questions answered</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://github.com/ray-project/ray-educational-materials" title="Training"><span class="navbar-link-title">Training</span>Hands-on learning</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/blog" title="Blog"><span class="navbar-link-title">Blog</span>Updates, best practices, user-stories</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/events" title="Events"><span class="navbar-link-title">Events</span>Webinars, meetups, office hours</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/blog/how-ray-and-anyscale-make-it-easy-to-do-massive-scale-machine-learning-on" title="Success Stories"><span class="navbar-link-title">Success Stories</span>Real-world workload examples</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../ray-overview/ray-libraries.html" title="Ecosystem"><span class="navbar-link-title">Ecosystem</span>Libraries integrated with Ray</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.ray.io/community" title="Community"><span class="navbar-link-title">Community</span>Connect with us</a></p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>

</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
        
          <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="versionswitcherbutton" type="button" role="button" class="version-switcher__button btn btn-sm navbar-btn dropdown-toggle" data-bs-toggle="dropdown" aria-haspopup="listbox" aria-controls="versionswitcherlist" aria-label="Version switcher list">
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="versionswitcherlist" class="version-switcher__menu dropdown-menu list-group-flush py-0" role="listbox" aria-labelledby="versionswitcherbutton">
    <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
        
          <div class="navbar-item"><a
  id="try-anyscale-href"
  href="https://www.anyscale.com/ray-on-anyscale/?utm_source=ray_docs&utm_medium=docs&utm_campaign=navbar"
>
  <div id="try-anyscale-text">
    <span>Try Ray on Anyscale</span>
  </div>
</a></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item"><nav id="main-sidebar" class="bd-docs-nav bd-links" aria-label="Section Navigation">
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../ray-overview/index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-overview/getting-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-overview/installation.html">Installation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../ray-overview/use-cases.html">Use Cases</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../ray-air/getting-started.html">Ray for ML Infrastructure</a></li>

</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../ray-overview/examples.html">Example Gallery</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-overview/ray-libraries.html">Ecosystem</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../ray-core/walkthrough.html">Ray Core</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../ray-core/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../ray-core/user-guide.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../ray-core/tasks.html">Tasks</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/tasks/nested-tasks.html">Nested Remote Functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/tasks/generators.html">Dynamic generators</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../ray-core/actors.html">Actors</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/actors/named-actors.html">Named Actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/actors/terminating-actors.html">Terminating Actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/actors/async_api.html">AsyncIO / Concurrency for Actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/actors/concurrency_group_api.html">Limiting Concurrency Per-Method with Concurrency Groups</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/actors/actor-utils.html">Utility Classes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/actors/out-of-band-communication.html">Out-of-band Communication</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/actors/task-orders.html">Actor Task Execution Order</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../ray-core/objects.html">Objects</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/objects/serialization.html">Serialization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/objects/object-spilling.html">Object Spilling</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../ray-core/handling-dependencies.html">Environment Dependencies</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../ray-core/scheduling/index.html">Scheduling</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/scheduling/resources.html">Resources</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/scheduling/accelerators.html">Accelerator Support</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/scheduling/placement-group.html">Placement Groups</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/scheduling/memory-management.html">Memory Management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/scheduling/ray-oom-prevention.html">Out-Of-Memory Prevention</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../ray-core/fault-tolerance.html">Fault Tolerance</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/fault_tolerance/tasks.html">Task Fault Tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/fault_tolerance/actors.html">Actor Fault Tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/fault_tolerance/objects.html">Object Fault Tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/fault_tolerance/nodes.html">Node Fault Tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/fault_tolerance/gcs.html">GCS Fault Tolerance</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../ray-core/patterns/index.html">Design Patterns &amp; Anti-patterns</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/patterns/nested-tasks.html">Pattern: Using nested tasks to achieve nested parallelism</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/patterns/generators.html">Pattern: Using generators to reduce heap memory usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/patterns/limit-pending-tasks.html">Pattern: Using ray.wait to limit the number of pending tasks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/patterns/limit-running-tasks.html">Pattern: Using resources to limit the number of concurrently running tasks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/patterns/concurrent-operations-async-actor.html">Pattern: Using asyncio to run actor methods concurrently</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/patterns/actor-sync.html">Pattern: Using an actor to synchronize other tasks and actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/patterns/tree-of-actors.html">Pattern: Using a supervisor actor to manage a tree of actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/patterns/pipelining.html">Pattern: Using pipelining to increase throughput</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/patterns/return-ray-put.html">Anti-pattern: Returning ray.put() ObjectRefs from a task harms performance and fault tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/patterns/ray-get-loop.html">Anti-pattern: Calling ray.get in a loop harms parallelism</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/patterns/unnecessary-ray-get.html">Anti-pattern: Calling ray.get unnecessarily harms performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/patterns/ray-get-submission-order.html">Anti-pattern: Processing results in submission order using ray.get increases runtime</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/patterns/ray-get-too-many-objects.html">Anti-pattern: Fetching too many objects at once with ray.get causes failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/patterns/too-fine-grained-tasks.html">Anti-pattern: Over-parallelizing with too fine-grained tasks harms speedup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/patterns/redefine-task-actor-loop.html">Anti-pattern: Redefining the same remote function or class harms performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/patterns/pass-large-arg-by-value.html">Anti-pattern: Passing the same large argument by value repeatedly harms performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/patterns/closure-capture-large-objects.html">Anti-pattern: Closure capturing large objects harms performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/patterns/global-variables.html">Anti-pattern: Using global variables to share state between tasks and actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/patterns/out-of-band-object-ref-serialization.html">Anti-pattern: Serialize ray.ObjectRef out of band</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../ray-core/compiled-graph/ray-compiled-graph.html">Ray Compiled Graph</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/compiled-graph/quickstart.html">Quickstart</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/compiled-graph/execution.html">Execution and failure semantics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/compiled-graph/visualization.html">Visualization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/compiled-graph/profiling.html">Profiling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/compiled-graph/overlap.html">Overlap communication and computation</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../ray-core/advanced-topics.html">Advanced Topics</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/tips-for-first-time.html">Tips for first-time users</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/starting-ray.html">Starting Ray</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/ray-generator.html">Ray Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/namespaces.html">Using Namespaces</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/cross-language.html">Cross-Language Programming</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/using-ray-with-jupyter.html">Working with Jupyter Notebooks &amp; JupyterLab</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/ray-dag.html">Lazy Computation Graphs with the Ray DAG API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/miscellaneous.html">Miscellaneous Topics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/runtime_env_auth.html">Authenticating Remote URIs in runtime_env</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-core/user-spawn-processes.html">Lifetimes of a User-Spawn Process</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../ray-core/examples/overview.html">Examples</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../ray-core/examples/automl_for_time_series.html">Simple AutoML for time series with Ray Core</a></li>

<li class="toctree-l3"><a class="reference internal" href="../ray-core/examples/batch_prediction.html">Batch Prediction with Ray Core</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-core/examples/gentle_walkthrough.html">A Gentle Introduction to Ray Core by Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-core/examples/highly_parallel.html">Using Ray for Highly Parallelizable Tasks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-core/examples/map_reduce.html">A Simple MapReduce Example with Ray Core</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-core/examples/monte_carlo_pi.html">Monte Carlo Estimation of π</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-core/examples/plot_hyperparameter.html">Simple Parallel Model Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-core/examples/plot_parameter_server.html">Parameter Server</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-core/examples/plot_pong_example.html">Learning to Play Pong</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-core/examples/web-crawler.html">Speed up your web crawler by parallelizing it with Ray</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../ray-core/api/index.html">Ray Core API</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../ray-core/api/core.html">Core API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-core/api/scheduling.html">Scheduling API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-core/api/runtime-env.html">Runtime Env API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-core/api/utility.html">Utility</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-core/api/exceptions.html">Exceptions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-core/api/cli.html">Ray Core CLI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-observability/reference/cli.html">State CLI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-observability/reference/api.html">State API</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../data/data.html">Ray Data</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../data/overview.html">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data/quickstart.html">Quickstart</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../data/user-guide.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../data/loading-data.html">Loading Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/inspecting-data.html">Inspecting Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/transforming-data.html">Transforming Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/iterating-over-data.html">Iterating over Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/shuffling-data.html">Shuffling Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/saving-data.html">Saving Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/working-with-images.html">Working with Images</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/working-with-text.html">Working with Text</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/working-with-tensors.html">Working with Tensors / NumPy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/working-with-pytorch.html">Working with PyTorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/monitoring-your-workload.html">Monitoring Your Workload</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/execution-configurations.html">Execution Configurations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/batch_inference.html">End-to-end: Offline Batch Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/performance-tips.html">Advanced: Performance Tips and Tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/custom-datasource-example.html">Advanced: Read and Write Custom File Types</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../data/examples.html">Examples</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../data/api/api.html">Ray Data API</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../data/api/input_output.html">Input/Output</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/api/dataset.html">Dataset API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/api/data_iterator.html">DataIterator API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/api/execution_options.html">ExecutionOptions API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/api/grouped_data.html">GroupedData API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/api/data_context.html">Global configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/api/preprocessor.html">Preprocessor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/api/from_other_data_libs.html">API Guide for Users from Other Data Libraries</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../data/data-internals.html">Ray Data Internals</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../train/train.html">Ray Train</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../train/overview.html">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../train/getting-started-pytorch.html">PyTorch Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../train/getting-started-pytorch-lightning.html">PyTorch Lightning Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../train/getting-started-transformers.html">Hugging Face Transformers Guide</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../train/more-frameworks.html">More Frameworks</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../train/huggingface-accelerate.html">Hugging Face Accelerate Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../train/deepspeed.html">DeepSpeed Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../train/distributed-tensorflow-keras.html">TensorFlow and Keras Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../train/distributed-xgboost-lightgbm.html">XGBoost and LightGBM Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../train/horovod.html">Horovod Guide</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../train/user-guides.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../train/user-guides/data-loading-preprocessing.html">Data Loading and Preprocessing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../train/user-guides/using-gpus.html">Configuring Scale and GPUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../train/user-guides/persistent-storage.html">Configuring Persistent Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../train/user-guides/monitoring-logging.html">Monitoring and Logging Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../train/user-guides/checkpoints.html">Saving and Loading Checkpoints</a></li>
<li class="toctree-l3"><a class="reference internal" href="../train/user-guides/experiment-tracking.html">Experiment Tracking</a></li>
<li class="toctree-l3"><a class="reference internal" href="../train/user-guides/results.html">Inspecting Training Results</a></li>
<li class="toctree-l3"><a class="reference internal" href="../train/user-guides/fault-tolerance.html">Handling Failures and Node Preemption</a></li>
<li class="toctree-l3"><a class="reference internal" href="../train/user-guides/reproducibility.html">Reproducibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="../train/user-guides/hyperparameter-optimization.html">Hyperparameter Optimization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../train/examples.html">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../train/benchmarks.html">Benchmarks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../train/api/api.html">Ray Train API</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../tune/index.html">Ray Tune</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-20"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../tune/getting-started.html">Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tune/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../tune/tutorials/overview.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-21"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../tune/tutorials/tune-run.html">Running Basic Experiments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tune/tutorials/tune-output.html">Logging and Outputs in Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tune/tutorials/tune-resources.html">Setting Trial Resources</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tune/tutorials/tune-search-spaces.html">Using Search Spaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tune/tutorials/tune-stopping.html">How to Define Stopping Criteria for a Ray Tune Experiment</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tune/tutorials/tune-trial-checkpoints.html">How to Save and Load Trial Checkpoints</a></li>

<li class="toctree-l3"><a class="reference internal" href="../tune/tutorials/tune-storage.html">How to Configure Persistent Storage in Ray Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tune/tutorials/tune-fault-tolerance.html">How to Enable Fault Tolerance in Ray Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tune/tutorials/tune-metrics.html">Using Callbacks and Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tune/tutorials/tune_get_data_in_and_out.html">Getting Data in and out of Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tune/examples/tune_analyze_results.html">Analyzing Tune Experiment Results</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../tune/examples/pbt_guide.html">A Guide to Population Based Training with Tune</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-22"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../tune/examples/pbt_visualization/pbt_visualization.html">Visualizing and Understanding PBT</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../tune/tutorials/tune-distributed.html">Deploying Tune in the Cloud</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tune/tutorials/tune-lifecycle.html">Tune Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tune/tutorials/tune-scalability.html">Scalability Benchmarks</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../tune/examples/index.html">Ray Tune Examples</a><input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-23"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../tune/examples/ml-frameworks.html">Examples using Ray Tune with ML Frameworks</a><input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-24"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../tune/examples/tune_mnist_keras.html">Keras Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../tune/examples/tune-pytorch-cifar.html">PyTorch Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../tune/examples/tune-pytorch-lightning.html">PyTorch Lightning Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../tune/examples/pbt_ppo_example.html">Ray RLlib Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../tune/examples/tune-xgboost.html">XGBoost Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../tune/examples/lightgbm_example.html">LightGBM Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../tune/examples/horovod_simple.html">Horovod Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../tune/examples/pbt_transformers.html">Hugging Face Transformers Example</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../tune/examples/experiment-tracking.html">Tune Experiment Tracking Examples</a><input class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-25"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../tune/examples/tune-wandb.html">Weights &amp; Biases Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../tune/examples/tune-mlflow.html">MLflow Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../tune/examples/tune-aim.html">Aim Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../tune/examples/tune-comet.html">Comet Example</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../tune/examples/hpo-frameworks.html">Tune Hyperparameter Optimization Framework Examples</a><input class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-26"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../tune/examples/ax_example.html">Ax Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../tune/examples/hyperopt_example.html">HyperOpt Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../tune/examples/bayesopt_example.html">Bayesopt Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../tune/examples/bohb_example.html">BOHB Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../tune/examples/nevergrad_example.html">Nevergrad Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../tune/examples/optuna_example.html">Optuna Example</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../tune/examples/other-examples.html">Other Examples</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tune/examples/exercises.html">Exercises</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tune/faq.html">Ray Tune FAQ</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../tune/api/api.html">Ray Tune API</a><input class="toctree-checkbox" id="toctree-checkbox-27" name="toctree-checkbox-27" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-27"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../tune/api/execution.html">Tune Execution (tune.Tuner)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tune/api/result_grid.html">Tune Experiment Results (tune.ResultGrid)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tune/api/trainable.html">Training in Tune (tune.Trainable, train.report)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tune/api/search_space.html">Tune Search Space API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tune/api/suggestion.html">Tune Search Algorithms (tune.search)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tune/api/schedulers.html">Tune Trial Schedulers (tune.schedulers)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tune/api/stoppers.html">Tune Stopping Mechanisms (tune.stopper)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tune/api/reporters.html">Tune Console Output (Reporters)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tune/api/syncing.html">Syncing in Tune (train.SyncConfig)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tune/api/logging.html">Tune Loggers (tune.logger)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tune/api/callbacks.html">Tune Callbacks (tune.Callback)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tune/api/env.html">Environment variables used by Ray Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tune/api/integration.html">External library integrations for Ray Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tune/api/internals.html">Tune Internals</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tune/api/cli.html">Tune CLI (Experimental)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../serve/index.html">Ray Serve</a><input class="toctree-checkbox" id="toctree-checkbox-28" name="toctree-checkbox-28" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-28"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../serve/getting_started.html">Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="../serve/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../serve/develop-and-deploy.html">Develop and Deploy an ML Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="../serve/model_composition.html">Deploy Compositions of Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../serve/multi-app.html">Deploy Multiple Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../serve/model-multiplexing.html">Model Multiplexing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../serve/configure-serve-deployment.html">Configure Ray Serve deployments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../serve/http-guide.html">Set Up FastAPI and HTTP</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../serve/production-guide/index.html">Production Guide</a><input class="toctree-checkbox" id="toctree-checkbox-29" name="toctree-checkbox-29" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-29"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../serve/production-guide/config.html">Serve Config Files</a></li>
<li class="toctree-l3"><a class="reference internal" href="../serve/production-guide/kubernetes.html">Deploy on Kubernetes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../serve/production-guide/docker.html">Custom Docker Images</a></li>
<li class="toctree-l3"><a class="reference internal" href="../serve/production-guide/fault-tolerance.html">Add End-to-End Fault Tolerance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../serve/production-guide/handling-dependencies.html">Handle Dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../serve/production-guide/best-practices.html">Best practices in production</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../serve/monitoring.html">Monitor Your Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="../serve/resource-allocation.html">Resource Allocation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../serve/autoscaling-guide.html">Ray Serve Autoscaling</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../serve/advanced-guides/index.html">Advanced Guides</a><input class="toctree-checkbox" id="toctree-checkbox-30" name="toctree-checkbox-30" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-30"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../serve/advanced-guides/app-builder-guide.html">Pass Arguments to Applications</a></li>
<li class="toctree-l3"><a class="reference internal" href="../serve/advanced-guides/advanced-autoscaling.html">Advanced Ray Serve Autoscaling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../serve/advanced-guides/performance.html">Performance Tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../serve/advanced-guides/dyn-req-batch.html">Dynamic Request Batching</a></li>
<li class="toctree-l3"><a class="reference internal" href="../serve/advanced-guides/inplace-updates.html">Updating Applications In-Place</a></li>
<li class="toctree-l3"><a class="reference internal" href="../serve/advanced-guides/dev-workflow.html">Development Workflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../serve/advanced-guides/grpc-guide.html">Set Up a gRPC Service</a></li>
<li class="toctree-l3"><a class="reference internal" href="../serve/advanced-guides/managing-java-deployments.html">Experimental Java API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../serve/advanced-guides/deploy-vm.html">Deploy on VM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../serve/advanced-guides/multi-app-container.html">Run Multiple Applications in Different Containers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../serve/architecture.html">Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../serve/examples.html">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../serve/api/index.html">Ray Serve API</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="index.html">Ray RLlib</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-31" name="toctree-checkbox-31" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-31"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="rllib-training.html">Getting Started with RLlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="key-concepts.html">Key concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="rllib-env.html">Environments</a><input class="toctree-checkbox" id="toctree-checkbox-32" name="toctree-checkbox-32" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-32"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="multi-agent-envs.html">Multi-Agent Environments</a></li>
<li class="toctree-l3"><a class="reference internal" href="hierarchical-envs.html">Hierarchical Environments</a></li>
<li class="toctree-l3"><a class="reference internal" href="external-envs.html">External Environments and Applications</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="algorithm-config.html">AlgorithmConfig API</a></li>
<li class="toctree-l2"><a class="reference internal" href="rllib-algorithms.html">Algorithms</a></li>
<li class="toctree-l2 current active has-children"><a class="reference internal" href="user-guides.html">User Guides</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-33" name="toctree-checkbox-33" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-33"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="rllib-advanced-api.html">Advanced Python APIs</a></li>
<li class="toctree-l3"><a class="reference internal" href="rllib-callback.html">Callbacks</a></li>
<li class="toctree-l3"><a class="reference internal" href="rllib-models.html">Models, Preprocessors, and Action Distributions</a></li>
<li class="toctree-l3"><a class="reference internal" href="checkpoints.html">Checkpointing</a></li>
<li class="toctree-l3"><a class="reference internal" href="metrics-logger.html">MetricsLogger API</a></li>
<li class="toctree-l3"><a class="reference internal" href="single-agent-episode.html">Episodes</a></li>

<li class="toctree-l3"><a class="reference internal" href="rllib-replay-buffers.html">Replay Buffers</a></li>
<li class="toctree-l3 current active"><a class="current reference internal" href="#">Working with offline data</a></li>
<li class="toctree-l3"><a class="reference internal" href="rllib-rlmodule.html">RL Modules</a></li>
<li class="toctree-l3"><a class="reference internal" href="rllib-learner.html">Learner (Alpha)</a></li>



<li class="toctree-l3"><a class="reference internal" href="rllib-torch2x.html">Using RLlib with torch 2.x compile</a></li>

<li class="toctree-l3"><a class="reference internal" href="rllib-fault-tolerance.html">Fault Tolerance And Elastic Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="rllib-dev.html">Install RLlib for Development</a></li>




<li class="toctree-l3"><a class="reference internal" href="scaling-guide.html">RLlib scaling guide</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="rllib-examples.html">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="new-api-stack-migration-guide.html">New API stack migration guide</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="package_ref/index.html">Ray RLlib API</a><input class="toctree-checkbox" id="toctree-checkbox-34" name="toctree-checkbox-34" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-34"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="package_ref/algorithm-config.html">Algorithm Configuration API</a><input class="toctree-checkbox" id="toctree-checkbox-35" name="toctree-checkbox-35" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-35"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_algo.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_algo</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner_group.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner_group</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.is_multi_agent.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.is_multi_agent</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.is_offline.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.is_offline</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learner_class.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learner_class</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.model_config.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.model_config</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.rl_module_spec.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.rl_module_spec</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.total_train_batch_size.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.total_train_batch_size</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_learner_class.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_learner_class</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_rl_module_spec.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_rl_module_spec</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_evaluation_config_object.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_evaluation_config_object</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_multi_rl_module_spec.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_multi_rl_module_spec</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_multi_agent_setup.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_multi_agent_setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_rollout_fragment_length.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_rollout_fragment_length</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.copy.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.copy</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.validate.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.validate</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.freeze.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.freeze</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="package_ref/algorithm.html">Algorithms</a><input class="toctree-checkbox" id="toctree-checkbox-36" name="toctree-checkbox-36" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-36"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.html">ray.rllib.algorithms.algorithm.Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.setup.html">ray.rllib.algorithms.algorithm.Algorithm.setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_default_config.html">ray.rllib.algorithms.algorithm.Algorithm.get_default_config</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.env_runner.html">ray.rllib.algorithms.algorithm.Algorithm.env_runner</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.eval_env_runner.html">ray.rllib.algorithms.algorithm.Algorithm.eval_env_runner</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.train.html">ray.rllib.algorithms.algorithm.Algorithm.train</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.training_step.html">ray.rllib.algorithms.algorithm.Algorithm.training_step</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.save_to_path.html">ray.rllib.algorithms.algorithm.Algorithm.save_to_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.restore_from_path.html">ray.rllib.algorithms.algorithm.Algorithm.restore_from_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.from_checkpoint.html">ray.rllib.algorithms.algorithm.Algorithm.from_checkpoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_state.html">ray.rllib.algorithms.algorithm.Algorithm.get_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.set_state.html">ray.rllib.algorithms.algorithm.Algorithm.set_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.evaluate.html">ray.rllib.algorithms.algorithm.Algorithm.evaluate</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_module.html">ray.rllib.algorithms.algorithm.Algorithm.get_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.add_policy.html">ray.rllib.algorithms.algorithm.Algorithm.add_policy</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.remove_policy.html">ray.rllib.algorithms.algorithm.Algorithm.remove_policy</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="package_ref/callback.html">Callback APIs</a><input class="toctree-checkbox" id="toctree-checkbox-37" name="toctree-checkbox-37" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-37"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.html">ray.rllib.callbacks.callbacks.RLlibCallback</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_algorithm_init.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_algorithm_init</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_sample_end.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_sample_end</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_train_result.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_train_result</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_evaluate_start.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_evaluate_start</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_evaluate_end.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_evaluate_end</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_env_runners_recreated.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_env_runners_recreated</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_checkpoint_loaded.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_checkpoint_loaded</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_environment_created.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_environment_created</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_created.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_created</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_start.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_start</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_step.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_step</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_end.html">ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_end</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="package_ref/env.html">Environments</a><input class="toctree-checkbox" id="toctree-checkbox-38" name="toctree-checkbox-38" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-38"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="package_ref/env/env_runner.html">EnvRunner API</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/env/single_agent_env_runner.html">SingleAgentEnvRunner API</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/env/single_agent_episode.html">SingleAgentEpisode API</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/env/multi_agent_env.html">MultiAgentEnv API</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/env/multi_agent_env_runner.html">MultiAgentEnvRunner API</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/env/multi_agent_episode.html">MultiAgentEpisode API</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/env/utils.html">Env Utils</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="package_ref/rl_modules.html">RLModule APIs</a><input class="toctree-checkbox" id="toctree-checkbox-39" name="toctree-checkbox-39" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-39"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.build.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.build</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec.build.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec.build</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.default_model_config.DefaultModelConfig.html">ray.rllib.core.rl_module.default_model_config.DefaultModelConfig</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.html">ray.rllib.core.rl_module.rl_module.RLModule</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.observation_space.html">ray.rllib.core.rl_module.rl_module.RLModule.observation_space</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.action_space.html">ray.rllib.core.rl_module.rl_module.RLModule.action_space</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.inference_only.html">ray.rllib.core.rl_module.rl_module.RLModule.inference_only</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.model_config.html">ray.rllib.core.rl_module.rl_module.RLModule.model_config</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.setup.html">ray.rllib.core.rl_module.rl_module.RLModule.setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.as_multi_rl_module.html">ray.rllib.core.rl_module.rl_module.RLModule.as_multi_rl_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.forward_exploration.html">ray.rllib.core.rl_module.rl_module.RLModule.forward_exploration</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.forward_inference.html">ray.rllib.core.rl_module.rl_module.RLModule.forward_inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.forward_train.html">ray.rllib.core.rl_module.rl_module.RLModule.forward_train</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule._forward.html">ray.rllib.core.rl_module.rl_module.RLModule._forward</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule._forward_exploration.html">ray.rllib.core.rl_module.rl_module.RLModule._forward_exploration</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule._forward_inference.html">ray.rllib.core.rl_module.rl_module.RLModule._forward_inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule._forward_train.html">ray.rllib.core.rl_module.rl_module.RLModule._forward_train</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.save_to_path.html">ray.rllib.core.rl_module.rl_module.RLModule.save_to_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.restore_from_path.html">ray.rllib.core.rl_module.rl_module.RLModule.restore_from_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.from_checkpoint.html">ray.rllib.core.rl_module.rl_module.RLModule.from_checkpoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.get_state.html">ray.rllib.core.rl_module.rl_module.RLModule.get_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.set_state.html">ray.rllib.core.rl_module.rl_module.RLModule.set_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.setup.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.as_multi_rl_module.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.as_multi_rl_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.add_module.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.add_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.remove_module.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.remove_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.save_to_path.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.save_to_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.restore_from_path.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.restore_from_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.from_checkpoint.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.from_checkpoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.get_state.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.get_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.set_state.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.set_state</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="package_ref/distributions.html">Distribution API</a><input class="toctree-checkbox" id="toctree-checkbox-40" name="toctree-checkbox-40" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-40"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.models.distributions.Distribution.html">ray.rllib.models.distributions.Distribution</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="package_ref/learner.html">LearnerGroup API</a><input class="toctree-checkbox" id="toctree-checkbox-41" name="toctree-checkbox-41" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-41"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learners.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learners</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner_group.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner_group</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.core.learner.learner_group.LearnerGroup.html">ray.rllib.core.learner.learner_group.LearnerGroup</a></li>
</ul>
</li>

<li class="toctree-l3 has-children"><a class="reference internal" href="package_ref/offline.html">Offline RL API</a><input class="toctree-checkbox" id="toctree-checkbox-42" name="toctree-checkbox-42" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-42"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.offline_data.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.offline_data</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learners.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learners</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.env_runners.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.env_runners</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner.html">ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_data.OfflineData.html">ray.rllib.offline.offline_data.OfflineData</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_data.OfflineData.__init__.html">ray.rllib.offline.offline_data.OfflineData.__init__</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_data.OfflineData.sample.html">ray.rllib.offline.offline_data.OfflineData.sample</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_data.OfflineData.default_map_batches_kwargs.html">ray.rllib.offline.offline_data.OfflineData.default_map_batches_kwargs</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_data.OfflineData.default_iter_batches_kwargs.html">ray.rllib.offline.offline_data.OfflineData.default_iter_batches_kwargs</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.__init__.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner.__init__</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_prelearner.SCHEMA.html">ray.rllib.offline.offline_prelearner.SCHEMA</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.__call__.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner.__call__</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner._map_to_episodes.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner._map_to_episodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner._map_sample_batch_to_episode.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner._map_sample_batch_to_episode</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner._should_module_be_updated.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner._should_module_be_updated</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.default_prelearner_buffer_class.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner.default_prelearner_buffer_class</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.default_prelearner_buffer_kwargs.html">ray.rllib.offline.offline_prelearner.OfflinePreLearner.default_prelearner_buffer_kwargs</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="package_ref/replay-buffers.html">Replay Buffer API</a><input class="toctree-checkbox" id="toctree-checkbox-43" name="toctree-checkbox-43" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-43"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.StorageUnit.html">ray.rllib.utils.replay_buffers.replay_buffer.StorageUnit</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.replay_buffers.prioritized_replay_buffer.PrioritizedReplayBuffer.html">ray.rllib.utils.replay_buffers.prioritized_replay_buffer.PrioritizedReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.replay_buffers.reservoir_replay_buffer.ReservoirReplayBuffer.html">ray.rllib.utils.replay_buffers.reservoir_replay_buffer.ReservoirReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.sample.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.sample</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.add.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.add</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.get_state.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.get_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.set_state.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.set_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.replay_buffers.multi_agent_replay_buffer.MultiAgentReplayBuffer.html">ray.rllib.utils.replay_buffers.multi_agent_replay_buffer.MultiAgentReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.replay_buffers.multi_agent_prioritized_replay_buffer.MultiAgentPrioritizedReplayBuffer.html">ray.rllib.utils.replay_buffers.multi_agent_prioritized_replay_buffer.MultiAgentPrioritizedReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.replay_buffers.utils.update_priorities_in_replay_buffer.html">ray.rllib.utils.replay_buffers.utils.update_priorities_in_replay_buffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.replay_buffers.utils.sample_min_n_steps_from_buffer.html">ray.rllib.utils.replay_buffers.utils.sample_min_n_steps_from_buffer</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="package_ref/utils.html">RLlib Utilities</a><input class="toctree-checkbox" id="toctree-checkbox-44" name="toctree-checkbox-44" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-44"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.peek.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger.peek</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_value.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_value</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_dict.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_dict</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.merge_and_log_n_dicts.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger.merge_and_log_n_dicts</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_time.html">ray.rllib.utils.metrics.metrics_logger.MetricsLogger.log_time</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.schedules.scheduler.Scheduler.html">ray.rllib.utils.schedules.scheduler.Scheduler</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.schedules.scheduler.Scheduler.validate.html">ray.rllib.utils.schedules.scheduler.Scheduler.validate</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.schedules.scheduler.Scheduler.get_current_value.html">ray.rllib.utils.schedules.scheduler.Scheduler.get_current_value</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.schedules.scheduler.Scheduler.update.html">ray.rllib.utils.schedules.scheduler.Scheduler.update</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.schedules.scheduler.Scheduler._create_tensor_variable.html">ray.rllib.utils.schedules.scheduler.Scheduler._create_tensor_variable</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.framework.try_import_torch.html">ray.rllib.utils.framework.try_import_torch</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.torch_utils.clip_gradients.html">ray.rllib.utils.torch_utils.clip_gradients</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.torch_utils.compute_global_norm.html">ray.rllib.utils.torch_utils.compute_global_norm</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.torch_utils.convert_to_torch_tensor.html">ray.rllib.utils.torch_utils.convert_to_torch_tensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.torch_utils.explained_variance.html">ray.rllib.utils.torch_utils.explained_variance</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.torch_utils.flatten_inputs_to_1d_tensor.html">ray.rllib.utils.torch_utils.flatten_inputs_to_1d_tensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.torch_utils.global_norm.html">ray.rllib.utils.torch_utils.global_norm</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.torch_utils.one_hot.html">ray.rllib.utils.torch_utils.one_hot</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.torch_utils.reduce_mean_ignore_inf.html">ray.rllib.utils.torch_utils.reduce_mean_ignore_inf</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.torch_utils.sequence_mask.html">ray.rllib.utils.torch_utils.sequence_mask</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.torch_utils.set_torch_seed.html">ray.rllib.utils.torch_utils.set_torch_seed</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.torch_utils.softmax_cross_entropy_with_logits.html">ray.rllib.utils.torch_utils.softmax_cross_entropy_with_logits</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.torch_utils.update_target_network.html">ray.rllib.utils.torch_utils.update_target_network</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.numpy.aligned_array.html">ray.rllib.utils.numpy.aligned_array</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.numpy.concat_aligned.html">ray.rllib.utils.numpy.concat_aligned</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.numpy.convert_to_numpy.html">ray.rllib.utils.numpy.convert_to_numpy</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.numpy.fc.html">ray.rllib.utils.numpy.fc</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.numpy.flatten_inputs_to_1d_tensor.html">ray.rllib.utils.numpy.flatten_inputs_to_1d_tensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.numpy.make_action_immutable.html">ray.rllib.utils.numpy.make_action_immutable</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.numpy.huber_loss.html">ray.rllib.utils.numpy.huber_loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.numpy.l2_loss.html">ray.rllib.utils.numpy.l2_loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.numpy.lstm.html">ray.rllib.utils.numpy.lstm</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.numpy.one_hot.html">ray.rllib.utils.numpy.one_hot</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.numpy.relu.html">ray.rllib.utils.numpy.relu</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.numpy.sigmoid.html">ray.rllib.utils.numpy.sigmoid</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.numpy.softmax.html">ray.rllib.utils.numpy.softmax</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.checkpoints.try_import_msgpack.html">ray.rllib.utils.checkpoints.try_import_msgpack</a></li>
<li class="toctree-l4"><a class="reference internal" href="package_ref/doc/ray.rllib.utils.checkpoints.Checkpointable.html">ray.rllib.utils.checkpoints.Checkpointable</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../ray-more-libs/index.html">More Libraries</a><input class="toctree-checkbox" id="toctree-checkbox-45" name="toctree-checkbox-45" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-45"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../ray-more-libs/joblib.html">Distributed Scikit-learn / Joblib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ray-more-libs/multiprocessing.html">Distributed multiprocessing.Pool</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ray-more-libs/ray-collective.html">Ray Collective Communication Lib</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../ray-more-libs/dask-on-ray.html">Using Dask on Ray</a><input class="toctree-checkbox" id="toctree-checkbox-46" name="toctree-checkbox-46" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-46"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../ray-more-libs/doc/ray.util.dask.RayDaskCallback.html">ray.util.dask.RayDaskCallback</a><input class="toctree-checkbox" id="toctree-checkbox-47" name="toctree-checkbox-47" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-47"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../ray-more-libs/doc/ray.util.dask.RayDaskCallback.ray_active.html">ray.util.dask.RayDaskCallback.ray_active</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_presubmit.html">ray.util.dask.callbacks.RayDaskCallback._ray_presubmit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_postsubmit.html">ray.util.dask.callbacks.RayDaskCallback._ray_postsubmit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_pretask.html">ray.util.dask.callbacks.RayDaskCallback._ray_pretask</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_posttask.html">ray.util.dask.callbacks.RayDaskCallback._ray_posttask</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_postsubmit_all.html">ray.util.dask.callbacks.RayDaskCallback._ray_postsubmit_all</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_finish.html">ray.util.dask.callbacks.RayDaskCallback._ray_finish</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../ray-more-libs/raydp.html">Using Spark on Ray (RayDP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ray-more-libs/mars-on-ray.html">Using Mars on Ray</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ray-more-libs/modin/index.html">Using Pandas on Ray (Modin)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../workflows/index.html">Ray Workflows (Alpha)</a><input class="toctree-checkbox" id="toctree-checkbox-48" name="toctree-checkbox-48" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-48"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../workflows/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../workflows/basics.html">Getting Started</a></li>
<li class="toctree-l3"><a class="reference internal" href="../workflows/management.html">Workflow Management</a></li>
<li class="toctree-l3"><a class="reference internal" href="../workflows/metadata.html">Workflow Metadata</a></li>
<li class="toctree-l3"><a class="reference internal" href="../workflows/events.html">Events</a></li>
<li class="toctree-l3"><a class="reference internal" href="../workflows/comparison.html">API Comparisons</a></li>
<li class="toctree-l3"><a class="reference internal" href="../workflows/advanced.html">Advanced Topics</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../workflows/api/api.html">Ray Workflows API</a><input class="toctree-checkbox" id="toctree-checkbox-49" name="toctree-checkbox-49" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-49"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../workflows/api/execution.html">Workflow Execution API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../workflows/api/management.html">Workflow Management API</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../cluster/getting-started.html">Ray Clusters</a><input class="toctree-checkbox" id="toctree-checkbox-50" name="toctree-checkbox-50" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-50"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../cluster/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../cluster/kubernetes/index.html">Deploying on Kubernetes</a><input class="toctree-checkbox" id="toctree-checkbox-51" name="toctree-checkbox-51" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-51"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../cluster/kubernetes/getting-started.html">Getting Started with KubeRay</a><input class="toctree-checkbox" id="toctree-checkbox-52" name="toctree-checkbox-52" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-52"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/getting-started/raycluster-quick-start.html">RayCluster Quickstart</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/getting-started/rayjob-quick-start.html">RayJob Quickstart</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/getting-started/rayservice-quick-start.html">RayService Quickstart</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../cluster/kubernetes/user-guides.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-53" name="toctree-checkbox-53" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-53"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/user-guides/rayservice.html">Deploy Ray Serve Apps</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/user-guides/rayservice-high-availability.html">RayService high availability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/user-guides/observability.html">KubeRay Observability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/user-guides/upgrade-guide.html">KubeRay upgrade guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/user-guides/k8s-cluster-setup.html">Managed Kubernetes services</a></li>



<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/user-guides/storage.html">Best Practices for Storage and Dependencies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/user-guides/config.html">RayCluster Configuration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/user-guides/configuring-autoscaling.html">KubeRay Autoscaling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/user-guides/kuberay-gcs-ft.html">GCS fault tolerance in KubeRay</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/user-guides/gke-gcs-bucket.html">Configuring KubeRay to use Google Cloud Storage Buckets in GKE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/user-guides/persist-kuberay-custom-resource-logs.html">Persist KubeRay custom resource logs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/user-guides/persist-kuberay-operator-logs.html">Persist KubeRay Operator Logs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/user-guides/gpu.html">Using GPUs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/user-guides/tpu.html">Use TPUs with KubeRay</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/user-guides/rayserve-dev-doc.html">Developing Ray Serve Python scripts on a RayCluster</a></li>









<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/user-guides/pod-command.html">Specify container commands for Ray head/worker Pods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/user-guides/pod-security.html">Pod Security</a></li>






<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/user-guides/helm-chart-rbac.html">Helm Chart RBAC</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/user-guides/tls.html">TLS Authentication</a></li>






<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/user-guides/k8s-autoscaler.html">(Advanced) Understanding the Ray Autoscaler in the Context of Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.html">(Advanced) Deploying a static Ray cluster without KubeRay</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/user-guides/kubectl-plugin.html">Use kubectl Plugin (alpha)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/user-guides/kuberay-auth.html">Configure Ray clusters with authentication and access control using KubeRay</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../cluster/kubernetes/examples.html">Examples</a><input class="toctree-checkbox" id="toctree-checkbox-54" name="toctree-checkbox-54" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-54"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/examples/ml-example.html">Ray Train XGBoostTrainer on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/examples/gpu-training-example.html">Train PyTorch ResNet model with GPUs on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/examples/mnist-training-example.html">Train a PyTorch model on Fashion MNIST with CPUs on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/examples/stable-diffusion-rayservice.html">Serve a StableDiffusion text-to-image model on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/examples/tpu-serve-stable-diffusion.html">Serve a Stable Diffusion model on GKE with TPUs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/examples/mobilenet-rayservice.html">Serve a MobileNet image classifier on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/examples/text-summarizer-rayservice.html">Serve a text summarizer on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/examples/rayjob-batch-inference-example.html">RayJob Batch Inference Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/examples/rayjob-kueue-priority-scheduling.html">Priority Scheduling with RayJob and Kueue</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/examples/rayjob-kueue-gang-scheduling.html">Gang Scheduling with RayJob and Kueue</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/examples/distributed-checkpointing-with-gcsfuse.html">Distributed checkpointing with KubeRay and GCSFuse</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/examples/modin-example.html">Use Modin with Ray on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/examples/vllm-rayservice.html">Serve a Large Language Model with vLLM on Kubernetes</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../cluster/kubernetes/k8s-ecosystem.html">KubeRay Ecosystem</a><input class="toctree-checkbox" id="toctree-checkbox-55" name="toctree-checkbox-55" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-55"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/k8s-ecosystem/ingress.html">Ingress</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/k8s-ecosystem/prometheus-grafana.html">Using Prometheus and Grafana</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/k8s-ecosystem/pyspy.html">Profiling with py-spy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/k8s-ecosystem/volcano.html">KubeRay integration with Volcano</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/k8s-ecosystem/yunikorn.html">KubeRay integration with Apache YuniKorn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/k8s-ecosystem/kubeflow.html">Kubeflow: an interactive development solution</a></li>


<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/k8s-ecosystem/kueue.html">Gang scheduling and priority scheduling for RayJob with Kueue</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/k8s-ecosystem/istio.html">mTLS and L7 observability with Istio</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../cluster/kubernetes/benchmarks.html">KubeRay Benchmarks</a><input class="toctree-checkbox" id="toctree-checkbox-56" name="toctree-checkbox-56" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-56"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/benchmarks/memory-scalability-benchmark.html">KubeRay memory and scalability benchmark</a></li>

</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../cluster/kubernetes/troubleshooting.html">KubeRay Troubleshooting</a><input class="toctree-checkbox" id="toctree-checkbox-57" name="toctree-checkbox-57" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-57"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/troubleshooting/troubleshooting.html">Troubleshooting guide</a></li>

<li class="toctree-l4"><a class="reference internal" href="../cluster/kubernetes/troubleshooting/rayservice-troubleshooting.html">RayService troubleshooting</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../cluster/kubernetes/references.html">API Reference</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../cluster/vms/index.html">Deploying on VMs</a><input class="toctree-checkbox" id="toctree-checkbox-58" name="toctree-checkbox-58" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-58"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../cluster/vms/getting-started.html">Getting Started</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../cluster/vms/user-guides/index.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-59" name="toctree-checkbox-59" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-59"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../cluster/vms/user-guides/launching-clusters/index.html">Launching Ray Clusters on AWS, GCP, Azure, vSphere, On-Prem</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/vms/user-guides/large-cluster-best-practices.html">Best practices for deploying large clusters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/vms/user-guides/configuring-autoscaling.html">Configuring Autoscaling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/vms/user-guides/logging.html">Log Persistence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/vms/user-guides/community/index.html">Community Supported Cluster Managers</a></li>

</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../cluster/vms/examples/index.html">Examples</a><input class="toctree-checkbox" id="toctree-checkbox-60" name="toctree-checkbox-60" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-60"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../cluster/vms/examples/ml-example.html">Ray Train XGBoostTrainer on VMs</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../cluster/vms/references/index.html">API References</a><input class="toctree-checkbox" id="toctree-checkbox-61" name="toctree-checkbox-61" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-61"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../cluster/vms/references/ray-cluster-cli.html">Cluster Launcher Commands</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/vms/references/ray-cluster-configuration.html">Cluster YAML Configuration Options</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../cluster/metrics.html">Collecting and monitoring metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cluster/configure-manage-dashboard.html">Configuring and Managing Ray Dashboard</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../cluster/running-applications/index.html">Applications Guide</a><input class="toctree-checkbox" id="toctree-checkbox-62" name="toctree-checkbox-62" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-62"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../cluster/running-applications/job-submission/index.html">Ray Jobs Overview</a><input class="toctree-checkbox" id="toctree-checkbox-63" name="toctree-checkbox-63" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-63"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../cluster/running-applications/job-submission/quickstart.html">Quickstart using the Ray Jobs CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/running-applications/job-submission/sdk.html">Python SDK Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/running-applications/job-submission/jobs-package-ref.html">Python SDK API Reference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/running-applications/job-submission/cli.html">Ray Jobs CLI API Reference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/running-applications/job-submission/rest.html">Ray Jobs REST API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cluster/running-applications/job-submission/ray-client.html">Ray Client</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../cluster/running-applications/autoscaling/reference.html">Programmatic Cluster Scaling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../cluster/faq.html">FAQ</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../cluster/package-overview.html">Ray Cluster Management API</a><input class="toctree-checkbox" id="toctree-checkbox-64" name="toctree-checkbox-64" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-64"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../cluster/cli.html">Cluster Management CLI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cluster/running-applications/job-submission/jobs-package-ref.html">Python SDK API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cluster/running-applications/job-submission/cli.html">Ray Jobs CLI API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cluster/running-applications/autoscaling/reference.html">Programmatic Cluster Scaling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../cluster/usage-stats.html">Usage Stats Collection</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../virtual-cluster/getting-started.html">Ray Virtual Clusters</a><input class="toctree-checkbox" id="toctree-checkbox-65" name="toctree-checkbox-65" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-65"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../virtual-cluster/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../virtual-cluster/design-overview.html">Design Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../virtual-cluster/management.html">Virtual Cluster Management API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../virtual-cluster/cli.html">Virtual Cluster CLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../virtual-cluster/examples.html">Examples</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../ray-observability/index.html">Monitoring and Debugging</a><input class="toctree-checkbox" id="toctree-checkbox-66" name="toctree-checkbox-66" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-66"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../ray-observability/getting-started.html">Ray Dashboard</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ray-observability/ray-distributed-debugger.html">Ray Distributed Debugger</a></li>



<li class="toctree-l2"><a class="reference internal" href="../ray-observability/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../ray-observability/user-guides/index.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-67" name="toctree-checkbox-67" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-67"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../ray-observability/user-guides/debug-apps/index.html">Debugging Applications</a><input class="toctree-checkbox" id="toctree-checkbox-68" name="toctree-checkbox-68" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-68"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../ray-observability/user-guides/debug-apps/general-debugging.html">General Debugging</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-observability/user-guides/debug-apps/debug-memory.html">Debugging Memory Issues</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-observability/user-guides/debug-apps/debug-hangs.html">Debugging Hangs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-observability/user-guides/debug-apps/debug-failures.html">Debugging Failures</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-observability/user-guides/debug-apps/optimize-performance.html">Optimizing Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ray-observability/ray-distributed-debugger.html">Ray Distributed Debugger</a></li>



<li class="toctree-l4"><a class="reference internal" href="../ray-observability/user-guides/debug-apps/ray-debugging.html">Using the Ray Debugger</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../ray-observability/user-guides/cli-sdk.html">Monitoring with the CLI or SDK</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-observability/user-guides/configure-logging.html">Configuring Logging</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-observability/user-guides/profiling.html">Profiling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-observability/user-guides/add-app-metrics.html">Adding Application-Level Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-observability/user-guides/ray-tracing.html">Tracing</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../ray-observability/reference/index.html">Reference</a><input class="toctree-checkbox" id="toctree-checkbox-69" name="toctree-checkbox-69" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-69"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../ray-observability/reference/api.html">State API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-observability/reference/cli.html">State CLI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-observability/reference/system-metrics.html">System Metrics</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../ray-contribute/index.html">Developer Guides</a><input class="toctree-checkbox" id="toctree-checkbox-70" name="toctree-checkbox-70" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-70"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../ray-contribute/stability.html">API Stability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ray-contribute/api-policy.html">API Policy</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../ray-contribute/getting-involved.html">Getting Involved / Contributing</a><input class="toctree-checkbox" id="toctree-checkbox-71" name="toctree-checkbox-71" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-71"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../ray-contribute/development.html">Building Ray from Source</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-contribute/ci.html">CI Testing Workflow on PRs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-contribute/docs.html">Contributing to the Ray Documentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-contribute/writing-code-snippets.html">How to write code snippets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-contribute/fake-autoscaler.html">Testing Autoscaling Locally</a></li>

<li class="toctree-l3"><a class="reference internal" href="../ray-contribute/testing-tips.html">Tips for testing Ray programs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-contribute/debugging.html">Debugging for Ray Developers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ray-contribute/profiling.html">Profiling for Ray Developers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../ray-core/configure.html">Configuring Ray</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ray-contribute/whitepaper.html">Architecture Whitepapers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../ray-references/glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-security/index.html">Security</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="index.html" class="nav-link">RLlib: Industry-Grade, Scalable Reinforcement Learning</a></li>
    
    
    <li class="breadcrumb-item"><a href="user-guides.html" class="nav-link">User Guides</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Working...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="working-with-offline-data">
<h1>Working with offline data<a class="headerlink" href="#working-with-offline-data" title="Link to this heading">#</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Ray 2.40 uses RLlib’s new API stack by default.
The Ray team has mostly completed transitioning algorithms, example scripts, and
documentation to the new code base.</p>
<p>If you’re still using the old API stack, see <a class="reference internal" href="new-api-stack-migration-guide.html"><span class="doc">New API stack migration guide</span></a> for details on how to migrate.</p>
</div>
<p>RLlib’s offline RL API enables you to work with experiences read from offline storage (for example, disk, cloud storage,
streaming systems, Hadoop Distributed File System (HDFS). For example, you might want to read experiences saved from previous training runs, collected
from experts, or gathered from policies deployed in <a class="reference external" href="https://arxiv.org/abs/1811.00260">web applications</a>. You can
also log new agent experiences produced during online training for future use.</p>
<p>RLlib represents trajectory sequences (for example, <code class="docutils literal notranslate"><span class="pre">(s,</span> <span class="pre">a,</span> <span class="pre">r,</span> <span class="pre">s',</span> <span class="pre">...)</span></code> tuples) with <a class="reference internal" href="package_ref/env/env/ray.rllib.env.single_agent_episode.SingleAgentEpisode.html#ray.rllib.env.single_agent_episode.SingleAgentEpisode" title="ray.rllib.env.single_agent_episode.SingleAgentEpisode"><code class="xref py py-class docutils literal notranslate"><span class="pre">SingleAgentEpisode</span></code></a>
objects (multi-agent offline training is currently not supported). Using this episode format
allows for efficient encoding and compression of experiences, rewriting trajectories, and user-friendly data access through getter methods.
During online training, RLlib uses <a class="reference internal" href="package_ref/env/single_agent_env_runner.html#ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner" title="ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner"><code class="xref py py-class docutils literal notranslate"><span class="pre">SingleAgentEnvRunner</span></code></a> actors to generate episodes of experiences
in parallel using the current policy. However, RLlib uses this same episode format for reading experiences from and writing experiences to offline storage (see <a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner.html#ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner" title="ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner"><code class="xref py py-class docutils literal notranslate"><span class="pre">OfflineSingleAgentEnvRunner</span></code></a>).</p>
<p>You can store experiences either directly in RLlib’s episode format or in table (columns)
format. You should use the episode format when</p>
<ol class="arabic simple">
<li><p>You need experiences grouped by their trajectory and ordered in time (for example, to train stateful modules).</p></li>
<li><p>You want to use recorded experiences exclusively within RLlib (for example for offline RL or behavior cloning).</p></li>
</ol>
<p>Contrary, you should prefer the table (columns) format, if</p>
<ol class="arabic simple">
<li><p>You need to read the data easily with other data tools or ML libraries.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>RLlib’s new API stack incorporates principles that support standalone applications. Consequently, the
<a class="reference internal" href="package_ref/env/env/ray.rllib.env.single_agent_episode.SingleAgentEpisode.html#ray.rllib.env.single_agent_episode.SingleAgentEpisode" title="ray.rllib.env.single_agent_episode.SingleAgentEpisode"><code class="xref py py-class docutils literal notranslate"><span class="pre">SingleAgentEpisode</span></code></a> class is usable outside of an RLlib context. To enable faster
access through external data tools (for example, for data transformations), it’s recommended to use the table record format.</p>
</div>
<p>Most importantly, RLlib’s offline RL API builds on top of <a class="reference internal" href="../data/data.html#data"><span class="std std-ref">Ray Data</span></a> and therefore features in general all read and
write methods supported by Ray Data (for example <a class="reference internal" href="../data/api/doc/ray.data.read_parquet.html#ray.data.read_parquet" title="ray.data.read_parquet"><code class="xref py py-class docutils literal notranslate"><span class="pre">read_parquet</span></code></a>, <a class="reference internal" href="../data/api/doc/ray.data.read_json.html#ray.data.read_json" title="ray.data.read_json"><code class="xref py py-class docutils literal notranslate"><span class="pre">read_json</span></code></a>, etc.) with
<a class="reference internal" href="../data/api/doc/ray.data.read_parquet.html#ray.data.read_parquet" title="ray.data.read_parquet"><code class="xref py py-class docutils literal notranslate"><span class="pre">read_parquet</span></code></a> and <a class="reference internal" href="../data/api/doc/ray.data.Dataset.write_parquet.html#ray.data.Dataset.write_parquet" title="ray.data.Dataset.write_parquet"><code class="xref py py-class docutils literal notranslate"><span class="pre">write_parquet</span></code></a> being the default read and write methods. A core design principle
of the API is to apply as many data transformations as possible on-the-fly prior to engaging the learner, allowing the latter to focus exclusively on model updates.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>During the transition phase from old- to new API stack you can use the new offline RL API also with your
<code class="xref py py-class docutils literal notranslate"><span class="pre">SampleBatch</span></code> data recorded with the old API stack. To enable this feature set
<code class="docutils literal notranslate"><span class="pre">config.offline_data(input_read_sample_batches=True)</span></code>.</p>
</div>
<section id="example-training-an-expert-policy">
<h2>Example: Training an expert policy<a class="headerlink" href="#example-training-an-expert-policy" title="Link to this heading">#</a></h2>
<p>In this example you train a PPO agent on the <code class="docutils literal notranslate"><span class="pre">CartPole-v1</span></code> environment until it reaches an episode mean return of <code class="docutils literal notranslate"><span class="pre">450.0</span></code>. You checkpoint
this agent and then use its policy to record expert data to local disk.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.ppo</span> <span class="kn">import</span> <span class="n">PPOConfig</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.rl_module.default_model_config</span> <span class="kn">import</span> <span class="n">DefaultModelConfig</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.metrics</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">ENV_RUNNER_RESULTS</span><span class="p">,</span>
    <span class="n">EVALUATION_RESULTS</span><span class="p">,</span>
    <span class="n">EPISODE_RETURN_MEAN</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">train</span><span class="p">,</span> <span class="n">tune</span>

<span class="c1"># Configure the PPO algorithm.</span>
<span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">PPOConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">training</span><span class="p">(</span>
        <span class="n">lr</span><span class="o">=</span><span class="mf">0.0003</span><span class="p">,</span>
        <span class="c1"># Run 6 SGD minibatch iterations on a batch.</span>
        <span class="n">num_epochs</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
        <span class="c1"># Weigh the value function loss smaller than</span>
        <span class="c1"># the policy loss.</span>
        <span class="n">vf_loss_coeff</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="o">.</span><span class="n">rl_module</span><span class="p">(</span>
        <span class="n">model_config</span><span class="o">=</span><span class="n">DefaultModelConfig</span><span class="p">(</span>
            <span class="n">fcnet_hiddens</span><span class="o">=</span><span class="p">[</span><span class="mi">32</span><span class="p">],</span>
            <span class="n">fcnet_activation</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span>
            <span class="c1"># Share encoder layers between value network</span>
            <span class="c1"># and policy.</span>
            <span class="n">vf_share_layers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Define the metric to use for stopping.</span>
<span class="n">metric</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">EVALUATION_RESULTS</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">ENV_RUNNER_RESULTS</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">EPISODE_RETURN_MEAN</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="c1"># Define the Tuner.</span>
<span class="n">tuner</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>
    <span class="s2">&quot;PPO&quot;</span><span class="p">,</span>
    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
    <span class="n">run_config</span><span class="o">=</span><span class="n">train</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span>
        <span class="n">stop</span><span class="o">=</span><span class="p">{</span>
            <span class="n">metric</span><span class="p">:</span> <span class="mf">450.0</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;docs_rllib_offline_pretrain_ppo&quot;</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">checkpoint_config</span><span class="o">=</span><span class="n">train</span><span class="o">.</span><span class="n">CheckpointConfig</span><span class="p">(</span>
            <span class="n">checkpoint_frequency</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">checkpoint_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Store the best checkpoint to use it later for recording</span>
<span class="c1"># an expert policy.</span>
<span class="n">best_checkpoint</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">results</span>
    <span class="o">.</span><span class="n">get_best_result</span><span class="p">(</span>
        <span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;max&quot;</span>
    <span class="p">)</span>
    <span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">path</span>
<span class="p">)</span>
</pre></div>
</div>
<p>In this example, you saved a checkpoint from an agent that has become an expert at playing <code class="docutils literal notranslate"><span class="pre">CartPole-v1</span></code>.  You use this checkpoint in the next
example to record expert data to disk, which is later utilized for offline training to clone another agent.</p>
</section>
<section id="example-record-expert-data-to-local-disk">
<h2>Example: Record expert data to local disk<a class="headerlink" href="#example-record-expert-data-to-local-disk" title="Link to this heading">#</a></h2>
<p>After you train an expert policy to play <code class="code docutils literal notranslate"><span class="pre">CartPole-v1</span></code> you load its policy here to record expert data during evaluation. You use <code class="docutils literal notranslate"><span class="pre">5</span></code>
<a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner.html#ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner" title="ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner"><code class="xref py py-class docutils literal notranslate"><span class="pre">OfflineSingleAgentEnvRunner</span></code></a> instances to collect <code class="docutils literal notranslate"><span class="pre">50</span></code> complete episodes per <code class="code docutils literal notranslate"><span class="pre">sample()</span></code> call. In this
example you store experiences directly in RLlib’s <a class="reference internal" href="package_ref/env/env/ray.rllib.env.single_agent_episode.SingleAgentEpisode.html#ray.rllib.env.single_agent_episode.SingleAgentEpisode" title="ray.rllib.env.single_agent_episode.SingleAgentEpisode"><code class="xref py py-class docutils literal notranslate"><span class="pre">SingleAgentEpisode</span></code></a> objects with no more than
<code class="docutils literal notranslate"><span class="pre">25</span></code> episode objects per Parquet file. Altogether you run 10 evaluation runs, which should result in <code class="docutils literal notranslate"><span class="pre">500</span></code> recorded episodes from the expert policy.
You use these data in the next example to train a new policy through Offline RL that should reach a return of <code class="docutils literal notranslate"><span class="pre">450.0</span></code> when playing <code class="docutils literal notranslate"><span class="pre">CartPole-v1</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.ppo</span> <span class="kn">import</span> <span class="n">PPOConfig</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">COMPONENT_LEARNER_GROUP</span><span class="p">,</span>
    <span class="n">COMPONENT_LEARNER</span><span class="p">,</span>
    <span class="n">COMPONENT_RL_MODULE</span><span class="p">,</span>
    <span class="n">DEFAULT_MODULE_ID</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.rl_module</span> <span class="kn">import</span> <span class="n">RLModuleSpec</span>

<span class="c1"># Store recording data under the following path.</span>
<span class="n">data_path</span> <span class="o">=</span> <span class="s2">&quot;/tmp/docs_rllib_offline_recording&quot;</span>

<span class="c1"># Configure the algorithm for recording.</span>
<span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">PPOConfig</span><span class="p">()</span>
    <span class="c1"># The environment needs to be specified.</span>
    <span class="o">.</span><span class="n">environment</span><span class="p">(</span>
        <span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># Make sure to sample complete episodes because</span>
    <span class="c1"># you want to record RLlib&#39;s episode objects.</span>
    <span class="o">.</span><span class="n">env_runners</span><span class="p">(</span>
        <span class="n">batch_mode</span><span class="o">=</span><span class="s2">&quot;complete_episodes&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># Set up 5 evaluation `EnvRunners` for recording.</span>
    <span class="c1"># Sample 50 episodes in each evaluation rollout.</span>
    <span class="o">.</span><span class="n">evaluation</span><span class="p">(</span>
        <span class="n">evaluation_num_env_runners</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">evaluation_duration</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
        <span class="n">evaluation_duration_unit</span><span class="o">=</span><span class="s2">&quot;episodes&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># Use the checkpointed expert policy from the preceding PPO training.</span>
    <span class="c1"># Note, we have to use the same `model_config` as</span>
    <span class="c1"># the one with which the expert policy was trained, otherwise</span>
    <span class="c1"># the module state can&#39;t be loaded.</span>
    <span class="o">.</span><span class="n">rl_module</span><span class="p">(</span>
        <span class="n">model_config</span><span class="o">=</span><span class="n">DefaultModelConfig</span><span class="p">(</span>
            <span class="n">fcnet_hiddens</span><span class="o">=</span><span class="p">[</span><span class="mi">32</span><span class="p">],</span>
            <span class="n">fcnet_activation</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span>
            <span class="c1"># Share encoder layers between value network</span>
            <span class="c1"># and policy.</span>
            <span class="n">vf_share_layers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="c1"># Define the output path and format. In this example you</span>
    <span class="c1"># want to store data directly in RLlib&#39;s episode objects.</span>
    <span class="c1"># Each Parquet file should hold no more than 25 episodes.</span>
    <span class="o">.</span><span class="n">offline_data</span><span class="p">(</span>
        <span class="n">output</span><span class="o">=</span><span class="n">data_path</span><span class="p">,</span>
        <span class="n">output_write_episodes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">output_max_rows_per_file</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Build the algorithm.</span>
<span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>
<span class="c1"># Load now the PPO-trained `RLModule` to use in recording.</span>
<span class="n">algo</span><span class="o">.</span><span class="n">restore_from_path</span><span class="p">(</span>
    <span class="n">best_checkpoint</span><span class="p">,</span>
    <span class="c1"># Load only the `RLModule` component here.</span>
    <span class="n">component</span><span class="o">=</span><span class="n">COMPONENT_RL_MODULE</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Run 10 evaluation iterations and record the data.</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">eval_results</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">eval_results</span><span class="p">)</span>

<span class="c1"># Stop the algorithm. Note, this is important for when</span>
<span class="c1"># defining `output_max_rows_per_file`. Otherwise,</span>
<span class="c1"># remaining episodes in the `EnvRunner`s buffer isn&#39;t written to disk.</span>
<span class="n">algo</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>RLlib formats The stored episode data as <code class="docutils literal notranslate"><span class="pre">binary</span></code>. Each episode is converted into its dictionary representation and serialized using <code class="docutils literal notranslate"><span class="pre">msgpack-numpy</span></code>,
ensuring version compatibility.</p>
</div>
<p>RLlib’s  recording process is efficient because it utilizes multiple <a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner.html#ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner" title="ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner"><code class="xref py py-class docutils literal notranslate"><span class="pre">OfflineSingleAgentEnvRunner</span></code></a> instances during
evaluation, enabling parallel data writing. You can explore the folder to review the stored Parquet data:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ ls -la /tmp/docs_rllib_offline_recording/cartpole-v1

drwxr-xr-x. 22 user user 440 21. Nov 17:23 .
drwxr-xr-x.  3 user user  60 21. Nov 17:23 ..
drwxr-xr-x.  2 user user 540 21. Nov 17:23 run-000001-00004
drwxr-xr-x.  2 user user 540 21. Nov 17:23 run-000001-00009
drwxr-xr-x.  2 user user 540 21. Nov 17:23 run-000001-00012
drwxr-xr-x.  2 user user 540 21. Nov 17:23 run-000001-00016
drwxr-xr-x.  2 user user 540 21. Nov 17:23 run-000002-00004
drwxr-xr-x.  2 user user 540 21. Nov 17:23 run-000002-00007
</pre></div>
</div>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>RLlib stores records under a folder named by the RL environment. Therein, you see one folder of Parquet files for each <a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner.html#ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner" title="ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner"><code class="xref py py-class docutils literal notranslate"><span class="pre">OfflineSingleAgentEnvRunner</span></code></a>
and write operation. The write operation count is given in the second numbering. For example: above, env-runner 1 has sampled 25 episodes at
its 4th <a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner.sample.html#ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner.sample" title="ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner.sample"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sample()</span></code></a> call and writes then (because <code class="docutils literal notranslate"><span class="pre">output_max_rows_per_file=25</span></code>) all sampled episodes
to disk into file <code class="docutils literal notranslate"><span class="pre">run-000001-00004</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The number of write operations per worker may vary because policy rollouts aren’t evenly distributed. Faster workers collect more episodes,
leading to differences in write operation counts. As a result, the second numbering may differ across files generated by different env-runner instances.</p>
</div>
</section>
<section id="example-training-on-previously-saved-experiences">
<h2>Example: Training on previously saved experiences<a class="headerlink" href="#example-training-on-previously-saved-experiences" title="Link to this heading">#</a></h2>
<p>In this example you are using behavior cloning with the previously recorded Parquet data from your expert policy playing <code class="docutils literal notranslate"><span class="pre">CartPole-v1</span></code>. The
data needs to be linked in the configuration of the algorithm (through the <code class="docutils literal notranslate"><span class="pre">input_</span></code> attribute).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">train</span><span class="p">,</span> <span class="n">tune</span>
<span class="kn">from</span> <span class="nn">ray.rllib.algorithms.bc</span> <span class="kn">import</span> <span class="n">BCConfig</span>

<span class="c1"># Setup the config for behavior cloning.</span>
<span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">BCConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">environment</span><span class="p">(</span>
        <span class="c1"># Use the `CartPole-v1` environment from which the</span>
        <span class="c1"># data was recorded. This is merely for receiving</span>
        <span class="c1"># action and observation spaces and to use it during</span>
        <span class="c1"># evaluation.</span>
        <span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="o">.</span><span class="n">learners</span><span class="p">(</span>
        <span class="c1"># Use a single learner.</span>
        <span class="n">num_learners</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="o">.</span><span class="n">training</span><span class="p">(</span>
        <span class="c1"># This has to be defined in the new offline RL API.</span>
        <span class="n">train_batch_size_per_learner</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="o">.</span><span class="n">offline_data</span><span class="p">(</span>
        <span class="c1"># Link the data.</span>
        <span class="n">input_</span><span class="o">=</span><span class="p">[</span><span class="n">data_path</span><span class="p">],</span>
        <span class="c1"># You want to read in RLlib&#39;s episode format b/c this</span>
        <span class="c1"># is how you recorded data.</span>
        <span class="n">input_read_episodes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="c1"># Read smaller batches from the data than the learner</span>
        <span class="c1"># trains on. Note, each batch element is an episode</span>
        <span class="c1"># with multiple timesteps.</span>
        <span class="n">input_read_batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
        <span class="c1"># Create exactly 2 `DataWorkers` that transform</span>
        <span class="c1"># the data on-the-fly. Give each of them a single</span>
        <span class="c1"># CPU.</span>
        <span class="n">map_batches_kwargs</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;concurrency&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
            <span class="s2">&quot;num_cpus&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="c1"># When iterating over the data, prefetch two batches</span>
        <span class="c1"># to improve the data pipeline. Don&#39;t shuffle the</span>
        <span class="c1"># buffer (the data is too small).</span>
        <span class="n">iter_batches_kwargs</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;prefetch_batches&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
            <span class="s2">&quot;local_shuffle_buffer_size&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="c1"># You must set this for single-learner setups.</span>
        <span class="n">dataset_num_iters_per_learner</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="o">.</span><span class="n">evaluation</span><span class="p">(</span>
        <span class="c1"># Run evaluation to see how well the learned policy</span>
        <span class="c1"># performs. Run every 3rd training iteration an evaluation.</span>
        <span class="n">evaluation_interval</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="c1"># Use a single `EnvRunner` for evaluation.</span>
        <span class="n">evaluation_num_env_runners</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="c1"># In each evaluation rollout, collect 5 episodes of data.</span>
        <span class="n">evaluation_duration</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="c1"># Evaluate the policy parallel to training.</span>
        <span class="n">evaluation_parallel_to_training</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Set the stopping metric to be the evaluation episode return mean.</span>
<span class="n">metric</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">EVALUATION_RESULTS</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">ENV_RUNNER_RESULTS</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">EPISODE_RETURN_MEAN</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="c1"># Configure Ray Tune.</span>
<span class="n">tuner</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>
    <span class="s2">&quot;BC&quot;</span><span class="p">,</span>
    <span class="n">param_space</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
    <span class="n">run_config</span><span class="o">=</span><span class="n">train</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;docs_rllib_offline_bc&quot;</span><span class="p">,</span>
        <span class="c1"># Stop behavior cloning when we reach 450 in return.</span>
        <span class="n">stop</span><span class="o">=</span><span class="p">{</span><span class="n">metric</span><span class="p">:</span> <span class="mf">450.0</span><span class="p">},</span>
        <span class="n">checkpoint_config</span><span class="o">=</span><span class="n">train</span><span class="o">.</span><span class="n">CheckpointConfig</span><span class="p">(</span>
            <span class="c1"># Only checkpoint at the end to be faster.</span>
            <span class="n">checkpoint_frequency</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">checkpoint_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="c1"># Run the experiment.</span>
<span class="n">analysis</span> <span class="o">=</span> <span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p>Behavior cloning in RLlib is highly performant, completing a single training iteration in approximately 2 milliseconds. The experiment’s
results should resemble the following:</p>
<a class="reference internal image-reference" href="../_images/docs_rllib_offline_bc_episode_return_mean.svg"><img alt="Episode mean return over the course of BC training." class="align-left" src="../_images/docs_rllib_offline_bc_episode_return_mean.svg" width="500" /></a>
<p>It should take you around <code class="docutils literal notranslate"><span class="pre">98</span></code> seconds (<code class="docutils literal notranslate"><span class="pre">456</span></code> iterations) to achieve the same episode return mean as the PPO agent. While this may not seem
impressive compared to the PPO training time, it’s important to note that <code class="docutils literal notranslate"><span class="pre">CartPole-v1</span></code> is a very simple environment to learn. In more
complex environments, which require more sophisticated agents and significantly longer training times, pre-training through behavior cloning
can be highly beneficial. Combining behavior cloning with subsequent fine-tuning using a reinforcement learning algorithm can substantially
reduce training time, resource consumption, and associated costs.</p>
</section>
<section id="using-external-expert-experiences">
<h2>Using external expert experiences<a class="headerlink" href="#using-external-expert-experiences" title="Link to this heading">#</a></h2>
<p>Your expert data is often already available, either recorded from an operational system or directly provided by human experts. Typically,
you might store this data in a tabular (columnar) format. RLlib’s new Offline RL API simplifies the use of such data by allowing direct ingestion
through a specified schema that organizes the expert data. The API default schema for reading data is provided in
<a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_prelearner.SCHEMA.html#ray.rllib.offline.offline_prelearner.SCHEMA" title="ray.rllib.offline.offline_prelearner.SCHEMA"><code class="xref py py-data docutils literal notranslate"><span class="pre">SCHEMA</span></code></a>.</p>
<p>Lets consider a simple example in which your expert data is stored with the schema: <code class="docutils literal notranslate"><span class="pre">(o_t,</span> <span class="pre">a_t,</span> <span class="pre">r_t,</span> <span class="pre">o_tp1,</span> <span class="pre">d_t,</span> <span class="pre">i_t,</span> <span class="pre">logprobs_t)</span></code>. In this case
you provide this schema as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.bc</span> <span class="kn">import</span> <span class="n">BCConfig</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.columns</span> <span class="kn">import</span> <span class="n">Columns</span>

<span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">BCConfig</span><span class="p">()</span>
    <span class="o">...</span>
    <span class="o">.</span><span class="n">offline_data</span><span class="p">(</span>
        <span class="n">input_</span><span class="o">=</span><span class="p">[</span><span class="o">&lt;</span><span class="n">input_path</span><span class="o">&gt;</span><span class="p">],</span>
        <span class="c1"># Provide the schema of your data (map to column names known to RLlib).</span>
        <span class="n">input_read_schema</span><span class="o">=</span><span class="p">{</span>
            <span class="n">Columns</span><span class="o">.</span><span class="n">OBS</span><span class="p">:</span> <span class="s2">&quot;o_t&quot;</span><span class="p">,</span>
            <span class="n">Columns</span><span class="o">.</span><span class="n">ACTIONS</span><span class="p">:</span> <span class="s2">&quot;a_t&quot;</span><span class="p">,</span>
            <span class="n">Columns</span><span class="o">.</span><span class="n">REWARDS</span><span class="p">:</span> <span class="s2">&quot;r_t&quot;</span><span class="p">,</span>
            <span class="n">Columns</span><span class="o">.</span><span class="n">NEXT_OBS</span><span class="p">:</span> <span class="s2">&quot;o_tp1&quot;</span><span class="p">,</span>
            <span class="n">Columns</span><span class="o">.</span><span class="n">INFOS</span><span class="p">:</span> <span class="s2">&quot;i_t&quot;</span><span class="p">,</span>
            <span class="s2">&quot;done&quot;</span><span class="p">:</span> <span class="s2">&quot;d_t&quot;</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Internally, the legacy <code class="docutils literal notranslate"><span class="pre">gym</span></code>’s <code class="docutils literal notranslate"><span class="pre">done</span></code> signals are mapped to <code class="docutils literal notranslate"><span class="pre">gymnasium</span></code>’s <code class="docutils literal notranslate"><span class="pre">terminated</span></code> signals, with <code class="docutils literal notranslate"><span class="pre">truncated</span></code> values defaulting to
<code class="docutils literal notranslate"><span class="pre">False</span></code>. RLlib’s <a class="reference internal" href="package_ref/env/env/ray.rllib.env.single_agent_episode.SingleAgentEpisode.html#ray.rllib.env.single_agent_episode.SingleAgentEpisode" title="ray.rllib.env.single_agent_episode.SingleAgentEpisode"><code class="xref py py-class docutils literal notranslate"><span class="pre">SingleAgentEpisode</span></code></a> structures align with <code class="docutils literal notranslate"><span class="pre">gymnasium</span></code>, adhering to the updated environment API standards in reinforcement learning.</p>
</div>
</section>
<section id="converting-tabular-data-to-rllib-s-episode-format">
<h2>Converting tabular data to RLlib’s episode format<a class="headerlink" href="#converting-tabular-data-to-rllib-s-episode-format" title="Link to this heading">#</a></h2>
<p>While the tabular format is widely compatible and seamlessly integrates with RLlib’s new Offline RL API, there are cases where you may prefer to use RLlib’s native episode format.
As briefly mentioned earlier, such scenarios typically arise when full expert trajectories are required.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>RLlib processes tabular data in batches, converting each row into a <em>single-step episode</em>. This approach is primarily for procedural simplicity, as data can’t
generally be assumed to arrive in time-ordered rows grouped by episodes, though this may occasionally be the case (however knowledge of such a structure resides
with the user as RLlib can’t easily infer it automatically). While it’s possible to concatenate consecutive <a class="reference internal" href="package_ref/env/env/ray.rllib.env.single_agent_episode.SingleAgentEpisode.html#ray.rllib.env.single_agent_episode.SingleAgentEpisode" title="ray.rllib.env.single_agent_episode.SingleAgentEpisode"><code class="xref py py-class docutils literal notranslate"><span class="pre">SingleAgentEpisode</span></code></a>
chunks, this can’t be done with chunks arriving in some scrambled order.</p>
</div>
<p>If you require full trajectories you can transform your tabular data into <a class="reference internal" href="package_ref/env/env/ray.rllib.env.single_agent_episode.SingleAgentEpisode.html#ray.rllib.env.single_agent_episode.SingleAgentEpisode" title="ray.rllib.env.single_agent_episode.SingleAgentEpisode"><code class="xref py py-class docutils literal notranslate"><span class="pre">SingleAgentEpisode</span></code></a> objects and store these in Parquet format. The next example shows
how to do this.
First, you store experiences of the preceding trained expert policy in tabular format (note the <code class="code docutils literal notranslate"><span class="pre">output_write_episodes=False</span></code> setting below to activate tabular data output):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.ppo</span> <span class="kn">import</span> <span class="n">PPOConfig</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">COMPONENT_LEARNER_GROUP</span><span class="p">,</span>
    <span class="n">COMPONENT_LEARNER</span><span class="p">,</span>
    <span class="n">COMPONENT_RL_MODULE</span><span class="p">,</span>
    <span class="n">DEFAULT_MODULE_ID</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.rl_module</span> <span class="kn">import</span> <span class="n">RLModuleSpec</span>

<span class="c1"># Set up a path for the tabular data records.</span>
<span class="n">tabular_data_path</span> <span class="o">=</span> <span class="s2">&quot;tmp/docs_rllib_offline_recording_tabular&quot;</span>

<span class="c1"># Configure the algorithm for recording.</span>
<span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">PPOConfig</span><span class="p">()</span>
    <span class="c1"># The environment needs to be specified.</span>
    <span class="o">.</span><span class="n">environment</span><span class="p">(</span>
        <span class="n">env</span><span class="o">=</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># Make sure to sample complete episodes because</span>
    <span class="c1"># you want to record RLlib&#39;s episode objects.</span>
    <span class="o">.</span><span class="n">env_runners</span><span class="p">(</span>
        <span class="n">batch_mode</span><span class="o">=</span><span class="s2">&quot;complete_episodes&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># Set up 5 evaluation `EnvRunners` for recording.</span>
    <span class="c1"># Sample 50 episodes in each evaluation rollout.</span>
    <span class="o">.</span><span class="n">evaluation</span><span class="p">(</span>
        <span class="n">evaluation_num_env_runners</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">evaluation_duration</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># Use the checkpointed expert policy from the preceding PPO training.</span>
    <span class="c1"># Note, we have to use the same `model_config` as</span>
    <span class="c1"># the one with which the expert policy was trained, otherwise</span>
    <span class="c1"># the module state can&#39;t be loaded.</span>
    <span class="o">.</span><span class="n">rl_module</span><span class="p">(</span>
        <span class="n">model_config</span><span class="o">=</span><span class="n">DefaultModelConfig</span><span class="p">(</span>
            <span class="n">fcnet_hiddens</span><span class="o">=</span><span class="p">[</span><span class="mi">32</span><span class="p">],</span>
            <span class="n">fcnet_activation</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span>
            <span class="c1"># Share encoder layers between value network</span>
            <span class="c1"># and policy.</span>
            <span class="n">vf_share_layers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="c1"># Define the output path and format. In this example you</span>
    <span class="c1"># want to store data directly in RLlib&#39;s episode objects.</span>
    <span class="o">.</span><span class="n">offline_data</span><span class="p">(</span>
        <span class="n">output</span><span class="o">=</span><span class="n">tabular_data_path</span><span class="p">,</span>
        <span class="c1"># You want to store for this example tabular data.</span>
        <span class="n">output_write_episodes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Build the algorithm.</span>
<span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>
<span class="c1"># Load the PPO-trained `RLModule` to use in recording.</span>
<span class="n">algo</span><span class="o">.</span><span class="n">restore_from_path</span><span class="p">(</span>
    <span class="n">best_checkpoint</span><span class="p">,</span>
    <span class="c1"># Load only the `RLModule` component here.</span>
    <span class="n">component</span><span class="o">=</span><span class="n">COMPONENT_RL_MODULE</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Run 10 evaluation iterations and record the data.</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">res_eval</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">res_eval</span><span class="p">)</span>

<span class="c1"># Stop the algorithm. Note, this is important for when</span>
<span class="c1"># defining `output_max_rows_per_file`. Otherwise,</span>
<span class="c1"># remaining episodes in the `EnvRunner`s buffer isn&#39;t written to disk.</span>
<span class="n">algo</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</pre></div>
</div>
<p>You may have noticed that recording data in tabular format takes significantly longer than recording in episode format. This slower performance is due to the additional post-processing
required to convert episode data into a columnar format. To confirm that the recorded data is now in columnar format, you can print its schema:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">data</span>

<span class="c1"># Read the tabular data into a Ray dataset.</span>
<span class="n">ds</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="n">tabular_data_path</span><span class="p">)</span>
<span class="c1"># Now, print its schema.</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tabular data schema of expert experiences:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ds</span><span class="o">.</span><span class="n">schema</span><span class="p">())</span>

<span class="c1"># Column              Type</span>
<span class="c1"># ------              ----</span>
<span class="c1"># eps_id              string</span>
<span class="c1"># agent_id            null</span>
<span class="c1"># module_id           null</span>
<span class="c1"># obs                 numpy.ndarray(shape=(4,), dtype=float)</span>
<span class="c1"># actions             int32</span>
<span class="c1"># rewards             double</span>
<span class="c1"># new_obs             numpy.ndarray(shape=(4,), dtype=float)</span>
<span class="c1"># terminateds         bool</span>
<span class="c1"># truncateds          bool</span>
<span class="c1"># action_dist_inputs  numpy.ndarray(shape=(2,), dtype=float)</span>
<span class="c1"># action_logp         float</span>
<span class="c1"># weights_seq_no      int64</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">infos</span></code> aren’t stored to disk when they’re all empty.</p>
</div>
<p>If your expert data is given in columnar format and you need to train on full expert trajectories you can follow the code in the following example to convert
your own data into RLlib’s <a class="reference internal" href="package_ref/env/env/ray.rllib.env.single_agent_episode.SingleAgentEpisode.html#ray.rllib.env.single_agent_episode.SingleAgentEpisode" title="ray.rllib.env.single_agent_episode.SingleAgentEpisode"><code class="xref py py-class docutils literal notranslate"><span class="pre">SingleAgentEpisode</span></code></a> objects:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gymnasium</span> <span class="k">as</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">msgpack</span>
<span class="kn">import</span> <span class="nn">msgpack_numpy</span> <span class="k">as</span> <span class="nn">mnp</span>

<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>

<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">data</span>
<span class="kn">from</span> <span class="nn">ray.rllib.env.single_agent_episode</span> <span class="kn">import</span> <span class="n">SingleAgentEpisode</span>

<span class="c1"># Load the dataset with the tabular data.</span>
<span class="n">ds</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="n">tabular_data_path</span><span class="p">)</span>

<span class="c1"># Build the environment from which the data was sampled to get the</span>
<span class="c1"># spaces.</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>
<span class="c1"># Define buffers for episode data.</span>
<span class="n">eps_obs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">eps_actions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">eps_rewards</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1"># Note, extra-model-outputs needs to be a dictionary with list</span>
<span class="c1"># values.</span>
<span class="n">eps_extra_model_outputs</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
<span class="c1"># Define a buffer for unwritten episodes.</span>
<span class="n">episodes</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Start iterating over the rows of your experience data.</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ds</span><span class="o">.</span><span class="n">iter_rows</span><span class="p">(</span><span class="n">prefetch_batches</span><span class="o">=</span><span class="mi">10</span><span class="p">)):</span>
    <span class="c1"># If the episode isn&#39;t terminated nor truncated, buffer the data.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;terminateds&quot;</span><span class="p">]</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;truncateds&quot;</span><span class="p">]:</span>
        <span class="n">eps_obs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;obs&quot;</span><span class="p">])</span>
        <span class="n">eps_actions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;actions&quot;</span><span class="p">])</span>
        <span class="n">eps_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;rewards&quot;</span><span class="p">])</span>
        <span class="n">eps_extra_model_outputs</span><span class="p">[</span><span class="s2">&quot;action_dist_inputs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;action_dist_inputs&quot;</span><span class="p">])</span>
        <span class="n">eps_extra_model_outputs</span><span class="p">[</span><span class="s2">&quot;action_logp&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;action_logp&quot;</span><span class="p">])</span>
    <span class="c1"># Otherwise, build the episode.</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">eps_obs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;new_obs&quot;</span><span class="p">])</span>
        <span class="n">episode</span> <span class="o">=</span> <span class="n">SingleAgentEpisode</span><span class="p">(</span>
            <span class="n">id_</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;eps_id&quot;</span><span class="p">],</span>
            <span class="n">agent_id</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;agent_id&quot;</span><span class="p">],</span>
            <span class="n">module_id</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;module_id&quot;</span><span class="p">],</span>
            <span class="n">observations</span><span class="o">=</span><span class="n">eps_obs</span><span class="p">,</span>
            <span class="c1"># Use the spaces from the environment.</span>
            <span class="n">observation_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span>
            <span class="n">action_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span>
            <span class="n">actions</span><span class="o">=</span><span class="n">eps_actions</span><span class="p">,</span>
            <span class="n">rewards</span><span class="o">=</span><span class="n">eps_rewards</span><span class="p">,</span>
            <span class="c1"># Set the starting timestep to zero.</span>
            <span class="n">t_started</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="c1"># You don&#39;t want to have a lookback buffer.</span>
            <span class="n">len_lookback_buffer</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">terminated</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;terminateds&quot;</span><span class="p">],</span>
            <span class="n">truncated</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;truncateds&quot;</span><span class="p">],</span>
            <span class="n">extra_model_outputs</span><span class="o">=</span><span class="n">eps_extra_model_outputs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># Store the ready-to-write episode to the episode buffer.</span>
        <span class="n">episodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">msgpack</span><span class="o">.</span><span class="n">packb</span><span class="p">(</span><span class="n">episode</span><span class="o">.</span><span class="n">get_state</span><span class="p">(),</span> <span class="n">default</span><span class="o">=</span><span class="n">mnp</span><span class="o">.</span><span class="n">encode</span><span class="p">))</span>
        <span class="c1"># Clear all episode data buffers.</span>
        <span class="n">eps_obs</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="n">eps_actions</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="n">eps_rewards</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="n">eps_extra_model_outputs</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>

    <span class="c1"># Write episodes to disk when the episode buffer holds 50 episodes.</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">episodes</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">49</span><span class="p">:</span>
        <span class="c1"># Generate a Ray dataset from episodes.</span>
        <span class="n">episodes_ds</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">from_items</span><span class="p">(</span><span class="n">episodes</span><span class="p">)</span>
        <span class="c1"># Write the Parquet data and compress it.</span>
        <span class="n">episodes_ds</span><span class="o">.</span><span class="n">write_parquet</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;/tmp/test_converting/file-</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">zfill</span><span class="p">(</span><span class="mi">6</span><span class="p">),</span>
            <span class="n">compression</span><span class="o">=</span><span class="s2">&quot;gzip&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># Delete the dataset in memory and clear the episode buffer.</span>
        <span class="k">del</span> <span class="n">episodes_ds</span>
        <span class="n">episodes</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>

<span class="c1"># If we are finished and have unwritten episodes, write them now.</span>
<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">episodes</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">episodes_ds</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">from_items</span><span class="p">(</span><span class="n">episodes</span><span class="p">)</span>
    <span class="n">episodes_ds</span><span class="o">.</span><span class="n">write_parquet</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;/tmp/test_converting/file-</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">zfill</span><span class="p">(</span><span class="mi">6</span><span class="p">),</span>
        <span class="n">compression</span><span class="o">=</span><span class="s2">&quot;gzip&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">del</span> <span class="n">episodes_ds</span>
    <span class="n">episodes</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="using-old-api-stack-samplebatch-recordings">
<h2>Using old API stack <code class="docutils literal notranslate"><span class="pre">SampleBatch</span></code> recordings<a class="headerlink" href="#using-old-api-stack-samplebatch-recordings" title="Link to this heading">#</a></h2>
<p>If you have expert data previously recorded using RLlib’s old API stack, it can be seamlessly utilized in the new stack’s Offline RL API by setting <code class="docutils literal notranslate"><span class="pre">input_read_sample_batches=True</span></code>. Alternatively,
you can convert your <code class="docutils literal notranslate"><span class="pre">SampleBatch</span></code> recordings into <a class="reference internal" href="package_ref/env/env/ray.rllib.env.single_agent_episode.SingleAgentEpisode.html#ray.rllib.env.single_agent_episode.SingleAgentEpisode" title="ray.rllib.env.single_agent_episode.SingleAgentEpisode"><code class="xref py py-class docutils literal notranslate"><span class="pre">SingleAgentEpisode</span></code></a> format using RLlib’s
<a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.html#ray.rllib.offline.offline_prelearner.OfflinePreLearner" title="ray.rllib.offline.offline_prelearner.OfflinePreLearner"><code class="xref py py-class docutils literal notranslate"><span class="pre">OfflinePreLearner</span></code></a> as demonstrated below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">msgpack</span>
<span class="kn">import</span> <span class="nn">msgpack_numpy</span> <span class="k">as</span> <span class="nn">mnp</span>

<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">data</span>
<span class="kn">from</span> <span class="nn">ray.rllib.offline.offline_prelearner</span> <span class="kn">import</span> <span class="n">OfflinePreLearner</span>

<span class="c1"># Set up the data path to your `SampleBatch` expert data.</span>
<span class="n">data_path</span> <span class="o">=</span> <span class="o">...</span>
<span class="c1"># Set up the write path for the Parquet episode data.</span>
<span class="n">output_data_path</span> <span class="o">=</span> <span class="s2">&quot;/tmp/sample_batch_data&quot;</span>

<span class="c1"># Load the `SampleBatch` recordings.</span>
<span class="n">ds</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">read_json</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>

<span class="c1"># Iterate over batches (of `SampleBatch`es) and convert them to episodes.</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ds</span><span class="o">.</span><span class="n">iter_batches</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">prefetch_batches</span><span class="o">=</span><span class="mi">2</span><span class="p">)):</span>
    <span class="c1"># Use the RLlib&#39;s `OfflinePreLearner` to convert `SampleBatch`es to episodes.</span>
    <span class="n">episodes</span> <span class="o">=</span> <span class="n">OfflinePreLearner</span><span class="o">.</span><span class="n">_map_sample_batch_to_episode</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">batch</span><span class="p">)[</span><span class="s2">&quot;episodes&quot;</span><span class="p">]</span>

    <span class="c1"># Create a dataset from the episodes. Note, for storing episodes you need to</span>
    <span class="c1"># serialize them through `msgpack-numpy`.</span>
    <span class="n">episode_ds</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">from_items</span><span class="p">([</span><span class="n">msgpack</span><span class="o">.</span><span class="n">packb</span><span class="p">(</span><span class="n">eps</span><span class="o">.</span><span class="n">get_state</span><span class="p">(),</span> <span class="n">default</span><span class="o">=</span><span class="n">mnp</span><span class="o">.</span><span class="n">encode</span><span class="p">)</span> <span class="k">for</span> <span class="n">eps</span> <span class="ow">in</span> <span class="n">episodes</span><span class="p">])</span>
    <span class="c1"># Write the batch of episodes to local disk.</span>
    <span class="n">episode_ds</span><span class="o">.</span><span class="n">write_parquet</span><span class="p">(</span><span class="n">output_data_path</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;/file-</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">zfill</span><span class="p">(</span><span class="mi">6</span><span class="p">),</span> <span class="n">compression</span><span class="o">=</span><span class="s2">&quot;gzip&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Finished converting `SampleBatch` data to episode data.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>RLlib considers your <code class="xref py py-class docutils literal notranslate"><span class="pre">SampleBatch</span></code> to represent a terminated/truncated episode and builds its <a class="reference internal" href="package_ref/env/env/ray.rllib.env.single_agent_episode.SingleAgentEpisode.html#ray.rllib.env.single_agent_episode.SingleAgentEpisode" title="ray.rllib.env.single_agent_episode.SingleAgentEpisode"><code class="xref py py-class docutils literal notranslate"><span class="pre">SingleAgentEpisode</span></code></a>
according to this assumption.</p>
</div>
</section>
<section id="pre-processing-filtering-and-post-processing">
<h2>Pre-processing, filtering and post-processing<a class="headerlink" href="#pre-processing-filtering-and-post-processing" title="Link to this heading">#</a></h2>
<p>During recording, your expert policy may utilize pre-processing techniques for observations, such as <em>frame-stacking</em>, or filtering methods like <em>mean-std filtering</em>. Similarly, actions may undergo pre-processing, such as <em>action
sampling</em> or <em>scaling</em>. In its <code class="docutils literal notranslate"><span class="pre">EnvRunner</span></code> instances, RLlib applies such pre-processing and filtering (through the <em>env-to-module</em> connector pipeline) <strong>before</strong> observations are passed to the <code class="docutils literal notranslate"><span class="pre">RLModule</span></code>. However, raw observations (as received
directly from the environment) are stored in the episodes. Likewise, actions are recorded in their raw form (as output directly from the <code class="docutils literal notranslate"><span class="pre">RLModule</span></code>) while undergoing pre-processing (through RLlib’s <em>module-to-env</em> connectors) before being
sent to the environment.</p>
<p>It’s crucial to carefully consider the pre-processing and filtering applied during the recording of experiences, as they significantly influence how the expert policy learns and subsequently performs in the environment. For example, if
the expert policy uses <em>mean-std filtering</em> for observations, it learns a strategy based on the filtered observations, where the filter itself is highly dependent on the experiences collected during training. When deploying this expert
policy, it’s essential to use the exact same filter during evaluation to avoid performance degradation. Similarly, a policy trained through behavior cloning may also require a <em>mean-std filter</em> for observations to accurately replicate the
behavior of the expert policy.</p>
</section>
<section id="scaling-i-o-throughput">
<h2>Scaling I/O throughput<a class="headerlink" href="#scaling-i-o-throughput" title="Link to this heading">#</a></h2>
<p>Just as online training can be scaled, offline recording I/O throughput can also be increased by configuring the number of RLlib env-runners. Use the <code class="docutils literal notranslate"><span class="pre">num_env_runners</span></code> setting to scale recording during training or <code class="docutils literal notranslate"><span class="pre">evaluation_num_env_runners</span></code>
for scaling during evaluation-only recording. Each worker operates independently, writing experiences in parallel, enabling linear scaling of I/O throughput for write operations. Within each <a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner.html#ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner" title="ray.rllib.offline.offline_env_runner.OfflineSingleAgentEnvRunner"><code class="xref py py-class docutils literal notranslate"><span class="pre">OfflineSingleAgentEnvRunner</span></code></a>, episodes
are sampled and serialized before being written to disk.</p>
<p>Offline RL training in RLlib is highly parallelized, encompassing data reading, post-processing, and, if applicable, updates. When training on offline data, scalability is achieved by increasing the number of <code class="docutils literal notranslate"><span class="pre">DataWorker</span></code> instances used to
transform offline experiences into a learner-compatible format (<code class="xref py py-class docutils literal notranslate"><span class="pre">MultiAgentBatch</span></code>). Ray Data optimizes reading operations under the hood by leveraging file metadata, predefined concurrency settings for batch post-processing, and available
system resources. It’s strongly recommended not to override these defaults, as doing so may disrupt this optimization process.</p>
<p>Data processing in RLlib involves three key layers, all of which are highly scalable:</p>
<ol class="arabic simple">
<li><p><strong>Read Operations:</strong> This layer handles data ingestion from files in a specified folder. It’s automatically optimized by Ray Data and shouldn’t be manually scaled or adjusted.</p></li>
<li><p><strong>Post-processing (PreLearner):</strong> In this stage, batches are converted, if necessary, into RLlib’s <a class="reference internal" href="package_ref/env/env/ray.rllib.env.single_agent_episode.SingleAgentEpisode.html#ray.rllib.env.single_agent_episode.SingleAgentEpisode" title="ray.rllib.env.single_agent_episode.SingleAgentEpisode"><code class="xref py py-class docutils literal notranslate"><span class="pre">SingleAgentEpisode</span></code></a> format and passed through the <em>learner connector pipeline</em>. The processed data is then transformed into <code class="xref py py-class docutils literal notranslate"><span class="pre">MultiAgentBatch</span></code> objects for updating. This layer can be scaling the <code class="docutils literal notranslate"><span class="pre">DataWorker</span></code> instances.</p></li>
<li><p><strong>Updating (Learner):</strong> This stage involves updating the policy and associated modules. Scalability is achieved by increasing the number of learners (<code class="docutils literal notranslate"><span class="pre">num_learners</span></code>), enabling parallel processing of batches during updates.</p></li>
</ol>
<p>The diagram below illustrates the layers and their scalability:</p>
<a class="reference internal image-reference" href="../_images/key_layers.svg"><img alt="Key layers of RLlib's fully scalable Offline RL API." src="../_images/key_layers.svg" width="500" /></a>
<p><strong>Read operations</strong> are executed exclusively on the CPU and are primarily scaled by allocating additional resources (see <a class="reference internal" href="#how-to-tune-performance"><span class="std std-ref">How to tune performance</span></a> for details), as they’re fully managed by Ray Data. <strong>Post-processing</strong> can be scaled by increasing
the concurrency level specified in the keyword arguments for the mapping operation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">AlgorithmConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">offline_data</span><span class="p">(</span>
        <span class="n">map_batches_kwargs</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;concurrency&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
            <span class="s2">&quot;num_cpus&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
        <span class="p">}</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>This initiates an actor pool with 10 <code class="docutils literal notranslate"><span class="pre">DataWorker</span></code> instances, each running an instance of RLlib’s callable <a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.html#ray.rllib.offline.offline_prelearner.OfflinePreLearner" title="ray.rllib.offline.offline_prelearner.OfflinePreLearner"><code class="xref py py-class docutils literal notranslate"><span class="pre">OfflinePreLearner</span></code></a> class to post-process batches for updating the
<a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.html#ray.rllib.core.rl_module.rl_module.RLModule" title="ray.rllib.core.rl_module.rl_module.RLModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">RLModule</span></code></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">num_cpus</span></code> (and similarly the <code class="docutils literal notranslate"><span class="pre">num_gpus</span></code>) attribute defines the resources <strong>allocated to each</strong> <code class="docutils literal notranslate"><span class="pre">DataWorker</span></code> not the full actor pool.</p>
</div>
<p>You scale the number of learners in RLlib’s <a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learners.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learners" title="ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learners"><code class="xref py py-meth docutils literal notranslate"><span class="pre">learners()</span></code></a> configuration block:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">AlgorithmConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">learners</span><span class="p">(</span>
        <span class="n">num_learners</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">num_gpus_per_learner</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>With this configuration you start an application with 4 (remote) <code class="xref py py-class docutils literal notranslate"><span class="pre">Learner`s</span> <span class="pre">(see</span> <span class="pre">:ref:`Learner</span> <span class="pre">(Alpha)</span></code> for more details about RLlib’s learners)
each of them using a single GPU.</p>
</section>
<section id="using-cloud-storage">
<h2>Using cloud storage<a class="headerlink" href="#using-cloud-storage" title="Link to this heading">#</a></h2>
<p>Unlike RLlib’s previous stack, the new Offline RL API is cloud-agnostic and fully integrates with PyArrow. You can utilize any available cloud storage path or PyArrow-compatible filesystem. If
using a PyArrow or compatible filesystem, ensure that your <code class="docutils literal notranslate"><span class="pre">input_</span></code> path is a relative path within this filesystem. Similar to Ray Data, you can also use placeholders, lists of files
or folders, or simply specify a single folder to read recursively from.</p>
<p>For example, to read from a storage bucket in GCS, you can specify the folder location as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span><span class="o">=</span><span class="p">(</span>
    <span class="n">AlgorithmConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">offline_data</span><span class="p">(</span>
        <span class="n">input_</span><span class="o">=</span><span class="s2">&quot;gs://&lt;your-bucket&gt;/dir1&quot;</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>This configuration allows RLlib to read data recursively from any folder beneath the specified path. If you are using a filesystem for GCS (for instance, due to authentication requirements),
use the following syntax:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyarrow.fs</span>

<span class="c1"># Define the PyArrow filesystem</span>
<span class="n">gcs</span> <span class="o">=</span> <span class="n">pyarrow</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">GcsFilesystem</span><span class="p">(</span>
    <span class="c1"># This is needed to resolve the hostname for public buckets.</span>
    <span class="n">anonymous</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">retry_time_limit</span><span class="o">=</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Define the configuration.</span>
<span class="n">config</span><span class="o">=</span> <span class="p">(</span>
    <span class="n">AlgorithmConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">offline_data</span><span class="p">(</span>
        <span class="c1"># NOTE: Use a relative file path now</span>
        <span class="n">input_</span><span class="o">=</span><span class="s2">&quot;&lt;public-bucket&gt;/dir1&quot;</span><span class="p">,</span>
        <span class="n">input_filesystem</span><span class="o">=</span><span class="n">gcs</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>You can learn more about PyArrow’s filesystems, particularly regarding cloud filesystems and required authentication, in <a class="reference external" href="https://arrow.apache.org/docs/python/filesystems.html#filesystem-interface">PyArrow Filesystem Interface</a>.</p>
<section id="using-cloud-storage-for-recording">
<h3>Using cloud storage for recording<a class="headerlink" href="#using-cloud-storage-for-recording" title="Link to this heading">#</a></h3>
<p>You can use cloud storage in a similar way when recording experiences from an expert policy:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span><span class="o">=</span> <span class="p">(</span>
    <span class="n">AlgorithmConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">offline_data</span><span class="p">(</span>
        <span class="n">output</span><span class="o">=</span><span class="s2">&quot;gs://&lt;your-bucket&gt;/dir1&quot;</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>RLlib writes then directly into the folder in the cloud storage and creates it if not already existent in the bucket. The only difference to reading is that you can’t use multiple paths for writing.
So something like</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span><span class="o">=</span> <span class="p">(</span>
    <span class="n">AlgorithmConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">offline_data</span><span class="p">(</span>
        <span class="n">output</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;gs://&lt;your-bucket&gt;/dir1&quot;</span><span class="p">,</span> <span class="s2">&quot;gs://&lt;your-bucket&gt;/dir2&quot;</span><span class="p">],</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>would not work. If the storage requires special permissions for creating folders and/or writing files, ensure that the cluster user is granted the necessary permissions. Failure to do so results
in denied write access, causing the recording process to stop.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using cloud storage, Ray Data typically streams data, meaning it’s consumed in chunks. This allows postprocessing and training to begin after a brief warmup phase. More specifically, even if your cloud storage is large, the same amount of
space isn’t required on the nodes running RLlib.</p>
</div>
</section>
</section>
<section id="how-to-tune-performance">
<span id="id1"></span><h2>How to tune performance<a class="headerlink" href="#how-to-tune-performance" title="Link to this heading">#</a></h2>
<p>In RLlib’s Offline RL API the various key layers are managed by distinct modules and configurations, making it non-trivial to scale these layers effectively. It’s important to understand the specific parameters and their respective impact on system performance.</p>
<section id="how-to-tune-reading-operations">
<span id="id2"></span><h3>How to tune reading operations<a class="headerlink" href="#how-to-tune-reading-operations" title="Link to this heading">#</a></h3>
<p>As noted earlier, the <strong>Reading Operations</strong> layer is automatically handled and dynamically optimized by <a class="reference internal" href="../data/data.html#data"><span class="std std-ref">Ray Data</span></a>. It’s strongly recommended to avoid modifying this process. However, there are certain parameters that can enhance performance on this
layer to some extent, including:</p>
<ol class="arabic simple">
<li><p>Available resources (dedicated to the job).</p></li>
<li><p>Data locality.</p></li>
<li><p>Data sharding.</p></li>
<li><p>Data pruning.</p></li>
</ol>
<section id="available-resources">
<h4>Available resources<a class="headerlink" href="#available-resources" title="Link to this heading">#</a></h4>
<p>The scheduling strategy employed by <a class="reference internal" href="../data/data.html#data"><span class="std std-ref">Ray Data</span></a> operates independently of any existing placement group, scheduling tasks and actors separately. Consequently, it’s essential to reserve adequate resources for other tasks and actors within your job. To
optimize <a class="reference internal" href="../data/data.html#data"><span class="std std-ref">Ray Data</span></a>’s scalability for read operations and improve reading performance, consider increasing the available resources in your cluster while preserving the resource allocation for existing tasks and actors. The key resources to monitor and
provision are CPUs and object store memory. Insufficient object store memory, especially under heavy backpressure, may lead to objects being spilled to disk, which can severely impact application performance.</p>
<p>Bandwidth is a crucial factor influencing the throughput within your cluster. In some cases, scaling the number of nodes can increase bandwidth, thereby enhancing the flow of data from storage to consuming processes. Scenarios where this approach is beneficial
include:</p>
<ul class="simple">
<li><p>Independent connections to the network backbone: Nodes utilize dedicated bandwidth, avoiding shared up-links and potential bottlenecks (see for ex. <a class="reference external" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-network-bandwidth.html">here</a> for AWS and <a class="reference external" href="https://cloud.google.com/compute/docs/network-bandwidth?hl=en">here</a> for GCP network bandwidth documentations).</p></li>
<li><p>Optimized cloud access: Employing features like <a class="reference external" href="https://aws.amazon.com/s3/transfer-acceleration/">S3 Transfer Acceleration</a>, <a class="reference external" href="https://cloud.google.com/storage/docs/cloud-storage-fuse/file-caching#configure-parallel-downloads">Google Cloud Storage FUSE</a> , or parallel and accelerated data transfer methods to enhance performance.</p></li>
</ul>
</section>
<section id="data-locality">
<h4>Data locality<a class="headerlink" href="#data-locality" title="Link to this heading">#</a></h4>
<p>Data locality is a critical factor in achieving fast data processing. For instance, if your data resides on GCP, running a Ray cluster on AWS S3 or a local machine inevitably results in low transfer rates and slow data processing. To ensure optimal performance, storing data within the same region, same zone and cloud provider as the Ray cluster is generally
sufficient to enable efficient streaming for RLlib’s Offline RL API. Additional adjustments to consider include:</p>
<ul class="simple">
<li><p>Multi-Region Buckets: Use multi-region storage to improve data availability and potentially enhance access speeds for distributed systems.</p></li>
<li><p>Storage class optimization within buckets: Use <strong>standard storage</strong> for frequent access and low-latency streaming. Avoid archival storage classes like AWS Glacier or GCP Archive for streaming workloads due to high retrieval times.</p></li>
</ul>
</section>
<section id="data-sharding">
<h4>Data sharding<a class="headerlink" href="#data-sharding" title="Link to this heading">#</a></h4>
<p>Data sharding improves the efficiency of fetching, transferring, and reading data by balancing chunk sizes. If chunks are too large, they can cause delays during transfer and processing, leading to bottlenecks. Conversely, chunks that are too small can result in high metadata fetching overhead, slowing down overall performance. Finding an optimal chunk size is
critical for balancing these trade-offs and maximizing throughput.</p>
<ul class="simple">
<li><p>As a rule-of-thumb keep data file sizes in between 64MiB to 256MiB.</p></li>
</ul>
</section>
<section id="data-pruning">
<h4>Data pruning<a class="headerlink" href="#data-pruning" title="Link to this heading">#</a></h4>
<p>If your data is in <strong>Parquet</strong> format (the recommended offline data format for RLlib), you can leverage data pruning to optimize performance. <a class="reference internal" href="../data/data.html#data"><span class="std std-ref">Ray Data</span></a> supports pruning in its <a class="reference internal" href="../data/api/doc/ray.data.read_parquet.html#ray.data.read_parquet" title="ray.data.read_parquet"><code class="xref py py-meth docutils literal notranslate"><span class="pre">read_parquet()</span></code></a> method through projection pushdown (column filtering) and filter pushdown (row filtering). These filters are applied directly during file
scans, reducing the amount of unnecessary data loaded into memory.</p>
<p>For instance, if you only require specific columns from your offline data (for example, to avoid loading the <code class="docutils literal notranslate"><span class="pre">infos</span></code> column):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm_config</span> <span class="kn">import</span> <span class="n">AlgorithmConfig</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.columns</span> <span class="kn">import</span> <span class="n">Columns</span>

<span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">AlgorithmConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">offline_Data</span><span class="p">(</span>
        <span class="n">input_read_method_kwargs</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;columns&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="n">Columns</span><span class="o">.</span><span class="n">EPS_ID</span><span class="p">,</span>
                <span class="n">Columns</span><span class="o">.</span><span class="n">AGENT_ID</span><span class="p">,</span>
                <span class="n">Columns</span><span class="o">.</span><span class="n">OBS</span><span class="p">,</span>
                <span class="n">Columns</span><span class="o">.</span><span class="n">NEXT_OBS</span><span class="p">,</span>
                <span class="n">Columns</span><span class="o">.</span><span class="n">REWARDS</span><span class="p">,</span>
                <span class="n">Columns</span><span class="o">.</span><span class="n">ACTIONS</span><span class="p">,</span>
                <span class="n">Columns</span><span class="o">.</span><span class="n">TERMINATED</span><span class="p">,</span>
                <span class="n">Columns</span><span class="o">.</span><span class="n">TRUNCATED</span><span class="p">,</span>
            <span class="p">],</span>
        <span class="p">},</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Similarly, if you only require specific rows from your dataset, you can apply pushdown filters as shown below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyarrow.dataset</span>

<span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm_config</span> <span class="kn">import</span> <span class="n">AlgorithmConfig</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.columns</span> <span class="kn">import</span> <span class="n">Columns</span>

<span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">AlgorithmConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">offline_data</span><span class="p">(</span>
        <span class="n">input_read_method_kwargs</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;filter&quot;</span><span class="p">:</span> <span class="n">pyarrow</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">field</span><span class="p">(</span><span class="n">Columns</span><span class="o">.</span><span class="n">AGENT_ID</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;agent_1&quot;</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="how-to-tune-post-processing-prelearner">
<h3>How to tune post-processing (PreLearner)<a class="headerlink" href="#how-to-tune-post-processing-prelearner" title="Link to this heading">#</a></h3>
<p>When enabling high throughput in Read Operations, it’s essential to ensure sufficient processing capacity in the Post-Processing (Pre-Learner) stage. Insufficient capacity in this stage can cause backpressure, leading to increased memory usage and, in severe cases,
object spilling to disk or even Out-Of-Memory (see <a class="reference internal" href="../ray-core/scheduling/ray-oom-prevention.html#ray-oom-prevention"><span class="std std-ref">Out-Of-Memory Prevention</span></a>) errors.</p>
<p>Tuning the <strong>Post-Processing (Pre-Learner)</strong> layer is generally more straightforward than optimizing the <strong>Read Operations</strong> layer. Tuning the Post-Processing (Pre-Learner) layer is generally more straightforward than optimizing the Read Operations layer. The following parameters can be adjusted to optimize its performance:</p>
<ul class="simple">
<li><p>Actor Pool Size</p></li>
<li><p>Allocated Resources</p></li>
<li><p>Read Batch and Buffer Sizes.</p></li>
</ul>
<section id="actor-pool-size">
<h4>Actor pool size<a class="headerlink" href="#actor-pool-size" title="Link to this heading">#</a></h4>
<p>Internally, the <strong>Post-Processing (PreLearner)</strong> layer is defined by a <a class="reference internal" href="../data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches" title="ray.data.Dataset.map_batches"><code class="xref py py-meth docutils literal notranslate"><span class="pre">map_batches()</span></code></a> operation that starts an <code class="xref py py-class docutils literal notranslate"><span class="pre">_ActorPool</span></code>. Each actor in this pool runs an <a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.html#ray.rllib.offline.offline_prelearner.OfflinePreLearner" title="ray.rllib.offline.offline_prelearner.OfflinePreLearner"><code class="xref py py-class docutils literal notranslate"><span class="pre">OfflinePreLearner</span></code></a>
instances to transform batches on their way from disk to RLlib’s <a class="reference internal" href="package_ref/doc/ray.rllib.core.learner.learner.Learner.html#ray.rllib.core.learner.learner.Learner" title="ray.rllib.core.learner.learner.Learner"><code class="xref py py-class docutils literal notranslate"><span class="pre">Learner</span></code></a>. Obviously, the size of this <code class="xref py py-class docutils literal notranslate"><span class="pre">_ActorPool</span></code> defines the throughput of this layer and needs to be fine-tuned in regard to the pervious layer’s
throughput to avoid backpressure. You can use the <code class="docutils literal notranslate"><span class="pre">concurrency</span></code> in RLlib’s <code class="docutils literal notranslate"><span class="pre">map_batches_kwargs</span></code> parameter to define this pool size:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithm_config</span> <span class="kn">import</span> <span class="n">AlgorithmConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">AlgorithmConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">offline_data</span><span class="p">(</span>
        <span class="n">map_batches_kwargs</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;concurrency&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>With the preceding code you would enable <a class="reference internal" href="../data/data.html#data"><span class="std std-ref">Ray Data</span></a> to start up to <code class="docutils literal notranslate"><span class="pre">4</span></code> parallel <a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.html#ray.rllib.offline.offline_prelearner.OfflinePreLearner" title="ray.rllib.offline.offline_prelearner.OfflinePreLearner"><code class="xref py py-class docutils literal notranslate"><span class="pre">OfflinePreLearner</span></code></a> actors that can post-process your data for training.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="../data/data.html#data"><span class="std std-ref">Ray Data</span></a> dynamically adjusts its read operations based on the parallelism of your <strong>Post-Processing (Pre-Learner)</strong> layer. It scales read operations up or down depending on the backpressure
in the <strong>Post-Processing (Pre-Learner)</strong> stage. This means the throughput of your entire streaming pipeline is determined by the performance of the downstream tasks and the resources allocated to the
<strong>Reading Operations</strong> layer (see <a class="reference internal" href="#how-to-tune-reading-operations"><span class="std std-ref">How to tune reading operations</span></a>). However, due to the overhead associated with scaling reading operations up or down, backpressure - and
in severe cases, object spilling or Out-Of-Memory (OOM) errors - can’t always be entirely avoided.</p>
</div>
<p>You can also enable auto-scaling in your <strong>Post-Processing (PreLearner)</strong> by providing an interval instead of a straight number:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithm_config</span> <span class="kn">import</span> <span class="n">AlgorithmConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">AlgorithmConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">offline_data</span><span class="p">(</span>
        <span class="n">map_batches_kwargs</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;concurrency&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
        <span class="p">},</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>This allows <a class="reference internal" href="../data/data.html#data"><span class="std std-ref">Ray Data</span></a> to start up to <code class="docutils literal notranslate"><span class="pre">8</span></code> post-processing actors to downstream data faster, for example in case of backpressure.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Implementing an autoscaled actor pool in the <strong>Post-Processing (Pre-Learner)</strong> layer doesn’t guarantee you the elimination of backpressure. Adding more <code class="xref py py-class docutils literal notranslate"><span class="pre">OffLinePreLearner</span></code> instances introduces additional overhead to the system. RLlib’s offline RL pipeline is
optimized for streaming data, which typically exhibits stable throughput and resource usage, except in cases of imbalances between upstream and downstream tasks. As a rule of thumb, consider using autoscaling only under the following conditions: (1) throughput is expected to be highly variable, (2) Cluster resources
are subject to fluctuations (for example, in shared or dynamic environments), and/or (3) workload characteristics are highly unpredictable.</p>
</div>
</section>
<section id="allocated-resources">
<h4>Allocated resources<a class="headerlink" href="#allocated-resources" title="Link to this heading">#</a></h4>
<p>Other than the number of post-processing actors you can tune performance on the <strong>Post-Processing (PreLearner)</strong> layer through defining resources to be allocated to each <code class="xref py py-class docutils literal notranslate"><span class="pre">OffLinePreLearner</span></code> in the actor pool. Such resources can be defined either through <code class="docutils literal notranslate"><span class="pre">num_cpus</span></code> and <code class="docutils literal notranslate"><span class="pre">num_gpus</span></code>
or in the <code class="docutils literal notranslate"><span class="pre">ray_remote_args</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Typically, increasing the number of CPUs is sufficient for performance tuning in the post-processing stage of your pipeline. GPUs are only needed in specialized cases, such as in customized pipelines. For example, RLlib’s <code class="xref py py-class docutils literal notranslate"><span class="pre">MARWIL</span></code> implementation uses the
<code class="xref py py-class docutils literal notranslate"><span class="pre">GeneralAdvantageEstimation</span></code> connector in its <code class="xref py py-class docutils literal notranslate"><span class="pre">ConnectorPipelineV2</span></code> to apply <a class="reference external" href="https://arxiv.org/abs/1506.02438">General Advantage Estimation</a> on experience batches. In these calculations, the value model of the algorithm’s
<code class="xref py py-class docutils literal notranslate"><span class="pre">RLModule</span></code> is applied, which you can accelerate by running on a GPU.</p>
</div>
<p>As an example, to provide each of your <code class="docutils literal notranslate"><span class="pre">4</span></code> <a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.html#ray.rllib.offline.offline_prelearner.OfflinePreLearner" title="ray.rllib.offline.offline_prelearner.OfflinePreLearner"><code class="xref py py-class docutils literal notranslate"><span class="pre">OfflinePreLearner</span></code></a> in the <strong>Post-Processing (PreLearner)</strong> <code class="docutils literal notranslate"><span class="pre">2</span></code> CPUs you can use the following syntax:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm_config</span> <span class="kn">import</span> <span class="n">AlgorithmConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">AlgorithmConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">offline_data</span><span class="p">(</span>
        <span class="n">map_batches_kwargs</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;concurrency&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
            <span class="s2">&quot;num_cpus&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Don’t override the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> in RLlib’s <code class="docutils literal notranslate"><span class="pre">map_batches_kwargs</span></code>. This usually leads to high performance degradations. Note, this <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> differs from the <code class="code docutils literal notranslate"><span class="pre">train_batch_size_per_learner</span></code>: the former specifies the batch size in transformations of
the streaming pipeline, while the latter defines the batch size used for training within each <a class="reference internal" href="package_ref/doc/ray.rllib.core.learner.learner.Learner.html#ray.rllib.core.learner.learner.Learner" title="ray.rllib.core.learner.learner.Learner"><code class="xref py py-class docutils literal notranslate"><span class="pre">Learner</span></code></a> (the batch size of the actual model forward- and backward passes performed for training).</p>
</div>
</section>
<section id="read-batch-and-buffer-sizes">
<h4>Read batch- and buffer sizes<a class="headerlink" href="#read-batch-and-buffer-sizes" title="Link to this heading">#</a></h4>
<p>When working with data from <a class="reference internal" href="package_ref/env/env/ray.rllib.env.single_agent_episode.SingleAgentEpisode.html#ray.rllib.env.single_agent_episode.SingleAgentEpisode" title="ray.rllib.env.single_agent_episode.SingleAgentEpisode"><code class="xref py py-class docutils literal notranslate"><span class="pre">SingleAgentEpisode</span></code></a> or the legacy <code class="xref py py-class docutils literal notranslate"><span class="pre">SampleBatch</span></code> format, fine-tuning the <code class="code docutils literal notranslate"><span class="pre">input_read_batch_size</span></code> parameter provides additional optimization opportunities. This parameter controls the size of batches retrieved from data
files. Its effectiveness is particularly notable when handling episodic or legacy <code class="xref py py-class docutils literal notranslate"><span class="pre">SampleBatch</span></code> data because the streaming pipeline utilizes for these data an <code class="xref py py-class docutils literal notranslate"><span class="pre">EpisodeReplayBuffer</span></code> to handle the multiple timesteps contained in each
data row. All incoming data is converted into <a class="reference internal" href="package_ref/env/env/ray.rllib.env.single_agent_episode.SingleAgentEpisode.html#ray.rllib.env.single_agent_episode.SingleAgentEpisode" title="ray.rllib.env.single_agent_episode.SingleAgentEpisode"><code class="xref py py-class docutils literal notranslate"><span class="pre">SingleAgentEpisode</span></code></a> instances - if not already in this format - and stored in an episode replay buffer, which precisely manages the sampling of <code class="code docutils literal notranslate"><span class="pre">train_batch_size_per_learner</span></code> for training.</p>
<a class="reference internal image-reference" href="../_images/docs_rllib_offline_prelearner.svg"><img alt="The OfflinePreLearner converts and buffers episodes before sampling the batches used in learning." class="align-left" src="../_images/docs_rllib_offline_prelearner.svg" width="500" /></a>
<p>Achieving an optimal balance between data ingestion efficiency and sampling variation in your streaming pipeline is crucial. Consider the following example: suppose each <a class="reference internal" href="package_ref/env/env/ray.rllib.env.single_agent_episode.SingleAgentEpisode.html#ray.rllib.env.single_agent_episode.SingleAgentEpisode" title="ray.rllib.env.single_agent_episode.SingleAgentEpisode"><code class="xref py py-class docutils literal notranslate"><span class="pre">SingleAgentEpisode</span></code></a> has a length of <code class="docutils literal notranslate"><span class="pre">100</span></code> timesteps, and your <code class="code docutils literal notranslate"><span class="pre">train_batch_size_per_learner</span></code> is configured to be <code class="docutils literal notranslate"><span class="pre">1000</span></code>.
Each <code class="xref py py-class docutils literal notranslate"><span class="pre">EpisodeReplayBuffer</span></code> instance is set with a capacity of <code class="docutils literal notranslate"><span class="pre">1000</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm_config</span> <span class="kn">import</span> <span class="n">AlgorithmConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">AlgorithmConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">training</span><span class="p">(</span>
        <span class="c1"># Train on a batch of 1000 timesteps each iteration.</span>
        <span class="n">train_batch_size_per_learner</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="o">.</span><span class="n">offline_data</span><span class="p">(</span>
        <span class="c1"># Read in RLlib&#39;s new stack `SingleAgentEpisode` data.</span>
        <span class="n">input_read_episodes</span><span class="o">=</span><span class="kc">True</span>
        <span class="c1"># Define an input read batch size of 10 episodes.</span>
        <span class="n">input_read_batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="c1"># Set the replay buffer in the `OfflinePrelearner`</span>
        <span class="c1"># to 1,000 timesteps.</span>
        <span class="n">prelearner_buffer_kwargs</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;capacity&quot;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>If you configure <code class="code docutils literal notranslate"><span class="pre">input_read_batch_size</span></code> to <code class="docutils literal notranslate"><span class="pre">10</span></code> as shown in the code, each of the <code class="docutils literal notranslate"><span class="pre">10</span></code> <a class="reference internal" href="package_ref/env/env/ray.rllib.env.single_agent_episode.SingleAgentEpisode.html#ray.rllib.env.single_agent_episode.SingleAgentEpisode" title="ray.rllib.env.single_agent_episode.SingleAgentEpisode"><code class="xref py py-class docutils literal notranslate"><span class="pre">SingleAgentEpisode</span></code></a> fit into the buffer, enabling sampling across a wide variety of timesteps from multiple episodes. This results in high sampling variation. Now, consider the case where the buffer
capacity is reduced to <code class="docutils literal notranslate"><span class="pre">500</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm_config</span> <span class="kn">import</span> <span class="n">AlgorithmConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">AlgorithmConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">training</span><span class="p">(</span>
        <span class="c1"># Train on a batch of 1000 timesteps each iteration.</span>
        <span class="n">train_batch_size_per_learner</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="o">.</span><span class="n">offline_data</span><span class="p">(</span>
        <span class="c1"># Read in RLlib&#39;s new stack `SingleAgentEpisode` data.</span>
        <span class="n">input_read_episodes</span><span class="o">=</span><span class="kc">True</span>
        <span class="c1"># Define an input read batch size of 10 episodes.</span>
        <span class="n">input_read_batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="c1"># Set the replay buffer in the `OfflinePrelearner`</span>
        <span class="c1"># to 500 timesteps.</span>
        <span class="n">prelearner_buffer_kwargs</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;capacity&quot;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>With the same <code class="code docutils literal notranslate"><span class="pre">input_read_batch_size</span></code>, only <code class="docutils literal notranslate"><span class="pre">5</span></code> <a class="reference internal" href="package_ref/env/env/ray.rllib.env.single_agent_episode.SingleAgentEpisode.html#ray.rllib.env.single_agent_episode.SingleAgentEpisode" title="ray.rllib.env.single_agent_episode.SingleAgentEpisode"><code class="xref py py-class docutils literal notranslate"><span class="pre">SingleAgentEpisode</span></code></a> can be buffered at a time, causing inefficiencies as more data is read than can be retained for sampling.</p>
<p>In another scenario, if each <a class="reference internal" href="package_ref/env/env/ray.rllib.env.single_agent_episode.SingleAgentEpisode.html#ray.rllib.env.single_agent_episode.SingleAgentEpisode" title="ray.rllib.env.single_agent_episode.SingleAgentEpisode"><code class="xref py py-class docutils literal notranslate"><span class="pre">SingleAgentEpisode</span></code></a> still has a length of <code class="docutils literal notranslate"><span class="pre">100</span></code> timesteps and the <code class="code docutils literal notranslate"><span class="pre">train_batch_size_per_learner</span></code> is set to <code class="docutils literal notranslate"><span class="pre">4000</span></code> timesteps as in the code below, the buffer holds <code class="docutils literal notranslate"><span class="pre">10</span></code> <a class="reference internal" href="package_ref/env/env/ray.rllib.env.single_agent_episode.SingleAgentEpisode.html#ray.rllib.env.single_agent_episode.SingleAgentEpisode" title="ray.rllib.env.single_agent_episode.SingleAgentEpisode"><code class="xref py py-class docutils literal notranslate"><span class="pre">SingleAgentEpisode</span></code></a> instances. This configuration
results in lower sampling variation because many timesteps are repeatedly sampled, reducing diversity across training batches. These examples highlight the importance of tuning these parameters to balance data ingestion and sampling diversity in your offline streaming pipeline effectively.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm_config</span> <span class="kn">import</span> <span class="n">AlgorithmConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">AlgorithmConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">training</span><span class="p">(</span>
        <span class="c1"># Train on a batch of 4000 timesteps each iteration.</span>
        <span class="n">train_batch_size_per_learner</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="o">.</span><span class="n">offline_data</span><span class="p">(</span>
        <span class="c1"># Read in RLlib&#39;s new stack `SingleAgentEpisode` data.</span>
        <span class="n">input_read_episodes</span><span class="o">=</span><span class="kc">True</span>
        <span class="c1"># Define an input read batch size of 10 episodes.</span>
        <span class="n">input_read_batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="c1"># Set the replay buffer in the `OfflinePrelearner`</span>
        <span class="c1"># to 1,000 timesteps.</span>
        <span class="n">prelearner_buffer_kwargs</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;capacity&quot;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>To choose an adequate <code class="code docutils literal notranslate"><span class="pre">input_read_batch_size</span></code> take a look at the length of your recorded episodes. In some cases each single episode is long enough to fulfill the <code class="code docutils literal notranslate"><span class="pre">train_batch_size_per_learner</span></code> and you could choose a <code class="code docutils literal notranslate"><span class="pre">input_read_batch_size</span></code> of <code class="docutils literal notranslate"><span class="pre">1</span></code>. Most times it’s not and you need to consider how many episodes should be buffered to balance
the amount of data digested from read input and the variation of data sampled from the <code class="xref py py-class docutils literal notranslate"><span class="pre">EpisodeReplayBuffer</span></code> instances in the <a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.html#ray.rllib.offline.offline_prelearner.OfflinePreLearner" title="ray.rllib.offline.offline_prelearner.OfflinePreLearner"><code class="xref py py-class docutils literal notranslate"><span class="pre">OfflinePreLearner</span></code></a>.</p>
</div>
</section>
</section>
<section id="how-to-tune-updating-learner">
<h3>How to tune updating (Learner)<a class="headerlink" href="#how-to-tune-updating-learner" title="Link to this heading">#</a></h3>
<p><strong>Updating (Learner)</strong>  is the final downstream task in RLlib’s Offline RL pipeline, and its consumption speed determines the overall throughput of the data pipeline. If the learning process is slow, it can cause backpressure in upstream layers, potentially leading to object spilling or Out-Of-Memory (OOM) errors. Therefore, it’s essential to fine-tune this
layer in coordination with the upstream components. Several parameters can be adjusted to optimize the learning speed in your Offline algorithm:</p>
<ul class="simple">
<li><p>Actor Pool Size</p></li>
<li><p>Allocated Resources</p></li>
<li><p>Scheduling Strategy</p></li>
<li><p>Batch Sizing</p></li>
<li><p>Batch Prefetching</p></li>
<li><p>Learner Iterations.</p></li>
</ul>
</section>
<section id="id3">
<span id="id4"></span><h3>Actor pool size<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>RLlib supports scaling <a class="reference internal" href="package_ref/doc/ray.rllib.core.learner.learner.Learner.html#ray.rllib.core.learner.learner.Learner" title="ray.rllib.core.learner.learner.Learner"><code class="xref py py-class docutils literal notranslate"><span class="pre">Learner</span></code></a> instances through the parameter <code class="code docutils literal notranslate"><span class="pre">num_learners</span></code>. When this value is <code class="docutils literal notranslate"><span class="pre">0</span></code>, RLlib uses a Learner instance in the local process, whereas for values <code class="docutils literal notranslate"><span class="pre">&gt;0</span></code>, RLlib scales out using a <code class="xref py py-class docutils literal notranslate"><span class="pre">backend_executor_BackendExecutor</span></code>. This executor spawns your specified
number of <a class="reference internal" href="package_ref/doc/ray.rllib.core.learner.learner.Learner.html#ray.rllib.core.learner.learner.Learner" title="ray.rllib.core.learner.learner.Learner"><code class="xref py py-class docutils literal notranslate"><span class="pre">Learner</span></code></a> instances, manages distributed training and aggregates intermediate results across <a class="reference internal" href="package_ref/doc/ray.rllib.core.learner.learner.Learner.html#ray.rllib.core.learner.learner.Learner" title="ray.rllib.core.learner.learner.Learner"><code class="xref py py-class docutils literal notranslate"><span class="pre">Learner</span></code></a> actors. <a class="reference internal" href="package_ref/doc/ray.rllib.core.learner.learner.Learner.html#ray.rllib.core.learner.learner.Learner" title="ray.rllib.core.learner.learner.Learner"><code class="xref py py-class docutils literal notranslate"><span class="pre">Learner</span></code></a> scaling increases training throughput and you should only apply it, if the upstream components in your
Offline Data pipeline can supply data at a rate sufficient to match the increased training capacity. RLlib’s Offline API offers powerful scalability at its final layer by utilizing <a class="reference internal" href="../data/api/doc/ray.data.Dataset.streaming_split.html#ray.data.Dataset.streaming_split" title="ray.data.Dataset.streaming_split"><code class="xref py py-class docutils literal notranslate"><span class="pre">streaming_split</span></code></a>. This functionality divides the data stream into multiple substreams, which are then processed by individual
<a class="reference internal" href="package_ref/doc/ray.rllib.core.learner.learner.Learner.html#ray.rllib.core.learner.learner.Learner" title="ray.rllib.core.learner.learner.Learner"><code class="xref py py-class docutils literal notranslate"><span class="pre">Learner</span></code></a> instances, enabling efficient parallel consumption and enhancing overall throughput.</p>
<p>For example to set the number of learners to <code class="docutils literal notranslate"><span class="pre">4</span></code>, you use the following syntax:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm_config</span> <span class="kn">import</span> <span class="n">AlgorithmConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">AlgorithmConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">learners</span><span class="p">(</span><span class="n">num_learners</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<section id="id5">
<h4>Allocated resources<a class="headerlink" href="#id5" title="Link to this heading">#</a></h4>
<p>Just as with the Post-Processing (Pre-Learner) layer, allocating additional resources can help address slow training issues. The primary resource to leverage is the GPU, as training involves forward and backward passes through the <a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.html#ray.rllib.core.rl_module.rl_module.RLModule" title="ray.rllib.core.rl_module.rl_module.RLModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">RLModule</span></code></a>, which GPUs can accelerate significantly. If your training
already utilizes GPUs and performance still remains an issue, consider scaling up by either adding more GPUs to each <a class="reference internal" href="package_ref/doc/ray.rllib.core.learner.learner.Learner.html#ray.rllib.core.learner.learner.Learner" title="ray.rllib.core.learner.learner.Learner"><code class="xref py py-class docutils literal notranslate"><span class="pre">Learner</span></code></a> to increase GPU memory and computational capacity (set <code class="code docutils literal notranslate"><span class="pre">config.learners(num_gpus_per_learner=...)</span></code>), or by adding additional <a class="reference internal" href="package_ref/doc/ray.rllib.core.learner.learner.Learner.html#ray.rllib.core.learner.learner.Learner" title="ray.rllib.core.learner.learner.Learner"><code class="xref py py-class docutils literal notranslate"><span class="pre">Learner</span></code></a> workers to further distribute the workload (by setting <code class="code docutils literal notranslate"><span class="pre">config.learners(num_learners=...)</span></code>). Additionally, ensure that data
throughput and upstream components are optimized to keep the learners fully utilized, as insufficient upstream capacity can bottleneck the training process.</p>
<p>To provide your learners with more compute use <code class="docutils literal notranslate"><span class="pre">num_gpus_per_learner</span></code> or <code class="docutils literal notranslate"><span class="pre">num_cpus_per_learner</span></code> as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm_config</span> <span class="kn">import</span> <span class="n">AlgorithmConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">AlgorithmConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">learners</span><span class="p">(</span><span class="n">num_learners</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">num_gpus_per_learner</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="scheduling-strategy">
<h4>Scheduling strategy<a class="headerlink" href="#scheduling-strategy" title="Link to this heading">#</a></h4>
<p>The scheduling strategy in Ray plays a key role in task and actor placement by attempting to distribute them across multiple nodes in a cluster, thereby maximizing resource utilization and fault tolerance. When running on a single-node cluster (that’s: one large head node), the scheduling strategy has little to no noticeable impact. However, in a multi-node cluster,
scheduling can significantly influence the performance of your Offline Data pipeline due to the importance of data locality. Data processing occurs across all nodes, and maintaining data locality during training can enhance performance.</p>
<p>In such scenarios, you can improve data locality by changing RLlib’s default scheduling strategy from <code class="docutils literal notranslate"><span class="pre">&quot;PACK&quot;</span></code> to <code class="docutils literal notranslate"><span class="pre">&quot;SPREAD&quot;</span></code>. This strategy distributes the <a class="reference internal" href="package_ref/doc/ray.rllib.core.learner.learner.Learner.html#ray.rllib.core.learner.learner.Learner" title="ray.rllib.core.learner.learner.Learner"><code class="xref py py-class docutils literal notranslate"><span class="pre">Learner</span></code></a> actors across the cluster, allowing <code class="code docutils literal notranslate"><span class="pre">Ray</span> <span class="pre">Data</span> <span class="pre">&lt;data&gt;</span></code> to take advantage of locality-aware bundle selection, which can improve efficiency.</p>
<p>Here is an example of how you can change the scheduling strategy:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;Just for show-casing, don&#39;t run.&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">data</span>
<span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm_config.AlgorithmConfig</span>

<span class="c1"># Configure a &quot;SPREAD&quot; scheduling strategy for learners.</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;TRAIN_ENABLE_WORKER_SPREAD_ENV&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>

<span class="c1"># Get the current data context.</span>
<span class="n">data_context</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">DataContext</span><span class="o">.</span><span class="n">get_current</span><span class="p">()</span>
<span class="c1"># Set the execution options such that the Ray Data tries to match</span>
<span class="c1"># the locality of an output stream with where learners are located.</span>
<span class="n">data_context</span><span class="o">.</span><span class="n">execution_options</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">ExecutionOptions</span><span class="p">(</span>
    <span class="n">locality_with_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Build the config.</span>
<span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">AlgorithmConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">learners</span><span class="p">(</span>
        <span class="c1"># Scale the learners.</span>
        <span class="n">num_learners</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">num_gpus_per_learner</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="o">.</span><span class="n">offline_data</span><span class="p">(</span>
        <span class="o">...</span><span class="p">,</span>
        <span class="c1"># Run in each RLlib training iteration 10</span>
        <span class="c1"># iterations per learner (each of them with</span>
        <span class="c1"># `train_batch_size_per_learner`).</span>
        <span class="n">dataset_num_iters_per_learner</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Build the algorithm from the config.</span>
<span class="n">algo</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>

<span class="c1"># Train for 10 iterations.</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="batch-size">
<h4>Batch size<a class="headerlink" href="#batch-size" title="Link to this heading">#</a></h4>
<p>Batch size is one of the simplest parameters to adjust for optimizing performance in RLlib’s new Offline RL API. Small batch sizes may under-utilize hardware, leading to inefficiencies, while overly large batch sizes can exceed memory limits. In a streaming pipeline, the selected batch size impacts how data is partitioned and processed across parallel workers. Larger
batch sizes reduce the overhead of frequent task coordination, but if they exceed hardware constraints, they can slow down the entire pipeline. You can configure the training batch size using the <code class="code docutils literal notranslate"><span class="pre">train_batch_size_per_learner</span></code> attribute as shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm_config</span> <span class="kn">import</span> <span class="n">AlgorithmConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">AlgorithmConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">training</span><span class="p">(</span>
        <span class="n">train_batch_size_per_learner</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>In <code class="code docutils literal notranslate"><span class="pre">Ray</span> <span class="pre">Data</span> <span class="pre">&lt;data&gt;</span></code>, it’s common practice to use batch sizes that are powers of two. However, you are free to select any integer value for the batch size based on your needs.</p>
</section>
<section id="batch-prefetching">
<h4>Batch prefetching<a class="headerlink" href="#batch-prefetching" title="Link to this heading">#</a></h4>
<p>Batch prefetching allows you to control data consumption on the downstream side of your offline data pipeline. The primary goal is to ensure that learners remain active, maintaining a continuous flow of data. This is achieved by preparing the next batch while the learner processes the current one. Prefetching determines how many batches are kept ready for learners
and should be tuned based on the time required to produce the next batch and the learner’s update speed. Prefetching too many batches can lead to memory inefficiencies and, in some cases, backpressure in upstream tasks.</p>
<p>You can configure batch prefetching in the <code class="code docutils literal notranslate"><span class="pre">iter_batches_kwargs</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm_config</span> <span class="kn">import</span> <span class="n">AlgorithmConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">AlgorithmConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">offline_data</span><span class="p">(</span>
        <span class="n">iter_batches_kwargs</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;prefetch_batches&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="p">}</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Don’t override the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> in RLlib’s <code class="code docutils literal notranslate"><span class="pre">map_batches_kwargs</span></code>. This usually leads to high performance degradations. Note, this <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> differs from the <code class="code docutils literal notranslate"><span class="pre">train_batch_size_per_learner</span></code>: the former specifies the batch size
in iterating over data output of the streaming pipeline, while the latter defines the batch size used for training within each <a class="reference internal" href="package_ref/doc/ray.rllib.core.learner.learner.Learner.html#ray.rllib.core.learner.learner.Learner" title="ray.rllib.core.learner.learner.Learner"><code class="xref py py-class docutils literal notranslate"><span class="pre">Learner</span></code></a>.</p>
</div>
</section>
<section id="learner-iterations">
<h4>Learner iterations<a class="headerlink" href="#learner-iterations" title="Link to this heading">#</a></h4>
<p>This tuning parameter is available only when using multiple instances of :<a class="reference internal" href="package_ref/doc/ray.rllib.core.learner.learner.Learner.html#ray.rllib.core.learner.learner.Learner" title="ray.rllib.core.learner.learner.Learner"><code class="xref py py-class docutils literal notranslate"><span class="pre">Learner</span></code></a>. In distributed learning, each <a class="reference internal" href="package_ref/doc/ray.rllib.core.learner.learner.Learner.html#ray.rllib.core.learner.learner.Learner" title="ray.rllib.core.learner.learner.Learner"><code class="xref py py-class docutils literal notranslate"><span class="pre">Learner</span></code></a> instance processes a sub-stream of the offline streaming pipeline, iterating over batches from that sub-stream. You can control the number of iterations each
<a class="reference internal" href="package_ref/doc/ray.rllib.core.learner.learner.Learner.html#ray.rllib.core.learner.learner.Learner" title="ray.rllib.core.learner.learner.Learner"><code class="xref py py-class docutils literal notranslate"><span class="pre">Learner</span></code></a> instance runs per RLlib training iteration. Result reporting occurs after each RLlib training iteration. Setting this parameter too low results in inefficiencies, while excessively high values can hinder training monitoring and, in some cases - such as in RLlib’s <code class="xref py py-class docutils literal notranslate"><span class="pre">MARWIL</span></code>
implementation - lead to stale training data. This happens because some data transformations rely on the same <a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.html#ray.rllib.core.rl_module.rl_module.RLModule" title="ray.rllib.core.rl_module.rl_module.RLModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">RLModule</span></code></a> that the <a class="reference internal" href="package_ref/doc/ray.rllib.core.learner.learner.Learner.html#ray.rllib.core.learner.learner.Learner" title="ray.rllib.core.learner.learner.Learner"><code class="xref py py-class docutils literal notranslate"><span class="pre">Learner</span></code></a> instances are training on. The number of iterations per sub-stream is controlled by the attribute
<code class="xref py py-class docutils literal notranslate"><span class="pre">dataset_num_iters_per_learner</span></code>, which has a default value of <code class="docutils literal notranslate"><span class="pre">None</span></code>, meaning it runs one epoch on the sub-stream.</p>
<p>You can modify this value as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm_config</span> <span class="kn">import</span> <span class="n">AlgorithmConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">AlgorithmConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">offline_data</span><span class="p">(</span>
        <span class="c1"># Train on 20 batches from the substream in each learner.</span>
        <span class="n">dataset_num_iters_per_learner</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="customization">
<h2>Customization<a class="headerlink" href="#customization" title="Link to this heading">#</a></h2>
<p>Customization of the Offline RL components in RLlib, such as the <a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.html#ray.rllib.algorithms.algorithm.Algorithm" title="ray.rllib.algorithms.algorithm.Algorithm"><code class="xref py py-class docutils literal notranslate"><span class="pre">Algorithm</span></code></a>, <a class="reference internal" href="package_ref/doc/ray.rllib.core.learner.learner.Learner.html#ray.rllib.core.learner.learner.Learner" title="ray.rllib.core.learner.learner.Learner"><code class="xref py py-class docutils literal notranslate"><span class="pre">Learner</span></code></a>, or <a class="reference internal" href="package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.html#ray.rllib.core.rl_module.rl_module.RLModule" title="ray.rllib.core.rl_module.rl_module.RLModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">RLModule</span></code></a>, follows a similar process to that of their Online RL counterparts. For detailed guidance, refer to the documentation on <a class="reference internal" href="rllib-algorithms.html#rllib-algorithms-doc"><span class="std std-ref">Algorithms</span></a>,
<a class="reference internal" href="rllib-learner.html#learner-guide"><span class="std std-ref">Learners</span></a>, and RLlib’s <a class="reference internal" href="rllib-rlmodule.html#rlmodule-guide"><span class="std std-ref">RLModule</span></a>. The new stack Offline RL streaming pipeline in RLlib supports customization at various levels and locations within the dataflow, allowing for tailored solutions to meet the specific requirements of your offline RL algorithm.</p>
<ul class="simple">
<li><p>Connector Level</p></li>
<li><p>PreLearner Level</p></li>
<li><p>Pipeline Level.</p></li>
</ul>
<section id="connector-level">
<h3>Connector level<a class="headerlink" href="#connector-level" title="Link to this heading">#</a></h3>
<p>Small data transformations on instances of <a class="reference internal" href="package_ref/env/env/ray.rllib.env.single_agent_episode.SingleAgentEpisode.html#ray.rllib.env.single_agent_episode.SingleAgentEpisode" title="ray.rllib.env.single_agent_episode.SingleAgentEpisode"><code class="xref py py-class docutils literal notranslate"><span class="pre">SingleAgentEpisode</span></code></a> can be easily implemented by modifying the <code class="xref py py-class docutils literal notranslate"><span class="pre">ConnectorPipelineV2</span></code>, which is part of the <a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.html#ray.rllib.offline.offline_prelearner.OfflinePreLearner" title="ray.rllib.offline.offline_prelearner.OfflinePreLearner"><code class="xref py py-class docutils literal notranslate"><span class="pre">OfflinePreLearner</span></code></a> and prepares episodes for training. You can leverage any connector from
RLlib’s library (see <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/connectors">RLlib’s default connectors</a>) or create a custom connector (see <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/connectors">RLlib’s ConnectorV2 examples</a>) to integrate into the <a class="reference internal" href="package_ref/doc/ray.rllib.core.learner.learner.Learner.html#ray.rllib.core.learner.learner.Learner" title="ray.rllib.core.learner.learner.Learner"><code class="xref py py-class docutils literal notranslate"><span class="pre">Learner</span></code></a>’s <code class="xref py py-class docutils literal notranslate"><span class="pre">ConnectorPipelineV2</span></code>.
Careful consideration must be given to the order in which <code class="xref py py-class docutils literal notranslate"><span class="pre">ConnectorV2</span></code> instances are applied, as demonstrated in the implementation of <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/marwil">RLlib’s MARWIL algorithm</a> (see the <a class="reference external" href="https://www.nematilab.info/bmijc/assets/012819_paper.pdf">MARWIL paper</a>).</p>
<p>The <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/marwil">MARWIL algorithm</a> computes a loss that extends beyond behavior cloning by improving the expert’s strategy during training using advantages. These advantages are calculated through <a class="reference external" href="https://arxiv.org/abs/1506.02438">General Advantage Estimation (GAE)</a> using a value model. GAE is computed on-the-fly through the
<code class="xref py py-class docutils literal notranslate"><span class="pre">GeneralAdvantageEstimation</span></code> connector. This connector has specific requirements: it processes a list of <a class="reference internal" href="package_ref/env/env/ray.rllib.env.single_agent_episode.SingleAgentEpisode.html#ray.rllib.env.single_agent_episode.SingleAgentEpisode" title="ray.rllib.env.single_agent_episode.SingleAgentEpisode"><code class="xref py py-class docutils literal notranslate"><span class="pre">SingleAgentEpisode</span></code></a> instances and must be one of the final components in the <code class="xref py py-class docutils literal notranslate"><span class="pre">ConnectorPipelineV2</span></code>. This is because
it relies on fully prepared batches containing <code class="code docutils literal notranslate"><span class="pre">OBS</span></code>, <code class="code docutils literal notranslate"><span class="pre">REWARDS</span></code>, <code class="code docutils literal notranslate"><span class="pre">NEXT_OBS</span></code>, <code class="code docutils literal notranslate"><span class="pre">TERMINATED</span></code>, and <code class="code docutils literal notranslate"><span class="pre">TRUNCATED</span></code> fields. Additionally, the incoming <a class="reference internal" href="package_ref/env/env/ray.rllib.env.single_agent_episode.SingleAgentEpisode.html#ray.rllib.env.single_agent_episode.SingleAgentEpisode" title="ray.rllib.env.single_agent_episode.SingleAgentEpisode"><code class="xref py py-class docutils literal notranslate"><span class="pre">SingleAgentEpisode</span></code></a> instances must already include one artificially elongated timestep.</p>
<p>To meet these requirements, the pipeline must include the following sequence of <code class="xref py py-class docutils literal notranslate"><span class="pre">ConnectorV2</span></code> instances:</p>
<ol class="arabic simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ray.rllib.connectors.learner.add_one_ts_to_episodes_and_truncate.AddOneTsToEpisodesAndTruncate</span></code> ensures the <a class="reference internal" href="package_ref/env/env/ray.rllib.env.single_agent_episode.SingleAgentEpisode.html#ray.rllib.env.single_agent_episode.SingleAgentEpisode" title="ray.rllib.env.single_agent_episode.SingleAgentEpisode"><code class="xref py py-class docutils literal notranslate"><span class="pre">SingleAgentEpisode</span></code></a> objects are elongated by one timestep.</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ray.rllib.connectors.common.add_observations_from_episodes_to_batch.AddObservationsFromEpisodesToBatch</span></code> incorporates the observations (<code class="code docutils literal notranslate"><span class="pre">OBS</span></code>) into the batch.</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ray.rllib.connectors.learner.add_next_observations_from_episodes_to_train_batch.AddNextObservationsFromEpisodesToTrainBatch</span></code> adds the next observations (<code class="code docutils literal notranslate"><span class="pre">NEXT_OBS</span></code>).</p></li>
<li><p>Finally, the <code class="xref py py-class docutils literal notranslate"><span class="pre">ray.rllib.connectors.learner.general_advantage_estimation.GeneralAdvantageEstimation</span></code> connector piece is applied.</p></li>
</ol>
<p>Below is the example code snippet from <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/marwil">RLlib’s MARWIL algorithm</a> demonstrating this setup:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@override</span><span class="p">(</span><span class="n">AlgorithmConfig</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">build_learner_connector</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_observation_space</span><span class="p">,</span>
    <span class="n">input_action_space</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">pipeline</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">build_learner_connector</span><span class="p">(</span>
        <span class="n">input_observation_space</span><span class="o">=</span><span class="n">input_observation_space</span><span class="p">,</span>
        <span class="n">input_action_space</span><span class="o">=</span><span class="n">input_action_space</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Before anything, add one ts to each episode (and record this in the loss</span>
    <span class="c1"># mask, so that the computations at this extra ts aren&#39;t used to compute</span>
    <span class="c1"># the loss).</span>
    <span class="n">pipeline</span><span class="o">.</span><span class="n">prepend</span><span class="p">(</span><span class="n">AddOneTsToEpisodesAndTruncate</span><span class="p">())</span>

    <span class="c1"># Prepend the &quot;add-NEXT_OBS-from-episodes-to-train-batch&quot; connector piece (right</span>
    <span class="c1"># after the corresponding &quot;add-OBS-...&quot; default piece).</span>
    <span class="n">pipeline</span><span class="o">.</span><span class="n">insert_after</span><span class="p">(</span>
        <span class="n">AddObservationsFromEpisodesToBatch</span><span class="p">,</span>
        <span class="n">AddNextObservationsFromEpisodesToTrainBatch</span><span class="p">(),</span>
    <span class="p">)</span>

    <span class="c1"># At the end of the pipeline (when the batch is already completed), add the</span>
    <span class="c1"># GAE connector, which performs a vf forward pass, then computes the GAE</span>
    <span class="c1"># computations, and puts the results of this (advantages, value targets)</span>
    <span class="c1"># directly back in the batch. This is then the batch used for</span>
    <span class="c1"># `forward_train` and `compute_losses`.</span>
    <span class="n">pipeline</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="n">GeneralAdvantageEstimation</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span> <span class="n">lambda_</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">pipeline</span>
</pre></div>
</div>
<section id="define-a-primer-learnerconnector-pipeline">
<h4>Define a primer LearnerConnector pipeline<a class="headerlink" href="#define-a-primer-learnerconnector-pipeline" title="Link to this heading">#</a></h4>
<p>There are multiple ways to customize the <code class="xref py py-class docutils literal notranslate"><span class="pre">LearnerConnectorPipeline</span></code>. One approach, as demonstrated above, is to override the <code class="code docutils literal notranslate"><span class="pre">build_learner_connector</span></code> method in the <a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.html#ray.rllib.algorithms.algorithm.Algorithm" title="ray.rllib.algorithms.algorithm.Algorithm"><code class="xref py py-class docutils literal notranslate"><span class="pre">Algorithm</span></code></a>. Alternatively, you can directly define a custom <code class="xref py py-class docutils literal notranslate"><span class="pre">ConnectorV2</span></code> piece to the
<code class="xref py py-class docutils literal notranslate"><span class="pre">LearnerConnectorPipeline</span></code> by utilizing the <code class="code docutils literal notranslate"><span class="pre">learner_connector</span></code> attribute:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_make_learner_connector</span><span class="p">(</span><span class="n">input_observation_space</span><span class="p">,</span> <span class="n">input_action_space</span><span class="p">):</span>
    <span class="c1"># Create the learner connector.</span>
    <span class="k">return</span> <span class="n">CustomLearnerConnector</span><span class="p">(</span>
        <span class="n">parameter_1</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
        <span class="n">parameter_2</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">AlgorithmConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">training</span><span class="p">(</span>
        <span class="c1"># Add the connector pipeline as the starting point for</span>
        <span class="c1"># the learner connector pipeline.</span>
        <span class="n">learner_connector</span><span class="o">=</span><span class="n">_make_learner_connector</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>As noted in the comments, this approach to adding a <code class="xref py py-class docutils literal notranslate"><span class="pre">ConnectorV2</span></code> piece to the <code class="xref py py-class docutils literal notranslate"><span class="pre">LearnerConnectorPipeline</span></code> is suitable only if you intend to manipulate raw episodes, as your <code class="xref py py-class docutils literal notranslate"><span class="pre">ConnectorV2</span></code> piece serves as the foundation for building the remainder of the pipeline (including batching and other processing
steps). If your goal is to modify data further along in the <code class="xref py py-class docutils literal notranslate"><span class="pre">LearnerConnectorPipeline</span></code>, you should either override the <a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.html#ray.rllib.algorithms.algorithm.Algorithm" title="ray.rllib.algorithms.algorithm.Algorithm"><code class="xref py py-class docutils literal notranslate"><span class="pre">Algorithm</span></code></a>’s <code class="code docutils literal notranslate"><span class="pre">build_learner_connector</span></code> method or consider the third option: overriding the entire <code class="xref py py-class docutils literal notranslate"><span class="pre">PreLearner</span></code>.</p>
</section>
</section>
<section id="prelearner-level">
<h3>PreLearner level<a class="headerlink" href="#prelearner-level" title="Link to this heading">#</a></h3>
<p>If you need to perform data transformations at a deeper level - before your data reaches the <a class="reference internal" href="package_ref/env/env/ray.rllib.env.single_agent_episode.SingleAgentEpisode.html#ray.rllib.env.single_agent_episode.SingleAgentEpisode" title="ray.rllib.env.single_agent_episode.SingleAgentEpisode"><code class="xref py py-class docutils literal notranslate"><span class="pre">SingleAgentEpisode</span></code></a> stage - consider overriding the <a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.html#ray.rllib.offline.offline_prelearner.OfflinePreLearner" title="ray.rllib.offline.offline_prelearner.OfflinePreLearner"><code class="xref py py-class docutils literal notranslate"><span class="pre">OfflinePreLearner</span></code></a>. This class orchestrates the complete data transformation pipeline, converting raw input data into
<code class="xref py py-class docutils literal notranslate"><span class="pre">MultiAgentBatch</span></code> objects ready for training. For instance, if your data is stored in specialized formats requiring pre-parsing and restructuring (for example, XML, HTML, Protobuf, images, or videos), you may need to handle these custom formats directly. You can leverage tools such as <code class="code docutils literal notranslate"><span class="pre">Ray</span> <span class="pre">Data's</span> <span class="pre">custom</span> <span class="pre">datasources</span> <span class="pre">&lt;custom_datasource&gt;</span></code> (for example, <a class="reference internal" href="../data/api/doc/ray.data.read_binary_files.html#ray.data.read_binary_files" title="ray.data.read_binary_files"><code class="xref py py-meth docutils literal notranslate"><span class="pre">read_binary_files()</span></code></a>) to manage the ingestion process. To ensure
this data is appropriately structured and sorted into <a class="reference internal" href="package_ref/env/env/ray.rllib.env.single_agent_episode.SingleAgentEpisode.html#ray.rllib.env.single_agent_episode.SingleAgentEpisode" title="ray.rllib.env.single_agent_episode.SingleAgentEpisode"><code class="xref py py-class docutils literal notranslate"><span class="pre">SingleAgentEpisode</span></code></a> objects, you can override the <a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner._map_to_episodes.html#ray.rllib.offline.offline_prelearner.OfflinePreLearner._map_to_episodes" title="ray.rllib.offline.offline_prelearner.OfflinePreLearner._map_to_episodes"><code class="xref py py-meth docutils literal notranslate"><span class="pre">_map_to_episodes()</span></code></a> static method.</p>
<p>For more extensive customization, you can rewrite the <code class="code docutils literal notranslate"><span class="pre">__call__</span></code> method to define custom transformation steps, implement a unique <code class="xref py py-class docutils literal notranslate"><span class="pre">LearnerConnectorPipeline</span></code>, and construct <code class="xref py py-class docutils literal notranslate"><span class="pre">MultiAgentBatch</span></code> instances for the <a class="reference internal" href="package_ref/doc/ray.rllib.core.learner.learner.Learner.html#ray.rllib.core.learner.learner.Learner" title="ray.rllib.core.learner.learner.Learner"><code class="xref py py-class docutils literal notranslate"><span class="pre">Learner</span></code></a>.</p>
<p>The following example demonstrates how to use a custom <a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.html#ray.rllib.offline.offline_prelearner.OfflinePreLearner" title="ray.rllib.offline.offline_prelearner.OfflinePreLearner"><code class="xref py py-class docutils literal notranslate"><span class="pre">OfflinePreLearner</span></code></a> to process text data and construct training batches:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gymnasium</span> <span class="k">as</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">uuid</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">data</span>
<span class="kn">from</span> <span class="nn">ray.rllib.env.single_agent_episode</span> <span class="kn">import</span> <span class="n">SingleAgentEpisode</span>
<span class="kn">from</span> <span class="nn">ray.rllib.offline.offline_prelearner</span> <span class="kn">import</span> <span class="n">OfflinePreLearner</span><span class="p">,</span> <span class="n">SCHEMA</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.annotations</span> <span class="kn">import</span> <span class="n">override</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.typing</span> <span class="kn">import</span> <span class="n">EpisodeType</span>

<span class="k">class</span> <span class="nc">TextOfflinePreLearner</span><span class="p">(</span><span class="n">OfflinePreLearner</span><span class="p">):</span>

    <span class="nd">@staticmethod</span>
    <span class="nd">@override</span><span class="p">(</span><span class="n">OfflinePreLearner</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">_map_to_episodes</span><span class="p">(</span>
        <span class="n">is_multi_agent</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">batch</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]],</span>
        <span class="n">schema</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">SCHEMA</span><span class="p">,</span>
        <span class="n">to_numpy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">input_compress_columns</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">observation_space</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">Space</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">action_space</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">Space</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">vocabulary</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">EpisodeType</span><span class="p">]]:</span>

        <span class="c1"># If we have no vocabulary raise an error.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">vocabulary</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;No `vocabulary`. It needs a vocabulary in form of dictionary &quot;</span><span class="p">,</span>
                <span class="s2">&quot;mapping tokens to their IDs.&quot;</span>
            <span class="p">)</span>
        <span class="c1"># Define container for episodes.</span>
        <span class="n">episodes</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Data comes in batches of string arrays under the `&quot;text&quot;` key.</span>
        <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]:</span>
            <span class="c1"># Split the text and tokenize.</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
            <span class="c1"># Encode tokens.</span>
            <span class="n">encoded</span> <span class="o">=</span> <span class="p">[</span><span class="n">vocabulary</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
            <span class="n">one_hot_vectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tokens</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">vocabulary</span><span class="p">:</span>
                    <span class="n">one_hot_vectors</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">vocabulary</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>

            <span class="c1"># Build the `SingleAgentEpisode`.</span>
            <span class="n">episode</span> <span class="o">=</span> <span class="n">SingleAgentEpisode</span><span class="p">(</span>
                <span class="c1"># Generate a unique ID.</span>
                <span class="n">id_</span><span class="o">=</span><span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">()</span><span class="o">.</span><span class="n">hex</span><span class="p">,</span>
                <span class="c1"># agent_id=&quot;default_policy&quot;,</span>
                <span class="c1"># module_id=&quot;default_policy&quot;,</span>
                <span class="c1"># We use the starting token with all added tokens as observations.</span>
                <span class="n">observations</span><span class="o">=</span><span class="p">[</span><span class="n">ohv</span> <span class="k">for</span> <span class="n">ohv</span> <span class="ow">in</span> <span class="n">one_hot_vectors</span><span class="p">],</span>
                <span class="n">observation_space</span><span class="o">=</span><span class="n">observation_space</span><span class="p">,</span>
                <span class="c1"># Actions are defined to be the &quot;chosen&quot; follow-up token after</span>
                <span class="c1"># given the observation.</span>
                <span class="n">actions</span><span class="o">=</span><span class="n">encoded</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span>
                <span class="n">action_space</span><span class="o">=</span><span class="n">action_space</span><span class="p">,</span>
                <span class="c1"># Rewards are zero until the end of a sequence.</span>
                <span class="n">rewards</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">],</span>
                <span class="c1"># The episode is always terminated (as sentences in the dataset are).</span>
                <span class="n">terminated</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">truncated</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="c1"># No lookback. You want the episode to start at timestep zero.</span>
                <span class="n">len_lookback_buffer</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                <span class="n">t_started</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># If episodes should be numpy&#39;ized. Some connectors need this.</span>
            <span class="k">if</span> <span class="n">to_numpy</span><span class="p">:</span>
                <span class="n">episode</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

            <span class="c1"># Append the episode to the list of episodes.</span>
            <span class="n">episodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">episode</span><span class="p">)</span>

        <span class="c1"># Return a batch with key `&quot;episodes&quot;`.</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;episodes&quot;</span><span class="p">:</span> <span class="n">episodes</span><span class="p">}</span>

<span class="c1"># Define the dataset.</span>
<span class="n">ds</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">read_text</span><span class="p">(</span><span class="s2">&quot;s3://anonymous@ray-example-data/this.txt&quot;</span><span class="p">)</span>

<span class="c1"># Create a vocabulary.</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">ds</span><span class="o">.</span><span class="n">iter_rows</span><span class="p">():</span>
    <span class="n">tokens</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">b</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">))</span>
<span class="n">vocabulary</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">tokens</span><span class="p">),</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">)}</span>

<span class="c1"># Take a small batch of 10 from the dataset.</span>
<span class="n">batch</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">take_batch</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Now use your `OfflinePreLearner`.</span>
<span class="n">episodes</span> <span class="o">=</span> <span class="n">TextOfflinePreLearner</span><span class="o">.</span><span class="n">_map_to_episodes</span><span class="p">(</span>
    <span class="n">is_multi_agent</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">batch</span><span class="o">=</span><span class="n">batch</span><span class="p">,</span>
    <span class="n">to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">schema</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_compress_columns</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">action_space</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">observation_space</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">vocabulary</span><span class="o">=</span><span class="n">vocabulary</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Show the constructed episodes.</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Episodes: </span><span class="si">{</span><span class="n">episodes</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The preceding example illustrates the flexibility of RLlib’s Offline RL API for custom data transformation. In this case, a customized <a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.html#ray.rllib.offline.offline_prelearner.OfflinePreLearner" title="ray.rllib.offline.offline_prelearner.OfflinePreLearner"><code class="xref py py-class docutils literal notranslate"><span class="pre">OfflinePreLearner</span></code></a> processes a batch of text data - organized as sentences - and converts each sentence into a <a class="reference internal" href="package_ref/env/env/ray.rllib.env.single_agent_episode.SingleAgentEpisode.html#ray.rllib.env.single_agent_episode.SingleAgentEpisode" title="ray.rllib.env.single_agent_episode.SingleAgentEpisode"><code class="xref py py-class docutils literal notranslate"><span class="pre">SingleAgentEpisode</span></code></a>. The static method returns a dictionary containing a list of these
<a class="reference internal" href="package_ref/env/env/ray.rllib.env.single_agent_episode.SingleAgentEpisode.html#ray.rllib.env.single_agent_episode.SingleAgentEpisode" title="ray.rllib.env.single_agent_episode.SingleAgentEpisode"><code class="xref py py-class docutils literal notranslate"><span class="pre">SingleAgentEpisode</span></code></a> instances. Similarly, you can extend this functionality by overriding the <a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.__call__.html#ray.rllib.offline.offline_prelearner.OfflinePreLearner.__call__" title="ray.rllib.offline.offline_prelearner.OfflinePreLearner.__call__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">__call__()</span></code></a> method. For instance, you could implement a <code class="xref py py-class docutils literal notranslate"><span class="pre">ray.rllib.connectors.learner.learner_connector_pipeline.LearnerConnectorPipeline</span></code> that stacks multiple observations (for example, tokens) together. This can be achieved using RLlib’s
<code class="xref py py-class docutils literal notranslate"><span class="pre">FrameStackingLearner</span></code> and is shown in the example below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gymnasium</span> <span class="k">as</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">uuid</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">data</span>
<span class="kn">from</span> <span class="nn">ray.actor</span> <span class="kn">import</span> <span class="n">ActorHandle</span>
<span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm_config</span> <span class="kn">import</span> <span class="n">AlgorithmConfig</span>
<span class="kn">from</span> <span class="nn">ray.rllib.algorithms.bc.bc_catalog</span> <span class="kn">import</span> <span class="n">BCCatalog</span>
<span class="kn">from</span> <span class="nn">ray.rllib.algorithms.bc.torch.default_bc_torch_rl_module</span> <span class="kn">import</span> <span class="n">DefaultBCTorchRLModule</span>
<span class="kn">from</span> <span class="nn">ray.rllib.connectors.common</span> <span class="kn">import</span> <span class="n">AddObservationsFromEpisodesToBatch</span><span class="p">,</span> <span class="n">BatchIndividualItems</span><span class="p">,</span> <span class="n">NumpyToTensor</span><span class="p">,</span> <span class="n">AgentToModuleMapping</span>
<span class="kn">from</span> <span class="nn">ray.rllib.connectors.learner.add_columns_from_episodes_to_train_batch</span> <span class="kn">import</span> <span class="n">AddColumnsFromEpisodesToTrainBatch</span>
<span class="kn">from</span> <span class="nn">ray.rllib.connectors.learner.frame_stacking</span> <span class="kn">import</span> <span class="n">FrameStackingLearner</span>
<span class="kn">from</span> <span class="nn">ray.rllib.connectors.learner.learner_connector_pipeline</span> <span class="kn">import</span> <span class="n">LearnerConnectorPipeline</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.learner.learner</span> <span class="kn">import</span> <span class="n">Learner</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.rl_module.default_model_config</span> <span class="kn">import</span> <span class="n">DefaultModelConfig</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.rl_module.multi_rl_module</span> <span class="kn">import</span> <span class="n">MultiRLModuleSpec</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.rl_module.rl_module</span> <span class="kn">import</span> <span class="n">RLModuleSpec</span>
<span class="kn">from</span> <span class="nn">ray.rllib.env.single_agent_episode</span> <span class="kn">import</span> <span class="n">SingleAgentEpisode</span>
<span class="kn">from</span> <span class="nn">ray.rllib.policy.sample_batch</span> <span class="kn">import</span> <span class="n">MultiAgentBatch</span><span class="p">,</span> <span class="n">SampleBatch</span>
<span class="kn">from</span> <span class="nn">ray.rllib.offline.offline_prelearner</span> <span class="kn">import</span> <span class="n">OfflinePreLearner</span><span class="p">,</span> <span class="n">SCHEMA</span>

<span class="kn">from</span> <span class="nn">ray.rllib.utils.annotations</span> <span class="kn">import</span> <span class="n">override</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.typing</span> <span class="kn">import</span> <span class="n">EpisodeType</span><span class="p">,</span> <span class="n">ModuleID</span>

<span class="k">class</span> <span class="nc">TextOfflinePreLearner</span><span class="p">(</span><span class="n">OfflinePreLearner</span><span class="p">):</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">OfflinePreLearner</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">,</span>
        <span class="n">learner</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Learner</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">ActorHandle</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">locality_hints</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">spaces</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">gym</span><span class="o">.</span><span class="n">Space</span><span class="p">,</span> <span class="n">gym</span><span class="o">.</span><span class="n">Space</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">module_spec</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MultiRLModuleSpec</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">module_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="n">ModuleID</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">vocabulary</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spaces</span> <span class="o">=</span> <span class="n">spaces</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary</span> <span class="o">=</span> <span class="n">vocabulary</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

        <span class="c1"># Build the `RLModule`.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_module</span> <span class="o">=</span> <span class="n">module_spec</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">module_state</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_module</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">module_state</span><span class="p">)</span>

        <span class="c1"># Build the learner connector pipeline.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_learner_connector</span> <span class="o">=</span> <span class="n">LearnerConnectorPipeline</span><span class="p">(</span>
            <span class="n">connectors</span><span class="o">=</span><span class="p">[</span>
                <span class="n">FrameStackingLearner</span><span class="p">(</span>
                    <span class="n">num_frames</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">],</span>
            <span class="n">input_action_space</span><span class="o">=</span><span class="n">module_spec</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span>
            <span class="n">input_observation_space</span><span class="o">=</span><span class="n">module_spec</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_learner_connector</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">AddObservationsFromEpisodesToBatch</span><span class="p">(</span><span class="n">as_learner_connector</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_learner_connector</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">AddColumnsFromEpisodesToTrainBatch</span><span class="p">(),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_learner_connector</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">BatchIndividualItems</span><span class="p">(</span><span class="n">multi_agent</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="c1"># Let us run exclusively on CPU, then we can convert here to Tensor.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_learner_connector</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">NumpyToTensor</span><span class="p">(</span><span class="n">as_learner_connector</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">OfflinePreLearner</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">EpisodeType</span><span class="p">]]:</span>

        <span class="c1"># Convert raw data to episodes.</span>
        <span class="n">episodes</span> <span class="o">=</span> <span class="n">TextOfflinePreLearner</span><span class="o">.</span><span class="n">_map_to_episodes</span><span class="p">(</span>
            <span class="n">is_multi_agent</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">batch</span><span class="o">=</span><span class="n">batch</span><span class="p">,</span>
            <span class="n">to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">schema</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">input_compress_columns</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">action_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">spaces</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">observation_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">spaces</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">vocabulary</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">,</span>
        <span class="p">)[</span><span class="s2">&quot;episodes&quot;</span><span class="p">]</span>

        <span class="c1"># Run the learner connector pipeline with the</span>
        <span class="c1"># `FrameStackLearner` piece.</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learner_connector</span><span class="p">(</span>
            <span class="n">rl_module</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_module</span><span class="p">,</span>
            <span class="n">batch</span><span class="o">=</span><span class="p">{},</span>
            <span class="n">episodes</span><span class="o">=</span><span class="n">episodes</span><span class="p">,</span>
            <span class="n">shared_data</span><span class="o">=</span><span class="p">{},</span>
        <span class="p">)</span>

        <span class="c1"># Convert to `MultiAgentBatch` for the learner.</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">MultiAgentBatch</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="n">module_id</span><span class="p">:</span> <span class="n">SampleBatch</span><span class="p">(</span><span class="n">module_data</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">module_id</span><span class="p">,</span> <span class="n">module_data</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="p">},</span>
            <span class="c1"># TODO (simon): This can be run once for the batch and the</span>
            <span class="c1"># metrics, but we run it twice: here and later in the learner.</span>
            <span class="n">env_steps</span><span class="o">=</span><span class="nb">sum</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">env_steps</span><span class="p">()</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">episodes</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="c1"># Return the `MultiAgentBatch` under the `&quot;batch&quot;` key.</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;batch&quot;</span><span class="p">:</span> <span class="n">batch</span><span class="p">}</span>

    <span class="nd">@staticmethod</span>
    <span class="nd">@override</span><span class="p">(</span><span class="n">OfflinePreLearner</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">_map_to_episodes</span><span class="p">(</span>
        <span class="n">is_multi_agent</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">batch</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]],</span>
        <span class="n">schema</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">SCHEMA</span><span class="p">,</span>
        <span class="n">to_numpy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">input_compress_columns</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">observation_space</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">Space</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">action_space</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">Space</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">vocabulary</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">EpisodeType</span><span class="p">]]:</span>

        <span class="c1"># If we have no vocabulary raise an error.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">vocabulary</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;No `vocabulary`. It needs a vocabulary in form of dictionary &quot;</span><span class="p">,</span>
                <span class="s2">&quot;mapping tokens to their IDs.&quot;</span>
            <span class="p">)</span>
        <span class="c1"># Define container for episodes.</span>
        <span class="n">episodes</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Data comes in batches of string arrays under the `&quot;text&quot;` key.</span>
        <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]:</span>
            <span class="c1"># Split the text and tokenize.</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
            <span class="c1"># Encode tokens.</span>
            <span class="n">encoded</span> <span class="o">=</span> <span class="p">[</span><span class="n">vocabulary</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
            <span class="n">one_hot_vectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tokens</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">vocabulary</span><span class="p">:</span>
                    <span class="n">one_hot_vectors</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">vocabulary</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>

            <span class="c1"># Build the `SingleAgentEpisode`.</span>
            <span class="n">episode</span> <span class="o">=</span> <span class="n">SingleAgentEpisode</span><span class="p">(</span>
                <span class="c1"># Generate a unique ID.</span>
                <span class="n">id_</span><span class="o">=</span><span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">()</span><span class="o">.</span><span class="n">hex</span><span class="p">,</span>
                <span class="c1"># agent_id=&quot;default_policy&quot;,</span>
                <span class="c1"># module_id=&quot;default_policy&quot;,</span>
                <span class="c1"># We use the starting token with all added tokens as observations.</span>
                <span class="n">observations</span><span class="o">=</span><span class="p">[</span><span class="n">ohv</span> <span class="k">for</span> <span class="n">ohv</span> <span class="ow">in</span> <span class="n">one_hot_vectors</span><span class="p">],</span>
                <span class="n">observation_space</span><span class="o">=</span><span class="n">observation_space</span><span class="p">,</span>
                <span class="c1"># Actions are defined to be the &quot;chosen&quot; follow-up token after</span>
                <span class="c1"># given the observation.</span>
                <span class="n">actions</span><span class="o">=</span><span class="n">encoded</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span>
                <span class="n">action_space</span><span class="o">=</span><span class="n">action_space</span><span class="p">,</span>
                <span class="c1"># Rewards are zero until the end of a sequence.</span>
                <span class="n">rewards</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">],</span>
                <span class="c1"># The episode is always terminated (as sentences in the dataset are).</span>
                <span class="n">terminated</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">truncated</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="c1"># No lookback. You want the episode to start at timestep zero.</span>
                <span class="n">len_lookback_buffer</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                <span class="n">t_started</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># If episodes should be numpy&#39;ized. Some connectors need this.</span>
            <span class="k">if</span> <span class="n">to_numpy</span><span class="p">:</span>
                <span class="n">episode</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

            <span class="c1"># Append the episode to the list of episodes.</span>
            <span class="n">episodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">episode</span><span class="p">)</span>

        <span class="c1"># Return a batch with key `&quot;episodes&quot;`.</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;episodes&quot;</span><span class="p">:</span> <span class="n">episodes</span><span class="p">}</span>

<span class="c1"># Define dataset on sample data.</span>
<span class="n">ds</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">read_text</span><span class="p">(</span><span class="s2">&quot;s3://anonymous@ray-example-data/this.txt&quot;</span><span class="p">)</span>

<span class="c1"># Create a vocabulary.</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">ds</span><span class="o">.</span><span class="n">iter_rows</span><span class="p">():</span>
    <span class="n">tokens</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">b</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">))</span>
<span class="n">vocabulary</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">tokens</span><span class="p">),</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">)}</span>

<span class="c1"># Specify an `RLModule` and wrap it with a `MultiRLModuleSpec`. Note,</span>
<span class="c1"># on `Learner`` side any `RLModule` is an `MultiRLModule`.</span>
<span class="n">module_spec</span> <span class="o">=</span> <span class="n">MultiRLModuleSpec</span><span class="p">(</span>
    <span class="n">rl_module_specs</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;default_policy&quot;</span><span class="p">:</span> <span class="n">RLModuleSpec</span><span class="p">(</span>
            <span class="n">model_config</span><span class="o">=</span><span class="n">DefaultModelConfig</span><span class="p">(</span>
                <span class="n">conv_filters</span><span class="o">=</span><span class="p">[[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span>
                <span class="n">conv_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">inference_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">module_class</span><span class="o">=</span><span class="n">DefaultBCTorchRLModule</span><span class="p">,</span>
            <span class="n">catalog_class</span><span class="o">=</span><span class="n">BCCatalog</span><span class="p">,</span>
            <span class="n">action_space</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Discrete</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)),</span>
            <span class="n">observation_space</span><span class="o">=</span><span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Box</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
        <span class="p">),</span>
    <span class="p">},</span>
<span class="p">)</span>

<span class="c1"># Take a small batch.</span>
<span class="n">batch</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">take_batch</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Build and instance your `OfflinePreLearner`.</span>
<span class="n">oplr</span> <span class="o">=</span> <span class="n">TextOfflinePreLearner</span><span class="p">(</span>
    <span class="n">config</span><span class="o">=</span><span class="n">AlgorithmConfig</span><span class="p">(),</span>
    <span class="n">spaces</span><span class="o">=</span><span class="p">(</span>
        <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Discrete</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)),</span>
        <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Box</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
    <span class="n">module_spec</span><span class="o">=</span><span class="n">module_spec</span><span class="p">,</span>
    <span class="n">vocabulary</span><span class="o">=</span><span class="n">vocabulary</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Run your `OfflinePreLearner`.</span>
<span class="n">transformed</span> <span class="o">=</span> <span class="n">oplr</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

<span class="c1"># Show the generated batch.</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Batch: </span><span class="si">{</span><span class="n">batch</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The ability to fully customize the <a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.html#ray.rllib.offline.offline_prelearner.OfflinePreLearner" title="ray.rllib.offline.offline_prelearner.OfflinePreLearner"><code class="xref py py-class docutils literal notranslate"><span class="pre">OfflinePreLearner</span></code></a> empowers you to design tailored data transformation workflows. This includes defining a specific learner connector pipeline and implementing raw data mapping, enabling multi-step processing of text data from its raw format to a <code class="xref py py-class docutils literal notranslate"><span class="pre">MultiAgentBatch</span></code>.</p>
<p>To integrate your custom <a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.html#ray.rllib.offline.offline_prelearner.OfflinePreLearner" title="ray.rllib.offline.offline_prelearner.OfflinePreLearner"><code class="xref py py-class docutils literal notranslate"><span class="pre">OfflinePreLearner</span></code></a>, simply specify it within your <a class="reference internal" href="package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.html#ray.rllib.algorithms.algorithm_config.AlgorithmConfig" title="ray.rllib.algorithms.algorithm_config.AlgorithmConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlgorithmConfig</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm_config</span> <span class="kn">import</span> <span class="n">AlgorithmConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">AlgorithmConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">offline_data</span><span class="p">(</span>
        <span class="c1"># Provide your custom `OfflinePreLearner`.</span>
        <span class="n">prelearner_class</span><span class="o">=</span><span class="n">TextOfflinePreLearner</span><span class="p">,</span>
        <span class="c1"># Provide special keyword arguments your `OfflinePreLearner` needs.</span>
        <span class="n">prelearner_kwargs</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;vocabulary&quot;</span><span class="p">:</span> <span class="n">vocabulary</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>If these customization capabilities still don’t meet your requirements, consider moving to the <strong>Pipeline Level</strong> for even greater flexibility.</p>
<section id="pipeline-level">
<h4>Pipeline level<a class="headerlink" href="#pipeline-level" title="Link to this heading">#</a></h4>
<p>On this level of RLlib’s Offline RL API you can redefine your complete pipeline from data reading to batch iteration by overriding the <code class="xref py py-class docutils literal notranslate"><span class="pre">OfflineData</span></code> class. In most cases however the other two levels should be sufficient for your requirements. Manipulating the complete pipeline needs sensible handling because it could degrade performance of your
pipeline to a high degree. Study carefully the <a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_data.OfflineData.html#ray.rllib.offline.offline_data.OfflineData" title="ray.rllib.offline.offline_data.OfflineData"><code class="xref py py-class docutils literal notranslate"><span class="pre">OfflineData</span></code></a> class to reach a good understanding of how the default pipeline works before going over to program your own one. There are mainly two methods that define this pipeline:</p>
<ul class="simple">
<li><p>The <a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_data.OfflineData.__init__.html#ray.rllib.offline.offline_data.OfflineData.__init__" title="ray.rllib.offline.offline_data.OfflineData.__init__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">__init__()</span></code></a> method that defines the data reading process.</p></li>
<li><p>The <a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_data.OfflineData.sample.html#ray.rllib.offline.offline_data.OfflineData.sample" title="ray.rllib.offline.offline_data.OfflineData.sample"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sample()</span></code></a> method that defines the data mapping and batch iteration.</p></li>
</ul>
<p>For example consider overriding the <a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_data.OfflineData.__init__.html#ray.rllib.offline.offline_data.OfflineData.__init__" title="ray.rllib.offline.offline_data.OfflineData.__init__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">__init__()</span></code></a> method, if you have some foundational data transformations as for example transforming image files into numpy arrays.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">io</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span>

<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">data</span>
<span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm_config</span> <span class="kn">import</span> <span class="n">AlgorithmConfig</span>
<span class="kn">from</span> <span class="nn">ray.rllib.offline.offline_data</span> <span class="kn">import</span> <span class="n">OfflineData</span>
<span class="kn">from</span> <span class="nn">ray.rllib.offline.offline_prelearner</span> <span class="kn">import</span> <span class="n">OfflinePreLearner</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.annotations</span> <span class="kn">import</span> <span class="n">override</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ImageOfflineData</span><span class="p">(</span><span class="n">OfflineData</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This class overrides `OfflineData` to read in raw image data.</span>

<span class="sd">    The image data is from Ray Data`s S3 example bucket, namely</span>
<span class="sd">    `ray-example-data/batoidea/JPEGImages/`.</span>
<span class="sd">    To read in this data the raw bytes have to be decoded and then</span>
<span class="sd">    converted to `numpy` arrays. Each image array has a dimension</span>
<span class="sd">    (32, 32, 3).</span>

<span class="sd">    To just read in the raw image data and convert it to arrays it</span>
<span class="sd">    suffices to override the `OfflineData.__init__` method only.</span>
<span class="sd">    Note, that further transformations of the data - specifically</span>
<span class="sd">    into `SingleAgentEpisode` data - will be performed in a custom</span>
<span class="sd">    `OfflinePreLearner` defined in the `image_offline_prelearner`</span>
<span class="sd">    file. You could hard-code the usage of this prelearner here,</span>
<span class="sd">    but you will use the `prelearner_class` attribute in the</span>
<span class="sd">    `AlgorithmConfig` instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">OfflineData</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">AlgorithmConfig</span><span class="p">):</span>

        <span class="c1"># Set class attributes.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_multi_agent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_multi_agent</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">materialize_mapped_data</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">input_</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">data_read_batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">input_read_batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_is_mapped</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Define your function to map images to numpy arrays.</span>
        <span class="k">def</span> <span class="nf">map_to_numpy</span><span class="p">(</span><span class="n">row</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
            <span class="c1"># Convert to byte stream.</span>
            <span class="n">bytes_stream</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;bytes&quot;</span><span class="p">])</span>
            <span class="c1"># Convert to image.</span>
            <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">bytes_stream</span><span class="p">)</span>
            <span class="c1"># Return an array of the image.</span>
            <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;array&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">image</span><span class="p">)}</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Load the dataset and transform to arrays on-the-fly.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">read_binary_files</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">path</span><span class="p">)</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">map_to_numpy</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>

        <span class="c1"># Define further attributes needed in the `sample` method.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_iterator</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">map_batches_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">map_batches_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">iter_batches_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">iter_batches_kwargs</span>
        <span class="c1"># Use a custom OfflinePreLearner if needed.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prelearner_class</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">prelearner_class</span> <span class="ow">or</span> <span class="n">OfflinePreLearner</span>

        <span class="c1"># For remote learner setups.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">locality_hints</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learner_handles</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module_spec</span> <span class="o">=</span> <span class="kc">None</span>
</pre></div>
</div>
<p>In the code example provided, you define a custom <a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_data.OfflineData.html#ray.rllib.offline.offline_data.OfflineData" title="ray.rllib.offline.offline_data.OfflineData"><code class="xref py py-class docutils literal notranslate"><span class="pre">OfflineData</span></code></a> class to handle the reading and preprocessing of image data, converting it from a binary encoding format into <code class="code docutils literal notranslate"><span class="pre">numpy</span></code> arrays. Additionally, you implement a custom <a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_prelearner.OfflinePreLearner.html#ray.rllib.offline.offline_prelearner.OfflinePreLearner" title="ray.rllib.offline.offline_prelearner.OfflinePreLearner"><code class="xref py py-class docutils literal notranslate"><span class="pre">OfflinePreLearner</span></code></a> to process this data further, transforming it into a learner-ready <code class="xref py py-class docutils literal notranslate"><span class="pre">MultiAgentBatch</span></code> format.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gymnasium</span> <span class="k">as</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">uuid</span>

<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">from</span> <span class="nn">ray.actor</span> <span class="kn">import</span> <span class="n">ActorHandle</span>
<span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm_config</span> <span class="kn">import</span> <span class="n">AlgorithmConfig</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.learner.learner</span> <span class="kn">import</span> <span class="n">Learner</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.rl_module.multi_rl_module</span> <span class="kn">import</span> <span class="n">MultiRLModuleSpec</span>
<span class="kn">from</span> <span class="nn">ray.rllib.env.single_agent_episode</span> <span class="kn">import</span> <span class="n">SingleAgentEpisode</span>
<span class="kn">from</span> <span class="nn">ray.rllib.offline.offline_prelearner</span> <span class="kn">import</span> <span class="n">OfflinePreLearner</span><span class="p">,</span> <span class="n">SCHEMA</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.annotations</span> <span class="kn">import</span> <span class="n">override</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.typing</span> <span class="kn">import</span> <span class="n">EpisodeType</span><span class="p">,</span> <span class="n">ModuleID</span>


<span class="k">class</span> <span class="nc">ImageOfflinePreLearner</span><span class="p">(</span><span class="n">OfflinePreLearner</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This class transforms image data to `MultiAgentBatch`es.</span>

<span class="sd">    While the `ImageOfflineData` class transforms raw image</span>
<span class="sd">    bytes to `numpy` arrays, this class maps these data in</span>
<span class="sd">    `SingleAgentEpisode` instances through the learner connector</span>
<span class="sd">    pipeline and finally outputs a &gt;`MultiAgentBatch` ready for</span>
<span class="sd">    training in RLlib&#39;s `Learner`s.</span>

<span class="sd">    Note, the basic transformation from images to `SingleAgentEpisode`</span>
<span class="sd">    instances creates synthetic data that does not rely on any MDP</span>
<span class="sd">    and therefore no agent can learn from it. However, this example</span>
<span class="sd">    should show how to transform data into this form through</span>
<span class="sd">    overriding the `OfflinePreLearner`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">,</span>
        <span class="n">learner</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Learner</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">ActorHandle</span><span class="p">]],</span>
        <span class="n">spaces</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">gym</span><span class="o">.</span><span class="n">Space</span><span class="p">,</span> <span class="n">gym</span><span class="o">.</span><span class="n">Space</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">module_spec</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MultiRLModuleSpec</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">module_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="n">ModuleID</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
    <span class="p">):</span>
        <span class="c1"># Set up necessary class attributes.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="n">spaces</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span> <span class="o">=</span> <span class="n">spaces</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_read_episodes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">input_read_episodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_read_sample_batches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">input_read_sample_batches</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_policies_to_train</span> <span class="o">=</span> <span class="s2">&quot;default_policy&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_multi_agent</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Build the `MultiRLModule` needed for the learner connector.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_module</span> <span class="o">=</span> <span class="n">module_spec</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>

        <span class="c1"># Build the learner connector pipeline.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_learner_connector</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">build_learner_connector</span><span class="p">(</span>
            <span class="n">input_observation_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span>
            <span class="n">input_action_space</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">OfflinePreLearner</span><span class="p">)</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_map_to_episodes</span><span class="p">(</span>
        <span class="n">is_multi_agent</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">batch</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]],</span>
        <span class="n">schema</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">SCHEMA</span><span class="p">,</span>
        <span class="n">to_numpy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">input_compress_columns</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">observation_space</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">Space</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">action_space</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">Space</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">EpisodeType</span><span class="p">]]:</span>

        <span class="c1"># Define a container for the episodes.</span>
        <span class="n">episodes</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Batches come in as numpy arrays.</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">obs</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;array&quot;</span><span class="p">]):</span>

            <span class="c1"># Construct your episode.</span>
            <span class="n">episode</span> <span class="o">=</span> <span class="n">SingleAgentEpisode</span><span class="p">(</span>
                <span class="n">id_</span><span class="o">=</span><span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">()</span><span class="o">.</span><span class="n">hex</span><span class="p">,</span>
                <span class="n">observations</span><span class="o">=</span><span class="p">[</span><span class="n">obs</span><span class="p">,</span> <span class="n">obs</span><span class="p">],</span>
                <span class="n">observation_space</span><span class="o">=</span><span class="n">observation_space</span><span class="p">,</span>
                <span class="n">actions</span><span class="o">=</span><span class="p">[</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()],</span>
                <span class="n">action_space</span><span class="o">=</span><span class="n">action_space</span><span class="p">,</span>
                <span class="n">rewards</span><span class="o">=</span><span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()],</span>
                <span class="n">terminated</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">truncated</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">len_lookback_buffer</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                <span class="n">t_started</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># Numpy&#39;ize, if necessary.</span>
            <span class="k">if</span> <span class="n">to_numpy</span><span class="p">:</span>
                <span class="n">episode</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

            <span class="c1"># Store the episode in the container.</span>
            <span class="n">episodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">episode</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;episodes&quot;</span><span class="p">:</span> <span class="n">episodes</span><span class="p">}</span>
</pre></div>
</div>
<p>This demonstrates how the entire Offline Data Pipeline can be customized with your own logic. You can run the example by using the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;Example showing how to customize an offline data pipeline.</span>

<span class="sd">This example:</span>
<span class="sd">    - demonstrates how you can customized your offline data pipeline.</span>
<span class="sd">    - shows how you can override the `OfflineData` to read raw image</span>
<span class="sd">    data and transform it into `numpy ` arrays.</span>
<span class="sd">    - explains how you can override the `OfflinePreLearner` to</span>
<span class="sd">    transform data further into `SingleAgentEpisode` instances that</span>
<span class="sd">    can be processes by the learner connector pipeline.</span>

<span class="sd">How to run this script</span>
<span class="sd">----------------------</span>
<span class="sd">`python [script file name].py --checkpoint-at-end`</span>

<span class="sd">For debugging, use the following additional command line options</span>
<span class="sd">`--no-tune --num-env-runners=0`</span>
<span class="sd">which should allow you to set breakpoints anywhere in the RLlib code and</span>
<span class="sd">have the execution stop there for inspection and debugging.</span>

<span class="sd">For logging to your WandB account, use:</span>
<span class="sd">`--wandb-key=[your WandB API key] --wandb-project=[some project name]</span>
<span class="sd">--wandb-run-name=[optional: WandB run name (within the defined project)]`</span>

<span class="sd">Results to expect</span>
<span class="sd">-----------------</span>
<span class="sd">2024-12-03 19:59:23,043 INFO streaming_executor.py:109 -- Execution plan</span>
<span class="sd">of Dataset: InputDataBuffer[Input] -&gt; TaskPoolMapOperator[ReadBinary] -&gt;</span>
<span class="sd">TaskPoolMapOperator[Map(map_to_numpy)] -&gt; LimitOperator[limit=128]</span>
<span class="sd">✔️  Dataset execution finished in 10.01 seconds: 100%|███████████████████</span>
<span class="sd">███████████████████████████████████████████████████████████████████████|</span>
<span class="sd">3.00/3.00 [00:10&lt;00:00, 3.34s/ row]</span>
<span class="sd">- ReadBinary-&gt;SplitBlocks(11): Tasks: 0; Queued blocks: 0; Resources: 0.0</span>
<span class="sd">CPU, 0.0B object store: 100%|█████████████████████████████████████████|</span>
<span class="sd">3.00/3.00 [00:10&lt;00:00, 3.34s/ row]</span>
<span class="sd">- Map(map_to_numpy): Tasks: 0; Queued blocks: 0; Resources: 0.0 CPU,</span>
<span class="sd">0.0B object store: 100%|███████████████████████████████████████████████████|</span>
<span class="sd">3.00/3.00 [00:10&lt;00:00, 3.34s/ row]</span>
<span class="sd">- limit=128: Tasks: 0; Queued blocks: 0; Resources: 0.0 CPU, 3.0KB object</span>
<span class="sd">store: 100%|██████████████████████████████████████████████████████████|</span>
<span class="sd">3.00/3.00 [00:10&lt;00:00, 3.34s/ row]</span>
<span class="sd">Batch: {&#39;batch&#39;: [MultiAgentBatch({}, env_steps=3)]}</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">gymnasium</span> <span class="k">as</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">ray.rllib.algorithms.bc</span> <span class="kn">import</span> <span class="n">BCConfig</span>
<span class="kn">from</span> <span class="nn">ray.rllib.algorithms.bc.bc_catalog</span> <span class="kn">import</span> <span class="n">BCCatalog</span>
<span class="kn">from</span> <span class="nn">ray.rllib.algorithms.bc.torch.bc_torch_rl_module</span> <span class="kn">import</span> <span class="n">BCTorchRLModule</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.rl_module.rl_module</span> <span class="kn">import</span> <span class="n">RLModuleSpec</span><span class="p">,</span> <span class="n">DefaultModelConfig</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.rl_module.multi_rl_module</span> <span class="kn">import</span> <span class="n">MultiRLModuleSpec</span>
<span class="kn">from</span> <span class="nn">ray.rllib.examples.offline_rl.classes.image_offline_data</span> <span class="kn">import</span> <span class="n">ImageOfflineData</span>
<span class="kn">from</span> <span class="nn">ray.rllib.examples.offline_rl.classes.image_offline_prelearner</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">ImageOfflinePreLearner</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Create an Algorithm configuration.</span>
<span class="c1"># TODO: Make this an actually running/learning example with RLunplugged</span>
<span class="c1"># data from S3 and add this to the CI.</span>
<span class="n">config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">BCConfig</span><span class="p">()</span>
    <span class="o">.</span><span class="n">environment</span><span class="p">(</span>
        <span class="n">action_space</span><span class="o">=</span><span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Discrete</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
        <span class="n">observation_space</span><span class="o">=</span><span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Box</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="o">.</span><span class="n">offline_data</span><span class="p">(</span>
        <span class="n">input_</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;s3://anonymous@ray-example-data/batoidea/JPEGImages/&quot;</span><span class="p">],</span>
        <span class="n">prelearner_class</span><span class="o">=</span><span class="n">ImageOfflinePreLearner</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Specify an `RLModule` and wrap it with a `MultiRLModuleSpec`. Note,</span>
<span class="c1"># on `Learner`` side any `RLModule` is an `MultiRLModule`.</span>
<span class="n">module_spec</span> <span class="o">=</span> <span class="n">MultiRLModuleSpec</span><span class="p">(</span>
    <span class="n">rl_module_specs</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;default_policy&quot;</span><span class="p">:</span> <span class="n">RLModuleSpec</span><span class="p">(</span>
            <span class="n">model_config</span><span class="o">=</span><span class="n">DefaultModelConfig</span><span class="p">(</span>
                <span class="n">conv_filters</span><span class="o">=</span><span class="p">[[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span>
                <span class="n">conv_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">inference_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">module_class</span><span class="o">=</span><span class="n">BCTorchRLModule</span><span class="p">,</span>
            <span class="n">catalog_class</span><span class="o">=</span><span class="n">BCCatalog</span><span class="p">,</span>
            <span class="n">action_space</span><span class="o">=</span><span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Discrete</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
            <span class="n">observation_space</span><span class="o">=</span><span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Box</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
        <span class="p">),</span>
    <span class="p">},</span>
<span class="p">)</span>

<span class="c1"># Construct your `OfflineData` class instance.</span>
<span class="n">offline_data</span> <span class="o">=</span> <span class="n">ImageOfflineData</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

<span class="c1"># Check, how the data is transformed. Note, the</span>
<span class="c1"># example dataset has only 3 such images.</span>
<span class="n">batch</span> <span class="o">=</span> <span class="n">offline_data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">take_batch</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Construct your `OfflinePreLearner`.</span>
<span class="n">offline_prelearner</span> <span class="o">=</span> <span class="n">ImageOfflinePreLearner</span><span class="p">(</span>
    <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
    <span class="n">learner</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">spaces</span><span class="o">=</span><span class="p">(</span>
        <span class="n">config</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span>
        <span class="n">config</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">module_spec</span><span class="o">=</span><span class="n">module_spec</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Transform the raw data to `MultiAgentBatch` data.</span>
<span class="n">batch</span> <span class="o">=</span> <span class="n">offline_prelearner</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

<span class="c1"># Show the transformed batch.</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Batch: </span><span class="si">{</span><span class="n">batch</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Consider this approach carefully: in many cases, fully transforming your data into a suitable format before engaging RLlib’s offline RL API can be more efficient. For instance, in the example above, you could preprocess the entire image dataset into <code class="code docutils literal notranslate"><span class="pre">numpy</span></code> arrays beforehand and utilize RLlib’s default <a class="reference internal" href="package_ref/doc/ray.rllib.offline.offline_data.OfflineData.html#ray.rllib.offline.offline_data.OfflineData" title="ray.rllib.offline.offline_data.OfflineData"><code class="xref py py-class docutils literal notranslate"><span class="pre">OfflineData</span></code></a> class for subsequent steps.</p>
</div>
</section>
</section>
</section>
<section id="monitoring">
<h2>Monitoring<a class="headerlink" href="#monitoring" title="Link to this heading">#</a></h2>
<p>To effectively monitor your offline data pipeline, leverage <a class="reference internal" href="../data/monitoring-your-workload.html#monitoring-your-workload"><span class="std std-ref">Ray Data’s built-in monitoring capacities</span></a>. Focus on ensuring that all stages of your offline data streaming pipeline are actively processing data. Additionally, keep an eye on the Learner instance, particularly the <code class="code docutils literal notranslate"><span class="pre">learner_update_timer</span></code>, which should maintain low values - around <code class="code docutils literal notranslate"><span class="pre">0.02</span></code> for small models - to indicate efficient data processing and model updates.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>RLlib doesn’t include <a class="reference internal" href="../data/data.html#data"><span class="std std-ref">Ray Data</span></a>  metrics in its results or display them in <code class="code docutils literal notranslate"><span class="pre">Tensorboard</span></code> through <a class="reference internal" href="../tune/index.html#tune-main"><span class="std std-ref">Ray Tune</span></a>’s <a class="reference internal" href="../tune/api/doc/ray.tune.logger.TBXLoggerCallback.html#ray.tune.logger.TBXLoggerCallback" title="ray.tune.logger.tensorboardx.TBXLoggerCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">TBXLoggerCallback</span></code></a>. It’s strongly recommended to enable the <a class="reference internal" href="../ray-observability/getting-started.html#observability-getting-started"><span class="std std-ref">Ray Dashboard</span></a>, accessible at <code class="code docutils literal notranslate"><span class="pre">127.0.0.1:8265</span></code>, for comprehensive monitoring and insights.</p>
</div>
</section>
<section id="input-api">
<h2>Input API<a class="headerlink" href="#input-api" title="Link to this heading">#</a></h2>
<p>You can configure experience input for an agent using the following options:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">offline_data</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="c1"># Specify how to generate experiences:</span>
    <span class="c1"># - A local directory or file glob expression (for example &quot;/tmp/*.json&quot;).</span>
    <span class="c1"># - A cloud storage path or file glob expression (for example &quot;gs://rl/&quot;).</span>
    <span class="c1"># - A list of individual file paths/URIs (for example [&quot;/tmp/1.json&quot;,</span>
    <span class="c1">#   &quot;s3://bucket/2.json&quot;]).</span>
    <span class="c1"># - A file or directory path in a given `input_filesystem`.</span>
    <span class="n">input_</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">IOContext</span><span class="p">],</span> <span class="n">InputReader</span><span class="p">]]],</span>
    <span class="c1"># Read method for the `ray.data.Dataset` to read in the</span>
    <span class="c1"># offline data from `input_`. The default is `read_parquet` for Parquet</span>
    <span class="c1"># files. See https://docs.ray.io/en/latest/data/api/input_output.html for</span>
    <span class="c1"># more info about available read methods in `ray.data`.</span>
    <span class="n">input_read_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]],</span>
    <span class="c1"># Keyword args for `input_read_method`. These</span>
    <span class="c1"># are passed into the read method without checking. Use these</span>
    <span class="c1"># keyword args together with `map_batches_kwargs` and</span>
    <span class="c1"># `iter_batches_kwargs` to tune the performance of the data pipeline. It</span>
    <span class="c1"># is strongly recommended to rely on Ray Data&#39;s automatic read performance</span>
    <span class="c1"># tuning</span>
    <span class="n">input_read_method_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">],</span>
    <span class="c1"># Table schema for converting offline data to episodes.</span>
    <span class="c1"># This schema maps the offline data columns to</span>
    <span class="c1"># `ray.rllib.core.columns.Columns`:</span>
    <span class="c1"># `{Columns.OBS: &#39;o_t&#39;, Columns.ACTIONS: &#39;a_t&#39;, ...}`. Columns in</span>
    <span class="c1"># the data set that aren&#39;t mapped through this schema are sorted into</span>
    <span class="c1"># episodes&#39; `extra_model_outputs`. If no schema is passed in the default</span>
    <span class="c1"># schema used is `ray.rllib.offline.offline_data.SCHEMA`. If your data set</span>
    <span class="c1"># contains already the names in this schema, no `input_read_schema` is</span>
    <span class="c1"># needed. The same applies, if the offline data is in RLlib&#39;s</span>
    <span class="c1"># `EpisodeType` or old `SampleBatch` format</span>
    <span class="n">input_read_schema</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]],</span>
    <span class="c1"># Whether offline data is already stored in RLlib&#39;s</span>
    <span class="c1"># `EpisodeType` format, i.e. `ray.rllib.env.SingleAgentEpisode` (multi</span>
    <span class="c1"># -agent is planned but not supported, yet). Reading episodes directly</span>
    <span class="c1"># avoids additional transform steps and is usually faster and</span>
    <span class="c1"># therefore the recommended format when your application remains fully</span>
    <span class="c1"># inside of RLlib&#39;s schema. The other format is a columnar format and is</span>
    <span class="c1"># agnostic to the RL framework used. Use the latter format, if you are</span>
    <span class="c1"># unsure when to use the data or in which RL framework. The default is</span>
    <span class="c1"># to read column data, i.e. `False`. `input_read_episodes` and</span>
    <span class="c1"># `input_read_sample_batches` can&#39;t be `True` at the same time. See</span>
    <span class="c1"># also `output_write_episodes` to define the output data format when</span>
    <span class="c1"># recording.</span>
    <span class="n">input_read_episodes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
    <span class="c1"># Whether offline data is stored in RLlib&#39;s old</span>
    <span class="c1"># stack `SampleBatch` type. This is usually the case for older data</span>
    <span class="c1"># recorded with RLlib in JSON line format. Reading in `SampleBatch`</span>
    <span class="c1"># data needs extra transforms and might not concatenate episode chunks</span>
    <span class="c1"># contained in different `SampleBatch`es in the data. If possible avoid</span>
    <span class="c1"># to read `SampleBatch`es and convert them in a controlled form into</span>
    <span class="c1"># RLlib&#39;s `EpisodeType` (i.e. `SingleAgentEpisode`). The default is</span>
    <span class="c1"># `False`. `input_read_episodes` and `input_read_sample_batches` can&#39;t</span>
    <span class="c1"># be True at the same time.</span>
    <span class="n">input_read_sample_batches</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
    <span class="c1"># Batch size to pull from the data set. This could</span>
    <span class="c1"># differ from the `train_batch_size_per_learner`, if a dataset holds</span>
    <span class="c1"># `EpisodeType` (i.e. `SingleAgentEpisode`) or `SampleBatch`, or any</span>
    <span class="c1"># other data type that contains multiple timesteps in a single row of the</span>
    <span class="c1"># dataset. In such cases a single batch of size</span>
    <span class="c1"># `train_batch_size_per_learner` potentially pulls a multiple of</span>
    <span class="c1"># `train_batch_size_per_learner` timesteps from the offline dataset. The</span>
    <span class="c1"># default is `None` in which the `train_batch_size_per_learner` is pulled.</span>
    <span class="n">input_read_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="c1"># A cloud filesystem to handle access to cloud storage when</span>
    <span class="c1"># reading experiences. Can be &quot;gcs&quot; for Google Cloud Storage, &quot;s3&quot; for AWS</span>
    <span class="c1"># S3 buckets, &quot;abs&quot; for Azure Blob Storage, or any filesystem supported</span>
    <span class="c1"># by PyArrow. In general the file path is sufficient for accessing data</span>
    <span class="c1"># from public or local storage systems. See</span>
    <span class="c1"># https://arrow.apache.org/docs/python/filesystems.html for details.</span>
    <span class="n">input_filesystem</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="c1"># A dictionary holding the kwargs for the filesystem</span>
    <span class="c1"># given by `input_filesystem`. See `gcsfs.GCSFilesystem` for GCS,</span>
    <span class="c1"># `pyarrow.fs.S3FileSystem`, for S3, and `ablfs.AzureBlobFilesystem` for</span>
    <span class="c1"># ABS filesystem arguments.</span>
    <span class="n">input_filesystem_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">],</span>
    <span class="c1"># What input columns are compressed with LZ4 in the</span>
    <span class="c1"># input data. If data is stored in RLlib&#39;s `SingleAgentEpisode` (</span>
    <span class="c1"># `MultiAgentEpisode` not supported, yet). Note the providing</span>
    <span class="c1"># `rllib.core.columns.Columns.OBS` also tries to decompress</span>
    <span class="c1"># `rllib.core.columns.Columns.NEXT_OBS`.</span>
    <span class="n">input_compress_columns</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
    <span class="c1"># Whether the raw data should be materialized in memory.</span>
    <span class="c1"># This boosts performance, but requires enough memory to avoid an OOM, so</span>
    <span class="c1"># make sure that your cluster has the resources available. For very large</span>
    <span class="c1"># data you might want to switch to streaming mode by setting this to</span>
    <span class="c1"># `False` (default). If your algorithm doesn&#39;t need the RLModule in the</span>
    <span class="c1"># Learner connector pipeline or all (learner) connectors are stateless</span>
    <span class="c1"># you should consider setting `materialize_mapped_data` to `True`</span>
    <span class="c1"># instead (and set `materialize_data` to `False`). If your data doesn&#39;t</span>
    <span class="c1"># fit into memory and your Learner connector pipeline requires an RLModule</span>
    <span class="c1"># or is stateful, set both `materialize_data` and</span>
    <span class="c1"># `materialize_mapped_data` to `False`.</span>
    <span class="n">materialize_data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
    <span class="c1"># Whether the data should be materialized after</span>
    <span class="c1"># running it through the Learner connector pipeline (i.e. after running</span>
    <span class="c1"># the `OfflinePreLearner`). This improves performance, but should only be</span>
    <span class="c1"># used in case the (learner) connector pipeline doesn&#39;t require an</span>
    <span class="c1"># RLModule and the (learner) connector pipeline is stateless. For example,</span>
    <span class="c1"># MARWIL&#39;s Learner connector pipeline requires the RLModule for value</span>
    <span class="c1"># function predictions and training batches would become stale after some</span>
    <span class="c1"># iterations causing learning degradation or divergence. Also ensure that</span>
    <span class="c1"># your cluster has enough memory available to avoid an OOM. If set to</span>
    <span class="c1"># `True`, make sure that `materialize_data` is set to `False` to</span>
    <span class="c1"># avoid materialization of two datasets. If your data doesn&#39;t fit into</span>
    <span class="c1"># memory and your Learner connector pipeline requires an RLModule or is</span>
    <span class="c1"># stateful, set both `materialize_data` and `materialize_mapped_data` to</span>
    <span class="c1"># `False`.</span>
    <span class="n">materialize_mapped_data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
    <span class="c1"># Keyword args for the `map_batches` method. These are</span>
    <span class="c1"># passed into the `ray.data.Dataset.map_batches` method when sampling</span>
    <span class="c1"># without checking. If no arguments passed in the default arguments</span>
    <span class="c1"># `{&#39;concurrency&#39;: max(2, num_learners), &#39;zero_copy_batch&#39;: True}` is</span>
    <span class="c1"># used. Use these keyword args together with `input_read_method_kwargs`</span>
    <span class="c1"># and `iter_batches_kwargs` to tune the performance of the data pipeline.</span>
    <span class="n">map_batches_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">],</span>
    <span class="c1"># Keyword args for the `iter_batches` method. These are</span>
    <span class="c1"># passed into the `ray.data.Dataset.iter_batches` method when sampling</span>
    <span class="c1"># without checking. If no arguments are passed in, the default argument</span>
    <span class="c1"># `{&#39;prefetch_batches&#39;: 2}` is used. Use these keyword args</span>
    <span class="c1"># together with `input_read_method_kwargs` and `map_batches_kwargs` to</span>
    <span class="c1"># tune the performance of the data pipeline.</span>
    <span class="n">iter_batches_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">],</span>
    <span class="c1"># An optional `OfflinePreLearner` class that&#39;s used to</span>
    <span class="c1"># transform data batches in `ray.data.map_batches` used in the</span>
    <span class="c1"># `OfflineData` class to transform data from columns to batches that can</span>
    <span class="c1"># be used in the `Learner.update...()` methods. Override the</span>
    <span class="c1"># `OfflinePreLearner` class and pass your derived class in here, if you</span>
    <span class="c1"># need to make some further transformations specific for your data or</span>
    <span class="c1"># loss. The default is `None`` which uses the base `OfflinePreLearner`</span>
    <span class="c1"># defined in `ray.rllib.offline.offline_prelearner`.</span>
    <span class="n">prelearner_class</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Type</span><span class="p">],</span>
    <span class="c1"># An optional `EpisodeReplayBuffer` class is</span>
    <span class="c1"># used to buffer experiences when data is in `EpisodeType` or</span>
    <span class="c1"># RLlib&#39;s previous `SampleBatch` type format. In this case, a single</span>
    <span class="c1"># data row may contain multiple timesteps and the buffer serves two</span>
    <span class="c1"># purposes: (a) to store intermediate data in memory, and (b) to ensure</span>
    <span class="c1"># that exactly `train_batch_size_per_learner` experiences are sampled</span>
    <span class="c1"># per batch. The default is RLlib&#39;s `EpisodeReplayBuffer`.</span>
    <span class="n">prelearner_buffer_class</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Type</span><span class="p">],</span>
    <span class="c1"># Optional keyword arguments for intializing the</span>
    <span class="c1"># `EpisodeReplayBuffer`. In most cases this is simply the `capacity`</span>
    <span class="c1"># for the default buffer used (`EpisodeReplayBuffer`), but it may</span>
    <span class="c1"># differ if the `prelearner_buffer_class` uses a custom buffer.</span>
    <span class="n">prelearner_buffer_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">],</span>
    <span class="c1"># Number of updates to run in each learner</span>
    <span class="c1"># during a single training iteration. If None, each learner runs a</span>
    <span class="c1"># complete epoch over its data block (the dataset is partitioned into</span>
    <span class="c1"># at least as many blocks as there are learners). The default is `None`.</span>
    <span class="c1"># This must be set to `1`, if a single (local) learner is used.</span>
    <span class="n">dataset_num_iters_per_learner</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="output-api">
<h2>Output API<a class="headerlink" href="#output-api" title="Link to this heading">#</a></h2>
<p>You can configure experience output for an agent using the following options:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">offline_data</span><span class="p">(</span>
    <span class="c1"># Specify where experiences should be saved:</span>
    <span class="c1"># - None: don&#39;t save any experiences</span>
    <span class="c1"># - a path/URI to save to a custom output directory (for example, &quot;s3://bckt/&quot;)</span>
    <span class="n">output</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="c1"># What sample batch columns to LZ4 compress in the output data.</span>
    <span class="c1"># Note that providing `rllib.core.columns.Columns.OBS` also</span>
    <span class="c1"># compresses `rllib.core.columns.Columns.NEXT_OBS`.</span>
    <span class="n">output_compress_columns</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
    <span class="c1"># Max output file size (in bytes) before rolling over to a new</span>
    <span class="c1"># file.</span>
    <span class="n">output_max_file_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
    <span class="c1"># Max output row numbers before rolling over to a new file.</span>
    <span class="n">output_max_rows_per_file</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="c1"># Write method for the `ray.data.Dataset` to write the</span>
    <span class="c1"># offline data to `output`. The default is `read_parquet` for Parquet</span>
    <span class="c1"># files. See https://docs.ray.io/en/latest/data/api/input_output.html for</span>
    <span class="c1"># more info about available read methods in `ray.data`.</span>
    <span class="n">output_write_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="c1"># Keyword arguments for the `output_write_method`. These are</span>
    <span class="c1"># passed into the write method without checking.</span>
    <span class="n">output_write_method_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">],</span>
    <span class="c1"># A cloud filesystem to handle access to cloud storage when</span>
    <span class="c1"># writing experiences. Can be &quot;gcs&quot; for Google Cloud Storage, &quot;s3&quot; for AWS</span>
    <span class="c1"># S3 buckets, &quot;abs&quot; for Azure Blob Storage, or any filesystem supported</span>
    <span class="c1"># by PyArrow. In general the file path is sufficient for accessing data</span>
    <span class="c1"># from public or local storage systems. See</span>
    <span class="c1"># https://arrow.apache.org/docs/python/filesystems.html for details.</span>
    <span class="n">output_filesystem</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="c1"># A dictionary holding the keyword arguments for the filesystem</span>
    <span class="c1"># given by `output_filesystem`. See `gcsfs.GCSFilesystem` for GCS,</span>
    <span class="c1"># `pyarrow.fs.S3FileSystem`, for S3, and `ablfs.AzureBlobFilesystem` for</span>
    <span class="c1"># ABS filesystem arguments.</span>
    <span class="n">output_filesystem_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">],</span>
    <span class="c1"># If data should be recorded in RLlib&#39;s `EpisodeType`</span>
    <span class="c1"># format (i.e. `SingleAgentEpisode` objects). Use this format, if you</span>
    <span class="c1"># need data to be ordered in time and directly grouped by episodes for</span>
    <span class="c1"># example to train stateful modules or if you plan to use recordings</span>
    <span class="c1"># exclusively in RLlib. Otherwise data is recorded in tabular (columnar)</span>
    <span class="c1"># format. Default is `True`.</span>
    <span class="n">output_write_episodes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
</pre></div>
</div>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="rllib-replay-buffers.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Replay Buffers</p>
      </div>
    </a>
    <a class="right-next"
       href="rllib-rlmodule.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">RL Modules</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-training-an-expert-policy">Example: Training an expert policy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-record-expert-data-to-local-disk">Example: Record expert data to local disk</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-training-on-previously-saved-experiences">Example: Training on previously saved experiences</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-external-expert-experiences">Using external expert experiences</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#converting-tabular-data-to-rllib-s-episode-format">Converting tabular data to RLlib’s episode format</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-old-api-stack-samplebatch-recordings">Using old API stack <code class="docutils literal notranslate"><span class="pre">SampleBatch</span></code> recordings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-processing-filtering-and-post-processing">Pre-processing, filtering and post-processing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling-i-o-throughput">Scaling I/O throughput</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-cloud-storage">Using cloud storage</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-cloud-storage-for-recording">Using cloud storage for recording</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-tune-performance">How to tune performance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-tune-reading-operations">How to tune reading operations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#available-resources">Available resources</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#data-locality">Data locality</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#data-sharding">Data sharding</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#data-pruning">Data pruning</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-tune-post-processing-prelearner">How to tune post-processing (PreLearner)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#actor-pool-size">Actor pool size</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#allocated-resources">Allocated resources</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#read-batch-and-buffer-sizes">Read batch- and buffer sizes</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-tune-updating-learner">How to tune updating (Learner)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Actor pool size</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Allocated resources</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#scheduling-strategy">Scheduling strategy</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-size">Batch size</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-prefetching">Batch prefetching</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#learner-iterations">Learner iterations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#customization">Customization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#connector-level">Connector level</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#define-a-primer-learnerconnector-pipeline">Define a primer LearnerConnector pipeline</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prelearner-level">PreLearner level</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pipeline-level">Pipeline level</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#monitoring">Monitoring</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#input-api">Input API</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#output-api">Output API</a></li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">  
<div class="tocsection editthispage">
  <a href="https://github.com/ray-project/ray/edit/master/doc/source/rllib/rllib-offline.rst">
    <i class="fa-solid fa-pencil"></i>
       Edit
    on GitHub  
  </a>
</div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="footer-content-items footer-content__inner">
  
    <div class="footer-content-item"><div id="csat">
  <div id="csat-feedback-received" class="csat-hidden">
    <span>Thanks for the feedback!</span>
  </div>
  <div id="csat-inputs">
    <span>Was this helpful?</span>
    <div id="csat-yes" class="csat-button">
      <svg id="csat-yes-icon" class="csat-hidden csat-icon" width="18" height="13" viewBox="0 0 18 13" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 10.172L16.1922 0.979004L17.6072 2.393L7.00023 13L0.63623 6.636L2.05023 5.222L7.00023 10.172Z" />
      </svg>
      <span>Yes<span>
    </div>
    <div id="csat-no" class="csat-button">
      <svg id="csat-no-icon" class="csat-hidden csat-icon" width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 5.58599L11.9502 0.635986L13.3642 2.04999L8.41423 6.99999L13.3642 11.95L11.9502 13.364L7.00023 8.41399L2.05023 13.364L0.63623 11.95L5.58623 6.99999L0.63623 2.04999L2.05023 0.635986L7.00023 5.58599Z" />
      </svg>
      <span>No<span>
    </div>
  </div>
  <div id="csat-textarea-group" class="csat-hidden">
    <span id="csat-feedback-label">Feedback</span>
    <textarea id="csat-textarea"></textarea>
    <div id="csat-submit">Submit</div>
  </div>
</div></div>
  
</div>

          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
  <p class="copyright">
    
      © Copyright 2025, The Ray Team.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">
  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.3.7.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.14.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>